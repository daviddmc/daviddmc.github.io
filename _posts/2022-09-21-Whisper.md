---
layout: paper-note
title: "Whisper"
description: Robust Speech Recognition via Large-Scale Weak Supervision
date: 2022-09-21

paper_type: PDF
paper_url: https://arxiv.org/pdf/2212.04356.pdf
code_type: Github
code_url: https://github.com/openai/whisper

bibliography: paper-notes.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Takeaways
  - name: Introduction
  - name: Methods
    subsections:
    - name: Unsupervised Pre-Training
    - name: Supervised Fine-Tuning
    - name: Task-Specific Input Transformations
  - name: Experiments
    subsections:
    - name: Results of Fine-Tuning
    - name: Impact of Number of Layers Transferred 
    - name: Zero-shot Behaviors

---

## Takeaways

https://raw.githubusercontent.com/openai/whisper/main/approach.png

## Introduction

Study the capabilities of speech processing systems trained simply to **predict large amounts of transcripts of audio** on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a **zero-shot transfer** setting.

Previous works of **unsupervised** pre-training

- scaled up to 1,000,000 hours of training data
- learn high-quality representations of speech
- lacking an equivalently performant decoder mapping those representations to usable outputs, necessitating a finetuning stage in order to actually perform a task.
- risk of fine-tuning: overfit to spurious patterns and don't generalize to other datasets

Previous works of **supervised** pre-training

- pre-training across many dataset -> higher robustness and better geralizability
- only a moderate amount of this data easily available

Dataset for **wealy supervised** learning

- noiser labels
- trade-off between quality and quantity
- these new datasets are only a few times larger than the sum of existing high-quality datasets and still much smaller than prior unsupervised work.

This work

- scaling weakly supervised speech recognition the next order of magnitude to 680,000 hours of labeled audio data
- removing the need for any dataset-specific fine-tuning to achieve high-quality
- Broadening the scope of weakly supervised pre-training beyond English-only speech recognition to be both multilingual and multitask

## Methods

### Dataset

Take a minimalist approach to data pre-processing: train models to predict the **raw text of transcripts** without any significant standardization, relying on the expressiveness of seq-to-seq models to learn to map between utterances and their transcribed form.

Construct the dataset from audio that is paired with transcripts on the Internet -> a very diverse

While diversity in audio quality can help train a model to be robust, diversity in transcript quality is not similarly beneficial.

filtering for transcript

- remove machine-generated transcripts from the training dataset
- remove the (audio, transcript) pair if the the spoken language doesn't match the language of the transcript. Exception: if the transcript language is English, add these pairs to the dataset as `X -> en` speech translation training examples instead
- fuzzy de-duping of transcript texts to reduce the amount of duplication and automatically generated content in the training dataset
- break audio files into 30-second segments paired with the subset of the transcript that occurs within that time segment
- train on all audio, including segments where there is no speech and use these segments as training data for voice activity detection
- after training an initial model, aggregated information about its error rate on training data sources and performed manual inspection of these data sources sorting by a combination of both high error rate and data source size in order to identify and remove low-quality ones efficiently
- To avoid contamination, perform de-duplication at a transcript level between the training dataset and the evaluation datasets

### Model

#### Input Audio

1. All audio is re-sampled to 16,000 Hz
2. an 80-channel logmagnitude Mel spectrogram representation is computed on 25-millisecond windows with a stride of 10 milliseconds.
3. globally scale the input to be between -1 and 1 with approximately zero mean across the pre-training dataset

#### Network

1. off-the-shelf architecture: encoder-decoder Transformer (Vaswani et al., 2017)
2. The encoder processes this input representation with a small stem consisting of two convolution layers with a filter width of 3 and the GELU activation function, where the second convolution layer has a stride of two.
3. Sinusoidal position embeddings are then added to the output of the stem after which the encoder Transformer blocks are applied
4. Uses pre-activation residual blocks
5. A final layer normalization is applied to the encoder output
6. The decoder uses learned position embeddings and tied input-output token representations
7. number of parameters for Whisper-Large: 1550M

#### Vocabulary

1. use the same byte-level BPE text tokenizer used in GPT2 for the Englishonly models
2. refit the vocabulary (but keep the same size) for the multilingual models to avoid excessive fragmentation on other languages

### Multitask Format

use a simple format to specify all tasks and conditioning information as a sequence of input tokens to the decoder

Since the decoder is an audio-conditional language model, the author also train it to condition on the history of text of the transcript in the hope that it will learn to use longer-range text context to resolve ambiguous audio. Specifically, with some probability we add the transcript text preceding the current audio segment to the decoder's context.

1. We indicate the beginning of prediction with a `<|startoftranscript|>` token.

2. First, we predict the language being spoken which is represented by a unique token for each language in our training set (99 total). If there is no speech in an audio segment, the model is trained to predict a `<|nospeech|>` token.

3. The next token specifies the task (either transcription or translation) with an `<|transcribe|>` or `<|translate|>` token.

4. After this, we specify whether to predict timestamps or not by including a `<|notimestamps|>` token for that case.

5. At this point, the task and desired format is fully specified, and the output begins.

6. timestamp prediction
    1. we predict time relative to the current audio segment, quantizing all times to the nearest 20 milliseconds which matches the native time resolution of Whisper models, and add additional tokens to our vocabulary for each of these.
    2. We interleave their prediction with the caption tokens: the start time token is predicted before each caption's text, and the end time token is predicted after.
    3. When a final transcript segment is only partially included in the current 30-second audio chunk, we predict only its start time token for the segment when in timestamp mode, to indicate that the subsequent decoding should be performed on an audio window aligned with that time, otherwise we truncate the audio to not include the segment.

7. Lastly, we add a `<|endoftranscript|>` token.

8. We only mask out the training loss over the previous context text, and train the model to predict all other tokens.

## Experiments

### Evaluation

zero-shot

metrics: word error rate (WER)

- sensitive to minor formatting differences
- extensive standardization of text before the WER calculation to minimize penalization of non-semantic differences

### English Speech Recognition

Although the best zero-shot Whisper model has a relatively unremarkable LibriSpeech clean-test WER of 2.5, which is roughly the performance of modern supervised baseline or the mid-2019 state of the art, zero-shot Whisper models have very different robustness properties than supervised LibriSpeech models and out-perform all benchmarked LibriSpeech models by large amounts on other datasets.

Zero-shot Whisper models close the gap to human robustness.

### Multi-lingual Speech Recognition

#### Low-data Benchmarks

Whisper performs well on Multilingual LibriSpeech. However, On VoxPopuli, Whisper significantly underperforms prior work.

#### relationship between size of dataset and performance

Studying the relationship between the amount of training data for a given language and the resulting downstream zero-shot performance for that language. Found a strong squared correlation coefficient of 0.83 between the log of the word error rate and the log of the amount of training data per language.

Many of the largest outliers in terms of worse than expected performance according to this trend are languages that have unique scripts and are more distantly related to the Indo-European languages making up the majority of the training dataset such as Hebrew (HE), Telugu (TE), Chinese (ZH), and Korean (KO). These differences could be due to a lack of transfer due to linguistic distance, our byte level BPE tokenizer being a poor match for these languages, or variations in data quality.

### Translation

`X -> en` translation

Achieved a new state of the art of 29.1 BLEU zero-shot without using any of the CoVoST2 training data. We attribute this to the 68,000 hours of `X -> en` translation data for these languages in our pre-training dataset which, although noisy, is vastly larger than the 861 hours of training data for `X -> en` translation in CoVoST2.

Since Whisper evaluation is zero-shot, it does particularly well on the lowest resource grouping of CoVoST2, improving over mSLAM by 6.7 BLEU. Conversely, the best Whisper model does not actually improve over Maestro and mSLAM on average for the highest resource languages.

### Language Identification

The zero-shot performance of Whisper is not competitive with prior supervised work here and underperforms the supervised SOTA by 13.6%.

### Robustness to Additive Noise

There are many models that outperform our zero-shot performance under low noise (40 dB SNR), which is unsurprising given those models are trained primarily on LibriSpeech, but all models quickly degrade as the noise becomes more intensive, performing worse than the Whisper model under additive pub noise of SNR below 10 dB. This showcases Whisperâ€™s robustness to noise, especially under more natural distribution shifts like the pub noise.

### Long-form Transcription

We developed a strategy to perform buffered transcription of long audio by consecutively transcribing 30-second segments of audio and shifting the window according to the timestamps predicted by the model.

We observed that it is crucial to have beam search and temperature scheduling based on the repetitiveness and the log probability of the model predictions in order to reliably transcribe long audio.