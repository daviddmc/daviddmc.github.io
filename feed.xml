<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://daviddmc.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://daviddmc.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-05-20T04:39:01+00:00</updated><id>https://daviddmc.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Whisper: Web-Scale Supervised Pretraining for Speech Recognition</title><link href="https://daviddmc.github.io/blog/2022/Whisper/" rel="alternate" type="text/html" title="Whisper: Web-Scale Supervised Pretraining for Speech Recognition" /><published>2022-09-21T00:00:00+00:00</published><updated>2022-09-21T00:00:00+00:00</updated><id>https://daviddmc.github.io/blog/2022/Whisper</id><content type="html" xml:base="https://daviddmc.github.io/blog/2022/Whisper/"><![CDATA[<h2 id="takeaways">Takeaways</h2>

<ul>
  <li>Whisper is a <strong>multi-lingual</strong> and <strong>multi-task</strong> speech processing pipeline based on the encoder-decoder Transformer.</li>
  <li>Whisper is trained on 680,000 hours of labeled audio data from the Internet (<strong>weakly supervised</strong>) to predict the transcripts of the audio.</li>
  <li>Whisper generalizes well to standard benchmarks and is often competitive with prior fully supervised results but in a <strong>zero-shot transfer</strong> setting.</li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Previous work on <strong>unsupervised</strong> pre-training</p>

<ul>
  <li>e.g., wav2vec 2.0 and BigSSL.</li>
  <li>Learn directly from <em>raw audio</em> without labels.</li>
  <li>Scaled up to 1,000,000 hours of training data.</li>
  <li>Learn high-quality representations of speech.</li>
  <li>Lack of an equivalently performant decoder mapping those representations to usable outputs, necessitating a finetuning stage in order to actually perform a task.</li>
  <li>Risk of fine-tuning: overfit to spurious patterns and don’t generalize to other datasets.</li>
</ul>

<p>Previous work on <strong>supervised</strong> pre-training</p>

<ul>
  <li>Pre-training across many datasets/domains in a supervised fashion.</li>
  <li>Higher robustness and better generalizability.</li>
  <li>Only a moderate amount of this data is easily available (thousands of hours).</li>
</ul>

<p>Dataset for <strong>weakly supervised</strong> learning</p>

<ul>
  <li>Trade-off between quality and quantity: create larger datasets for speech recognition by relaxing the requirement of gold-standard human-validated transcripts</li>
  <li>These new datasets are only a few times larger than the sum of existing high-quality datasets and still much smaller than prior unsupervised work.</li>
</ul>

<p>This work</p>

<ul>
  <li>Study the capabilities of speech processing systems trained simply to <strong>predict large amounts of transcripts of audio</strong> on the Internet.</li>
  <li>Scale weakly supervised speech recognition the next order of magnitude to 680,000 hours of labeled audio data.</li>
  <li>Remove the need for any dataset-specific fine-tuning to achieve high quality.</li>
  <li>Broaden the scope of weakly supervised pre-training beyond English-only speech recognition to be both <em>multilingual</em> and <em>multitask</em>.</li>
  <li>The resulting models (Whisper) generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a <strong>zero-shot transfer</strong> setting.</li>
</ul>

<h2 id="methods">Methods</h2>

<div class="l-page" style="text-align:center;">
  <img src="https://raw.githubusercontent.com/openai/whisper/main/approach.png" width="100%" style="margin-bottom: 12px; background-color: white;" />
  <p>Overview of Whisper.</p>
</div>

<h3 id="dataset">Dataset</h3>

<p>A minimalist approach to data pre-processing:</p>

<ul>
  <li>Train models to predict the <strong>raw text of transcripts</strong>.</li>
  <li>No significant standardization</li>
  <li>Rely on the expressiveness of seq-to-seq models to learn to map between utterances and their transcribed form.</li>
  <li>Simplify the pipeline since it removes the need for a separate inverse text normalization step.</li>
</ul>

<p>Construct a very diverse dataset from the audio that is paired with transcripts on the Internet. While diversity in audio quality can help train a model to be robust, diversity in transcript quality is not similarly beneficial.</p>

<p>Filtering for transcript:</p>

<ul>
  <li>Remove machine-generated transcripts from the training dataset.</li>
  <li>Remove the (audio, transcript) pair if the spoken language doesn’t match the language of the transcript. Exception: if the transcript language is English, add these pairs to the dataset as <code class="language-plaintext highlighter-rouge">X -&gt; en</code> speech translation training examples instead.</li>
  <li>Fuzzy de-duping of transcript texts to reduce the amount of duplication and automatically generated content in the training dataset.</li>
  <li>Break audio files into 30-second segments paired with the subset of the transcript that occurs within that time segment.</li>
  <li>Train on all audio, including segments where there is no speech, and use these segments as training data for voice activity detection.</li>
  <li>After training an initial model, aggregated information about its error rate on training data sources and performed a manual inspection of these data sources sorting by a combination of both high error rate and data source size in order to identify and remove low-quality ones efficiently.</li>
  <li>To avoid contamination, perform de-duplication at a transcript level between the training dataset and the evaluation datasets.</li>
</ul>

<h3 id="model">Model</h3>

<h4 id="input-audio">Input Audio</h4>

<ol>
  <li>All audio is re-sampled to 16,000 Hz.</li>
  <li>An 80-channel log magnitude Mel spectrogram representation is computed on 25-millisecond windows with a stride of 10 milliseconds.</li>
  <li>Globally scale the input to be between -1 and 1 with approximately zero mean across the pre-training dataset.</li>
</ol>

<h4 id="network">Network</h4>

<ul>
  <li>Use off-the-shelf architecture: encoder-decoder Transformer<d-cite key="Transformer"></d-cite></li>
  <li>The encoder processes this input representation with a small stem consisting of two convolution layers with a filter width of 3 and the GELU activation function, where the second convolution layer has a stride of two.</li>
  <li>Sinusoidal position embeddings are then added to the output of the stem after which the encoder Transformer blocks are applied</li>
  <li>Uses pre-activation residual blocks</li>
  <li>A final layer normalization is applied to the encoder output</li>
  <li>The decoder uses learned position embeddings and tied input-output token representations</li>
  <li>Number of parameters for Whisper-Large: 1550M</li>
</ul>

<h4 id="vocabulary">Vocabulary</h4>

<ul>
  <li>Use the same byte-level BPE text tokenizer used in GPT2 for the English only models.</li>
  <li>Refit the vocabulary (but keep the same size) for the multilingual models to avoid excessive fragmentation on other languages.</li>
</ul>

<h3 id="multitask-format">Multitask Format</h3>

<p>In addition to predicting which words were spoken in a given audio snippet, A fully featured speech recognition system can involve many additional components, e.g., voice activity detection, speaker diarization (i.e., the process of partitioning an input audio stream into homogeneous segments according to the speaker identity), and inverse text normalization.</p>

<p>To reduce this complexity, this work uses a single model to perform the entire speech processing pipeline and uses a simple format to specify all tasks and conditioning information as a sequence of input tokens to the decoder.</p>

<p>Since the decoder is an audio-conditional language model, the authors also train it to condition on the history of the text of the transcript in the hope that it will learn to use longer-range text context to resolve ambiguous audio. Specifically, with some probability, the transcript text preceding the current audio segment is added to the decoder’s context.</p>

<ol>
  <li>Indicate the beginning of the prediction with a <code class="language-plaintext highlighter-rouge">&lt;|startoftranscript|&gt;</code> token.</li>
  <li>First, predict the language being spoken which is represented by a unique token for each language in our training set (99 total). If there is no speech in an audio segment, the model is trained to predict a <code class="language-plaintext highlighter-rouge">&lt;|nospeech|&gt;</code> token.</li>
  <li>The next token specifies the task (either transcription or translation) with an <code class="language-plaintext highlighter-rouge">&lt;|transcribe|&gt;</code> or <code class="language-plaintext highlighter-rouge">&lt;|translate|&gt;</code> token.</li>
  <li>After this, specify whether to predict timestamps or not by including a <code class="language-plaintext highlighter-rouge">&lt;|notimestamps|&gt;</code> token for that case.</li>
  <li>At this point, the task and desired format are fully specified, and the output begins.</li>
  <li>Timestamp prediction:
    <ol>
      <li>Predict time relative to the current audio segment, quantizing all times to the nearest 20 milliseconds which matches the native time resolution of Whisper models, and add additional tokens to our vocabulary for each of these.</li>
      <li>Interleave timestamp prediction with the caption tokens: the start time token is predicted before each caption’s text, and the end time token is predicted after.</li>
      <li>When a final transcript segment is only partially included in the current 30-second audio chunk, we predict only its start time token for the segment when in timestamp mode, to indicate that the subsequent decoding should be performed on an audio window aligned with that time, otherwise we truncate the audio to not include the segment.</li>
    </ol>
  </li>
  <li>Lastly, add a <code class="language-plaintext highlighter-rouge">&lt;|endoftranscript|&gt;</code> token.</li>
  <li>Only mask out the training loss over the previous context text, and train the model to predict all other tokens.</li>
</ol>

<h2 id="experiments">Experiments</h2>

<h3 id="evaluation">Evaluation</h3>

<p>The goal of Whisper is to develop a single robust speech processing system that works reliably without the need for dataset-specific fine-tuning to achieve high-quality results on specific distributions.</p>

<p>Evaluate Whisper in a <strong>zero-shot setting</strong> without using any of the training data for each of the datasets to measure broad generalization.</p>

<p>Metrics: word error rate (WER)</p>

<ul>
  <li>Sensitive to minor formatting differences.</li>
  <li>Apply extensive standardization of text before the WER calculation to minimize penalization of non-semantic differences.</li>
</ul>

<h3 id="english-speech-recognition">English Speech Recognition</h3>

<p>For the previous SOTA supervised methods, there is a gap between reportedly superhuman performance in-distribution and subhuman performance out-of-distribution.</p>

<p>This might be due to conflating different capabilities being measured by human and machine performance on a test set.</p>

<ul>
  <li>Humans are often asked to perform a task given little to no supervision on the specific data distribution. Thus human performance is a measure of out-of-distribution generalization.</li>
  <li>But machine learning models are usually evaluated after training on a large amount of supervision from the evaluation distribution, meaning that machine performance is instead a measure of in-distribution generalization.</li>
</ul>

<p>Results:</p>

<ul>
  <li>Although the best zero-shot Whisper model has a relatively unremarkable LibriSpeech clean-test WER of 2.5, which is roughly the performance of a modern supervised baseline or the mid-2019 SOTA,</li>
  <li>zero-shot Whisper models have very different robustness properties than supervised LibriSpeech models and outperform all benchmarked LibriSpeech models by large amounts on other datasets.</li>
  <li>Zero-shot Whisper models close the gap to human robustness: The best zero-shot Whisper models roughly match human accuracy and robustness.</li>
</ul>

<h3 id="multi-lingual-speech-recognition">Multi-lingual Speech Recognition</h3>

<h4 id="low-data-benchmarks">Low-data Benchmarks</h4>

<p>Whisper performs well on Multilingual LibriSpeech. However, On VoxPopuli, Whisper significantly underperforms prior work.</p>

<h4 id="relationship-between-size-of-dataset-and-performance">Relationship between Size of Dataset and Performance</h4>

<ul>
  <li>Studied the relationship between the amount of training data for a given language and the resulting downstream zero-shot performance for that language.</li>
  <li>Found a strong squared correlation coefficient of 0.83 between the log of the word error rate and the log of the amount of training data per language.</li>
  <li>Many of the largest outliers in terms of worse-than-expected performance according to this trend are languages that have unique scripts and are more distantly related to the Indo-European languages. These differences could be due to a lack of transfer due to linguistic distance, the byte-level BPE tokenizer being a poor match for these languages or variations in data quality.</li>
</ul>

<h3 id="translation">Translation</h3>

<p>Study the translation capabilities of Whisper by measuring the performance on the <code class="language-plaintext highlighter-rouge">X -&gt; en</code> translation.</p>

<p>Results:</p>

<ul>
  <li>Achieved a new SOTA of 29.1 BLEU zero-shot without using any of the CoVoST2 training data. These results may be attributed to the 68,000 hours of <code class="language-plaintext highlighter-rouge">X -&gt; en</code> translation data for these languages in our pre-training dataset which, although noisy, is vastly larger than the 861 hours of training data for <code class="language-plaintext highlighter-rouge">X -&gt; en</code> translation in CoVoST2.</li>
  <li>Since Whisper evaluation is zero-shot, it does particularly well on the lowest resource grouping of CoVoST2, improving over mSLAM by 6.7 BLEU. Conversely, the best Whisper model does not actually improve over Maestro and mSLAM on average for the highest resource languages.</li>
</ul>

<h3 id="language-identification">Language Identification</h3>

<p>The zero-shot performance of Whisper is not competitive with prior supervised work here and underperforms the supervised SOTA by 13.6%.</p>

<h3 id="robustness-to-additive-noise">Robustness to Additive Noise</h3>

<ul>
  <li>There are many models that outperform the zero-shot Whisper performance under low noise (40 dB SNR), which is unsurprising given those models are trained primarily on LibriSpeech,</li>
  <li>but all models quickly degrade as the noise becomes more intensive, performing worse than the Whisper model under additive pub noise of SNR below 10 dB.</li>
  <li>This showcases Whisper’s robustness to noise, especially under more natural distribution shifts like the pub noise.</li>
</ul>

<h3 id="long-form-transcription">Long-form Transcription</h3>

<ul>
  <li>Whisper models are trained on 30-second audio chunks and cannot consume longer audio inputs at once.</li>
  <li>The authors developed a strategy to perform buffered transcription of long audio by consecutively transcribing 30-second segments of audio and shifting the window according to the timestamps predicted by the model.</li>
  <li>It is crucial to have beam search and temperature scheduling based on the repetitiveness and the log probability of the model predictions in order to reliably transcribe long audio.</li>
</ul>

<p>Results: Whisper is competitive with state-of-the-art commercial and open-source ASR systems in long-form transcription.</p>

<h3 id="comparison-with-human-performance">Comparison with Human Performance</h3>

<p>These results indicate that Whisper’s English ASR performance is not perfect but very close to human-level accuracy.</p>

<h3 id="ablations">Ablations</h3>

<h4 id="model-scaling">Model Scaling</h4>

<p>Concerns with using a large but noisy dataset:</p>

<ul>
  <li>Although it may look promising to begin with, the performance of models trained on this kind of data may saturate at the inherent quality level of the dataset.</li>
  <li>As capacity and compute spent training on the dataset increases, models may learn to exploit the idiosyncrasies of the dataset, and their ability to generalize robustly to out-of-distribution data could even degrade.</li>
</ul>

<p>Study the zero-shot generalization of Whisper models as a function of the model size.</p>

<ul>
  <li>With the exception of English speech recognition, performance continues to increase with model size across multilingual speech recognition, speech translation, and language identification.</li>
  <li>The diminishing returns for English speech recognition could be due to saturation effects from approaching human-level performance.</li>
</ul>

<h4 id="dataset-scaling">Dataset Scaling</h4>

<p>To study how important is the raw dataset size to Whisper’s performance</p>

<p>Results:</p>

<ul>
  <li>All increases in the dataset size result in improved performance on all tasks</li>
  <li>The general trend across tasks of diminishing returns when moving from 54,000 hours to the full dataset size of 680,000 hours could suggest that
    <ul>
      <li>the current best Whisper models are under-trained relative to dataset size and performance could be further improved by a combination of longer training and larger models.</li>
      <li>we are nearing the end of performance improvements from dataset size scaling for speech recognition.</li>
      <li>Further analysis is needed to characterize “scaling laws” for speech recognition in order to decide between these explanations.</li>
    </ul>
  </li>
</ul>

<h4 id="multitask-and-multilingual-transfer">Multitask and Multilingual Transfer</h4>

<p>A potential concern with jointly training a single model on many tasks and languages is the possibility of negative transfer where interference between the learning of several tasks results in performance worse than would be achieved by training on only a single task or language.</p>

<p>Compare the performance of models trained on <em>just English</em> speech recognition with the standard multitask and multilingual training setup.</p>

<p>Results:</p>

<ul>
  <li>For small models trained with moderate amounts of compute, there is indeed negative transfer between tasks and languages: joint models underperform English-only models trained for the same amount of compute.</li>
  <li>Multitask and multi-lingual models scale better and for the largest experiments outperform their English-only counterparts demonstrating positive transfer from other tasks.</li>
</ul>

<h4 id="text-normalization">Text Normalization</h4>

<p>There is a risk of overfitted text normalization.</p>

<p>Compare the performance of Whisper using the proposed normalizer versus an independently developed one from another project.</p>

<p>Results:</p>

<ul>
  <li>On most datasets, the two normalizers perform similarly.</li>
  <li>On some datasets, the proposed normalizer reduces the WER of Whisper significantly more. The differences in reduction can be traced down to different formats used by the ground truth and how the two normalizers are penalizing them.</li>
</ul>

<h4 id="strategies-for-reliable-long-form-transcription">Strategies for Reliable Long-form Transcription</h4>

<p>Transcribing long-form audio using Whisper relies on accurate prediction of the timestamp tokens to determine the amount to shift the model’s 30-second audio context window by, and inaccurate transcription in one window may negatively impact transcription in the subsequent windows.</p>

<p>Strategies to avoid failure cases of long-form transcription:</p>

<ul>
  <li><strong>Beam search:</strong> Use beam search with 5 beams using the log probability as the score function, to reduce repetition looping which happens more frequently in greedy decoding.</li>
  <li><strong>Temperature fallback:</strong> The temperature starts with 0, i.e. always selecting the tokens with the highest probability, and increases by 0.2 up to 1.0 when either the average log probability over the generated tokens is lower than −1 or the generated text has a gzip compression rate higher than 2.4.</li>
  <li><strong>Previous text conditioning:</strong> Providing the transcribed text from the preceding window as previous-text conditioning when the applied temperature is below 0.5 further improves the performance.</li>
  <li><strong>Voice activity detection:</strong> The probability of the <code class="language-plaintext highlighter-rouge">&lt;|nospeech|&gt;</code> token alone is not sufficient to distinguish a segment with no speech, but combining the no-speech probability threshold of 0.6 and the average log-probability threshold of −1 makes the voice activity detection of Whisper more reliable.</li>
  <li><strong>Initial timestamp constraint:</strong> To avoid a failure mode where the model ignores the first few words in the input, the authors constrained the initial timestamp token to be between 0.0 and 1.0 second.</li>
</ul>

<h2 id="limitations">Limitations</h2>

<h3 id="improved-decoding-strategies">Improved Decoding Strategies</h3>

<p>Perception-related errors:</p>

<ul>
  <li>e.g., confusing similar-sounding words.</li>
  <li>Larger models have made steady and reliable progress on reducing perception-related errors.</li>
</ul>

<p>Non-perceptual errors:</p>

<ul>
  <li>e.g., getting stuck in repeat loops, not transcribing the first or last few words of an audio segment, or outputting a transcript entirely unrelated to the actual audio.</li>
  <li>More stubborn in nature.</li>
  <li>They are a combination of failure modes of seq2seq models, language models, and text-audio alignment.</li>
  <li>Potential solutions:
    <ul>
      <li>Fine-tuning on a high-quality supervised dataset</li>
      <li>Using reinforcement learning to more directly optimize for decoding performance.</li>
    </ul>
  </li>
</ul>

<h3 id="increase-training-data-for-lower-resource-languages">Increase Training Data For Lower-Resource Languages</h3>

<p>Whisper’s speech recognition performance is still quite poor on many languages.</p>

<p>The pre-training dataset is currently very English-heavy due to biases of the data collection pipeline.</p>

<p>A targeted effort at increasing the amount of data for these rarer languages could result in a large improvement to average speech recognition performance even with only a small increase in the overall training dataset size.</p>

<h3 id="studying-fine-tuning">Studying Fine-Tuning</h3>

<p>This work focused on the robustness properties of speech processing systems and as a result only studied the zero-shot transfer.</p>

<p>It is likely that results can be improved further by fine-tuning.</p>

<h3 id="studying-the-impact-of-language-models-on-robustness">Studying the Impact of Language Models on Robustness</h3>

<p>The authors suspect that Whisper’s robustness is partially due to its strong decoder, which is an audio-conditional LM.</p>

<p>It’s currently unclear to what degree the benefits of Whisper stem from training its encoder, decoder, or both.</p>

<p>Potential Experiments:</p>

<ul>
  <li>Train a decoder-less CTC model.</li>
  <li>Study how the performance of existing speech recognition encoders change when used together with a language model.</li>
</ul>

<h3 id="adding-auxiliary-training-objectives">Adding Auxiliary Training Objectives</h3>

<p>Whisper departs noticeably from the most recent SOTA speech recognition systems due to the lack of unsupervised pre-training or self-teaching methods.  It is possible that the results could be further improved by incorporating this.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Robust Speech Recognition via Large-Scale Weak Supervision]]></summary></entry><entry><title type="html">Latent Diffusion (Stable Diffusion)</title><link href="https://daviddmc.github.io/blog/2022/Stable-Diffusion/" rel="alternate" type="text/html" title="Latent Diffusion (Stable Diffusion)" /><published>2022-04-13T00:00:00+00:00</published><updated>2022-04-13T00:00:00+00:00</updated><id>https://daviddmc.github.io/blog/2022/Stable-Diffusion</id><content type="html" xml:base="https://daviddmc.github.io/blog/2022/Stable-Diffusion/"><![CDATA[<h2 id="takeaways">Takeaways</h2>

<ul>
  <li>This work proposes a method for training diffusion models in a learned latent space (<strong>Latent Diffusion Models, LDMs</strong>), improving both the training and sampling efficiency of DMs without degrading their quality.</li>
  <li>LDMs learn the encoder/decoder (perceptual compression model) and the DMs separately <strong>in two stages</strong> such that
    <ul>
      <li>it doesn’t require a delicate weighting of reconstruction and generative abilities;</li>
      <li>the same encoder/decoder can be used to train different DMs.</li>
    </ul>
  </li>
  <li>LDMs incorporate a general-purpose <strong>conditioning mechanism</strong> based on <strong>cross-attention</strong>, enabling multi-modal training.</li>
  <li>LDMs achieve new SOTA results for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including text-to-image synthesis, unconditional image generation and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.</li>
</ul>

<h2 id="introduction">Introduction</h2>

<h3 id="image-synthesis">Image Synthesis</h3>

<ul>
  <li>Autoregressive (AR) transformers (with likelihood-based models)
    <ul>
      <li>Spectacular results (e.g. DALL-E<d-cite key="DALL-E"></d-cite>, VQ-VAE-2<d-cite key="VQ-VAE-2"></d-cite>).</li>
      <li>Large models (billions of parameters), high computational cost.</li>
    </ul>
  </li>
  <li>GANs
    <ul>
      <li>e.g., BigGAN, StyleGAN.</li>
      <li>They are mostly confined to data with comparably limited variability.</li>
      <li>The adversarial learning procedure does not easily scale to modeling complex, multi-modal distributions.</li>
    </ul>
  </li>
  <li>DMs<d-cite key="DDPM"></d-cite>
    <ul>
      <li>Learn a hierarchy of denoising autoencoders.</li>
      <li>Define new SOTA in class-conditional image synthesis (e.g., ADM<d-cite key="ADM"></d-cite>) and super-resolution (e.g., SR3<d-cite key="SR3"></d-cite>).</li>
      <li>Unconditional DMs can readily be applied to tasks such as inpainting and colorization or stroke-based synthesis.</li>
      <li>Being likelihood-based models, they do not exhibit mode-collapse and training instabilities as GANs.</li>
      <li>By heavily exploiting parameter sharing, they can model highly complex distributions without involving large AR models.</li>
    </ul>
  </li>
</ul>

<h3 id="problems-with-dms">Problems with DMs</h3>

<ul>
  <li>DMs belong to the class of likelihood-based models, whose mode-covering behavior makes them prone to spend excessive amounts of capacity on modeling imperceptible details of the data</li>
  <li>DMs are computationally demanding, since training and evaluating such a model requires repeated function evaluations (and gradient computations) in the high-dimensional space of RGB images.</li>
</ul>

<p>Goal: Reducing the computational demands of DMs without impairing their performance.</p>

<h3 id="semantic-compression-and-perceptual-compression">Semantic Compression and Perceptual Compression</h3>

<div class="l-body" style="text-align:center;">
  <img src="https://research.runwayml.com/images/publications/ldm/article-Figure2-1.png" width="40%" style="margin-bottom: 12px; background-color: white;" />
  <p>Illustrating perceptual and semantic compression.</p>
</div>

<p>Learning a likelihood-based model (e.g., DMs) can be roughly divided into two stages:</p>

<ol>
  <li><strong>Perceptual Compression:</strong> The model removes high-frequency details but still learns little semantic variation.</li>
  <li><strong>Semantic Compression:</strong> The actual generative model learns the semantic and conceptual composition of the data</li>
</ol>

<p>The distortion decreases steeply in the low-rate region of the rate-distortion plot, indicating that the majority of the bits are indeed allocated to imperceptible distortion. While DMs allow suppressing this semantically meaningless information by minimizing the responsible loss term, gradients (during training) and the neural network backbone (training and inference) still need to be evaluated on all pixels, leading to superfluous computations and unnecessarily expensive optimization and inference.</p>

<h3 id="ideas">Ideas</h3>

<ul>
  <li>Aim to first find a perceptually equivalent, but computationally more suitable space, in which DMs can be trained for high-resolution image synthesis.</li>
  <li>Propose latent diffusion models (LDMs) as an effective generative model and a separate mild compression stage that only eliminates imperceptible details.</li>
  <li>Separate training into two distinct phases:
    <ol>
      <li>Train an autoencoder in a lower-dimensional representational space that is perceptually equivalent to the data space.</li>
      <li>Train DMs in the learned latent space, which exhibits better scaling properties with respect to the spatial dimensionality.</li>
    </ol>
  </li>
</ul>

<p>Advantages:</p>

<ul>
  <li>The resulting DMs are computationally much more efficient because sampling is performed on a low-dimensional space.</li>
  <li>Exploit the inductive bias of DMs inherited from their UNet architecture, which makes them particularly effective for data with spatial structure and therefore alleviates the need for aggressive, quality-reducing compression levels.</li>
  <li>The general-purpose compression models can be used for training multiple generative models or other downstream applications.</li>
</ul>

<h3 id="contributions">Contributions</h3>

<ul>
  <li>In contrast to purely transformer-based approaches, LDM scales more gracefully to higher dimensional data.</li>
  <li>LDM achieves competitive performance on multiple tasks while significantly lowering computational costs.</li>
  <li>In contrast to previous work that learns both an encoder/decoder architecture and a score-based prior simultaneously, LDM does not require a delicate weighting of reconstruction and generative abilities. This ensures extremely faithful reconstructions and requires very little regularization of the latent space.</li>
  <li>For densely conditioned tasks (e.g., super-resolution, inpainting, and semantic synthesis), LDM can be applied in a convolutional fashion and render large, consistent images.</li>
  <li>A general-purpose conditioning mechanism based on cross-attention is proposed, enabling multi-modal training.</li>
</ul>

<h2 id="methods">Methods</h2>

<div class="l-body" style="text-align:center;">
  <img src="https://research.runwayml.com/images/publications/ldm/article-Figure3-1.png" width="60%" style="margin-bottom: 12px; background-color: white;" />
  <p>The architecture of LDM.</p>
</div>

<h3 id="perceptual-image-compression-autoencoder">Perceptual Image Compression (Autoencoder)</h3>

<p>The architecture of the perceptual compression model is based on VQGAN.</p>

<ul>
  <li>An autoencoder architecture</li>
</ul>

<table>
  <thead>
    <tr>
      <th>Encoder</th>
      <th>Decoder</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>\(x\in\mathbb{R}^{H\times W\times C}\)</td>
      <td>\(z\in\mathbb{R}^{h\times w\times c}\)</td>
    </tr>
    <tr>
      <td>\(\text{Conv2D} \rightarrow\mathbb{R}^{H\times W\times C'}\)</td>
      <td>\(\text{Conv2D} \rightarrow\mathbb{R}^{H\times W\times C''}\)</td>
    </tr>
    <tr>
      <td>\(m\times\{\ \text{ResBlock, Downsample}\}\rightarrow\mathbb{R}^{h\times w\times C''}\)</td>
      <td>\(\text{ResBlock}\rightarrow\mathbb{R}^{h\times w\times C''}\)</td>
    </tr>
    <tr>
      <td>\(\text{ResBlock}\rightarrow\mathbb{R}^{h\times w\times C''}\)</td>
      <td>\(\text{Non-Local Block}\rightarrow\mathbb{R}^{h\times w\times C''}\)</td>
    </tr>
    <tr>
      <td>\(\text{Non-Local Block}\rightarrow\mathbb{R}^{h\times w\times C''}\)</td>
      <td>\(\text{ResBlock}\rightarrow\mathbb{R}^{h\times w\times C''}\)</td>
    </tr>
    <tr>
      <td>\(\text{ResBlock}\rightarrow\mathbb{R}^{h\times w\times C''}\)</td>
      <td>\(m\times\{\ \text{ResBlock, Upsample}\}\rightarrow\mathbb{R}^{H\times W\times C'}\)</td>
    </tr>
    <tr>
      <td>\(\text{GroupNorm, Swish, Conv2D} \rightarrow\mathbb{R}^{h\times w\times c}\)</td>
      <td>\(\text{GroupNorm, Swish, Conv2D} \rightarrow\mathbb{R}^{H\times W\times C}\)</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>Loss: a combination of a perceptual loss and a patch-based adversarial objective</li>
  <li>This ensures that the reconstructions are confined to the image manifold by enforcing local realism and avoids blurriness introduced by relying solely on pixel-space losses such as \(L_2\) or \(L_1\) objectives.</li>
</ul>

<ol>
  <li>Given an RGB image \(x\in\mathbb{R}^{H\times W\times 3}\).</li>
  <li>The encoder \(\mathcal{E}\) encodes \(x\) into a latent representation \(z=\mathcal{E}(x)\in\mathbb{R}^{h\times w\times c}\). The encoder downsamples the image by a factor \(f = H/h = W/w\),</li>
  <li>The decoder \(\mathcal{D}\) reconstructs the image from the latent, giving \(\tilde{x}=\mathcal{D}(z)=\mathcal{D}(\mathcal{E}(x))\).</li>
</ol>

<p>To avoid arbitrarily high-variance latent spaces, the authors experiment with two kinds of regularizations.</p>

<ul>
  <li><strong>KL-reg.</strong>
    <ul>
      <li>KL-penalty towards a standard normal on the learned latent, similar to a VAE.</li>
      <li>For a KL-regularized latent space, the latent \(z\) is sampled by \(\mathcal{E}(x):=\mathcal{E}_\mu(x)+\mathcal{E}_\sigma(x)\epsilon, \epsilon\sim\mathcal{N}(0,1).\)</li>
      <li>The latent \(z\) is then rescaled with component-wise variance (i.e., the variance of the entire tensor \(z\)).</li>
    </ul>
  </li>
  <li><strong>VQ-reg.</strong>
    <ul>
      <li>Use a vector quantization layer within the decoder \(\mathcal{D}\). This model can be interpreted as a VQGAN but with the quantization layer absorbed by the decoder.</li>
      <li>For a VQ-regularized latent space, features <em>before</em> the quantization layer is extracted as the latent \(z\)</li>
      <li>No rescaling is needed.</li>
    </ul>
  </li>
</ul>

<p>Because the subsequent DM is designed to work with the <em>2D</em> structure of the learned latent space \(z = \mathcal{E}(x)\), the authors use mild compression rates and achieve very good reconstructions. This is in contrast to previous works (e.g. VQGAN), which relied on an arbitrary <em>1D</em> ordering of the learned space \(z\) to model its distribution autoregressively and thereby ignored much of the inherent structure of \(z\).</p>

<h3 id="latent-diffusion-models">Latent Diffusion Models</h3>

<h4 id="diffusion-models-on-image-domain">Diffusion Models (on Image Domain)</h4>

<ul>
  <li>Probabilistic models designed to learn a data distribution \(p(x)\) by gradually denoising a normally distributed variable.</li>
  <li>Learning the reverse process of a fixed Markov Chain of length \(T\).</li>
  <li>Rely on a reweighted variant of the variational lower bound on \(p(x)\).</li>
  <li>
    <p>Can be interpreted as an equally weighted sequence of denoising autoencoders \(\epsilon_\theta (x_t, t); t=1,\dots,T\)</p>

\[L_{DM}=\mathbb{E}_{x,\epsilon\sim\mathcal{N}(0,1),t}\left[ || \epsilon - \epsilon_\theta(x_t,t) ||_2^2 \right]\]
  </li>
</ul>

<h4 id="generative-modeling-of-latent-representations">Generative Modeling of Latent Representations</h4>

<ul>
  <li>Use latent space instead of high-dimensional pixel space.
    <ul>
      <li>Focus on the important, semantic bits of the data.</li>
      <li>Train in a lower dimensional, computationally much more efficient space.</li>
    </ul>
  </li>
  <li>Unlike previous work that relied on autoregressive Transformers in a highly compressed, discrete latent space, LDM takes advantage of image-specific inductive biases.</li>
  <li>The neural backbone in LDM \(\epsilon_\theta(\cdot, t)\) is realized as a <strong>time-conditional UNet</strong>.
    <ol>
      <li>The time step \(t\) is mapped to a sinusoidal embedding.</li>
      <li>It then goes through Linear, SiLU, Linear, SiLU, Linear</li>
      <li>The outputs can be either added to the UNet features or used as affine weights to transform the UNet features.</li>
    </ol>
  </li>
  <li>Since the forward process is fixed, \(z_t\) can be efficiently obtained from \(\mathcal{E}\) during training.</li>
  <li>Samples from \(p(z)\) can be decoded to image space with a single pass through \(\mathcal{D}\).</li>
  <li>
    <p>The LDM focuses the objective on the perceptually most relevant bits using the reweighted bound</p>

\[L_{LDM} = \mathbb{E}_{\mathcal{E}(x),\epsilon\sim\mathcal{N}(0,1),t}\left[ || \epsilon - \epsilon_\theta(z_t,t) ||_2^2 \right].\]
  </li>
</ul>

<h3 id="conditioning-mechanisms">Conditioning Mechanisms</h3>

<table>
  <tbody>
    <tr>
      <td>DMs are capable of modeling conditional distributions of the form $$p(z</td>
      <td>y)$$.</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>This can be implemented with a conditional denoising autoencoder \(\epsilon_\theta(z_t, t, y)\).</li>
  <li>Control the synthesis process through inputs y (e.g., text, semantic maps, or other image-to-image translation tasks).</li>
</ul>

<p>Augmenting the UNet backbone with the cross-attention mechanism</p>

<ol>
  <li>Use a domain-specific encoder \(\tau_\theta(y)\) to project \(y\) to an intermediate representation \(\tau_\theta(y)\in\mathbb{R}^{M\times d_\tau}\).</li>
  <li>
    <p>The intermediate (flattened )representation of the UNet \(\varphi_i(z_t)\in\mathbb{R}^{N\times d_\epsilon^i}\) is mapped with a cross-attention layer</p>

\[\begin{aligned}
 \text{Attention}(Q, K, V)&amp;=\text{softmax}(QK^T/\sqrt{d})V\\
 Q=W_Q^{(i)}\varphi_i(z_t),\quad K&amp;=W_K^{(i)}\tau_\theta(y),\quad V=W_V^{(i)}\tau_\theta(y)
 \end{aligned}\]
  </li>
</ol>

<p>Loss for conditional LDM:</p>

\[L_{LDM} = \mathbb{E}_{\mathcal{E}(x),y,\epsilon\sim\mathcal{N}(0,1),t}\left[ || \epsilon - \epsilon_\theta(z_t,t,\tau_\theta(y)) ||_2^2 \right].\]

<h2 id="experiments">Experiments</h2>

<h3 id="on-perceptual-compression-tradeoffs">On Perceptual Compression Tradeoffs</h3>

<p>Train class-conditional LDMs on the ImageNet with different downsampling factors \(f\).</p>

<ul>
  <li>Small downsampling factors for \(f=1,2\) result in slow training progress.</li>
  <li>Overly large values of \(f\) cause stagnating fidelity after comparably few training steps. This might be due to
    <ul>
      <li>leaving most of the perceptual compression to the diffusion model,</li>
      <li>too strong first stage compression resulting in information loss and thus limiting the achievable quality.</li>
    </ul>
  </li>
  <li>LDMs with \(f=4\sim 16\) strike a good balance between efficiency and perceptually faithful results.</li>
</ul>

<p>Train LDMs on CelebAHQ and ImageNet with different \(f\) and plot sample speed against FID scores.</p>

<ul>
  <li>LDMs with \(f=4,8\) achieve much lower FID scores while simultaneously significantly increasing sample throughput, compared to pixel-level DMs (\(f=1\)).</li>
  <li>Complex datasets such as ImageNet require reduced compression rates to avoid reducing quality.</li>
  <li>LDMs with \(f=4,8\) offer the best conditions for achieving high-quality synthesis results.</li>
</ul>

<h3 id="image-generation-with-latent-diffusion">Image Generation with Latent Diffusion</h3>

<p>Train <em>unconditional</em> LDMs and evaluate:</p>

<ul>
  <li>sample quality (FID)</li>
  <li>coverage of the data manifold (Precision and Recall)</li>
</ul>

<p>Results:</p>

<ul>
  <li>New SOTA FID on CelebA-HQ.</li>
  <li>Outperform LSGM (a latent diffusion model is trained jointly together with the first stage).</li>
  <li>Outperform prior diffusion-based approaches on all but the LSUN-Bedrooms dataset.</li>
  <li>LDMs consistently improve upon GAN-based methods in Precision and Recall, thus confirming the advantages of their mode-covering likelihood-based training objective over adversarial approaches.</li>
</ul>

<h3 id="conditional-latent-diffusion">Conditional Latent Diffusion</h3>

<ul>
  <li>Cross-attention and domain-specific encoder \(\tau_\theta\) (Transformers):
    <ul>
      <li>text-to-image</li>
      <li>sematic layouts-to-image</li>
    </ul>
  </li>
  <li>Embedding for each class (added to time embedding)
    <ul>
      <li>Class-conditional image generation</li>
    </ul>
  </li>
  <li>Concatenate spatially aligned conditioning information to the input of \(\epsilon_\theta\)
    <ul>
      <li>image-to-image translation</li>
      <li>semantic synthesis</li>
      <li>super-resolution</li>
      <li>inpainting</li>
    </ul>
  </li>
</ul>

<h4 id="text-to-image-synthesis">Text-to-Image Synthesis</h4>

<ul>
  <li>Train a 1.45B parameter KL-regularized LDM conditioned on language prompts on LAION-400M dataset.</li>
  <li>Use BERT-tokenizer</li>
  <li>Implement \(\tau_\theta\) as a transformer to infer a latent code</li>
  <li>The model generalizes well to complex, user-defined text prompts.</li>
</ul>

<h4 id="layouts-to-image-synthesis">Layouts-to-image Synthesis</h4>

<ul>
  <li>Use a Transformer-based \(\tau_\theta\) similar to the text-to-image synthesis.</li>
  <li>The layout-to-image model discretizes the spatial locations of the bounding boxes and encodes each box as a \((l, b, c)\)-tuple, where \(l\) denotes the (discrete) top-left and \(b\) the bottom-right position. Class information is contained in \(c\).</li>
</ul>

<h4 id="semantic-synthesis">Semantic Synthesis</h4>

<ul>
  <li>Use images of landscapes paired with semantic maps.</li>
  <li>Concatenate downsampled versions of the semantic maps with the latent image representation of a \(f = 4\) model.</li>
  <li>Train on an input resolution of \(256^2\) (crops from \(384^2\)).</li>
  <li>Generalize to larger resolutions and can generate images up to the megapixel regime when evaluated in a convolutional manner.</li>
</ul>

<h4 id="super-resolution-with-latent-diffusion">Super-Resolution with Latent Diffusion</h4>

<p>LDMs can be efficiently trained for super-resolution by directly conditioning on low-resolution images via concatenation.</p>

<p>LDM-SR (trained with bicubic degradation)</p>

<ul>
  <li>Qualitative and quantitative results show competitive performance of LDM-SR and SR3
    <ul>
      <li>LDM-SR outperforms SR3 in FID.</li>
      <li>SR3 has a better IS.</li>
    </ul>
  </li>
  <li>Results of human preference affirm the good performance of LDM-SR.</li>
  <li>PSNR and SSIM can be pushed by using a post-hoc guiding mechanism (<em>image-based guider</em>)</li>
  <li>LDM-SR does not generalize well to images that do not follow this pre-processing.</li>
</ul>

<p>LDM-BSR (a generic model trained with more diverse degradation)</p>

<ul>
  <li>JPEG compressions noise, camera sensor noise, different image interpolations for downsampling, Gaussian blur kernels, and Gaussian noise (applied in random order).</li>
  <li>LDM-BSR produces images much sharper than the models confined to a fixed preprocessing, making it suitable for real-world applications.</li>
</ul>

<h4 id="inpainting-with-latent-diffusion">Inpainting with Latent Diffusion</h4>

<p>Inpainting is the task of filling masked regions of an image with new content either because parts of the image are corrupted or to replace existing but undesired content within the image.</p>

<p>Use the general approach for conditional image generation (concatenation)</p>

<p>The best model:</p>

<ul>
  <li>A larger diffusion model that has 387M parameters instead of 215M</li>
  <li>Use the VQ-regularized for the first stage and remove attention (non-local layer).</li>
  <li>Fine-tune the model for half at resolution \(512^2\).</li>
</ul>

<h4 id="convolutional-sampling">Convolutional Sampling</h4>

<ul>
  <li>The LDMs generalize to larger resolutions and can generate images up to the megapixel regime when evaluated in a convolutional manner.</li>
  <li>The authors exploit this behavior in semantic synthesis, super-resolution, and inpainting.</li>
</ul>

<p>The SNR induced by the variance of the latent space (i.e., \(\text{Var}(z)/\sigma_t^2\)) significantly affects the results for convolutional sampling.</p>

<ul>
  <li>For the KL-regularized model, this ratio is high, such that the model allocates a lot of semantic detail early on in the reverse denoising process.</li>
  <li>When rescaling the latent space by the component-wise standard deviation of the latents, the SNR is decreased.</li>
  <li>The VQ-regularized space has a variance close to 1, such that it does not have to be rescaled.</li>
</ul>

<h2 id="limitations">Limitations</h2>

<ul>
  <li>While LDMs significantly reduce computational requirements compared to pixel-based approaches, their sequential sampling process is still slower than that
of GANs.</li>
  <li>The use of LDMs can be questionable when high precision is required.</li>
</ul>]]></content><author><name></name></author><summary type="html"><![CDATA[High-Resolution Image Synthesis with Latent Diffusion Models]]></summary></entry><entry><title type="html">InstructGPT</title><link href="https://daviddmc.github.io/blog/2022/InstructGPT/" rel="alternate" type="text/html" title="InstructGPT" /><published>2022-01-27T00:00:00+00:00</published><updated>2022-01-27T00:00:00+00:00</updated><id>https://daviddmc.github.io/blog/2022/InstructGPT</id><content type="html" xml:base="https://daviddmc.github.io/blog/2022/InstructGPT/"><![CDATA[<h2 id="takeaways">Takeaways</h2>

<ul>
  <li>Making LMs bigger does not inherently make them better at following a user’s intent.</li>
  <li>Reinforcement learning from human feedback (<strong>RLHF</strong>) is a promising direction for aligning LM with user intent.</li>
  <li>Outputs from the 1.3B InstructGPT model are preferred by humans to outputs from the 175B GPT-3, despite having 100x fewer parameters.</li>
  <li>InstructGPT shows improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets.</li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Making LMs bigger does not inherently make them better at following a user’s intent, i.e., not <em>aligned</em> with their users.</p>

<p>The model may generate outputs that are untruthful, toxic, or not helpful.</p>

<p>This is because the LM objective used for many recent large LMs, i.e., predicting the next token on a webpage from the internet, is different from the objective “follow the user’s instructions helpfully and safely”.</p>

<p>Ideas:</p>

<ul>
  <li>Aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback.</li>
  <li>Use reinforcement learning from human feedback (<strong>RLHF</strong>) to fine-tune GPT-3<d-cite key="GPT-3"></d-cite> to follow a broad class of written instructions.</li>
  <li>This technique uses human preferences as a reward signal to fine-tune the models.</li>
</ul>

<p>Main Findings:</p>

<ul>
  <li>Labelers significantly prefer InstructGPT outputs over outputs from GPT-3</li>
  <li>InstructGPT generalizes to the preferences of “held-out” labelers.</li>
  <li>Public NLP datasets are not reflective of how our language models are used.</li>
  <li>InstructGPT models show improvements in truthfulness over GPT-3.</li>
  <li>InstructGPT shows small improvements in toxicity over GPT-3, but not bias.</li>
  <li>The performance regressions on public NLP datasets can be minimized by modifying the RLHF fine-tuning procedure.</li>
  <li>InstructGPT shows promising generalization to instructions outside of the RLHF finetuning distribution.</li>
  <li>InstructGPT still makes simple mistakes.</li>
</ul>

<h2 id="methods">Methods</h2>

<h3 id="high-level-methodology">High-Level Methodology</h3>

<ol>
  <li>Collect demonstration data, and train a supervised policy (supervised fine-tune, <strong>SFT</strong>).</li>
  <li>Collect comparison data, and train a reward model (<strong>RM</strong>).</li>
  <li>Optimize a policy against the reward model using <strong>PPO</strong>.</li>
</ol>

<p>Steps 2 and 3 can be iterated continuously.</p>

<div class="l-page" style="text-align:center;">
  <img src="https://cdn.openai.com/instruction-following/draft-20220126f/methods.svg" width="100%" style="margin-bottom: 12px; background-color: white;" />
  <p></p>
</div>

<h3 id="dataset">Dataset</h3>

<h4 id="prompt-dataset">Prompt Dataset</h4>

<ul>
  <li>Use text prompts submitted to the OpenAI API, specifically those using <em>an earlier version of the InstructGPT models</em>.</li>
  <li>Deduplicate prompts by checking for prompts that share a long common prefix.</li>
  <li>Limit the number of prompts to 200 per user ID.</li>
  <li>Create the train, validation, and test splits based on <em>user ID</em>.</li>
  <li>Filter all prompts in the training split for personally identifiable information.</li>
</ul>

<p>For each prompt, the task can be</p>

<ul>
  <li>a natural language instruction (e.g. “Write a story about a wise frog”),</li>
  <li>few-shot examples (e.g. giving two examples of frog stories, and prompting the model to generate a new one)</li>
  <li>an implicit continuation (e.g. providing the start of a story about a frog).</li>
</ul>

<h4 id="train-the-very-first-instructgpt">Train the Very First InstructGPT</h4>

<p>To train the very first InstructGPT models, the authors asked labelers to <em>write prompts themselves</em>. This is because an initial source of instruction-like prompts is needed to bootstrap the process and these kinds of prompts weren’t often submitted to the regular GPT-3 models on the API.</p>

<p>Three kinds of prompts were written by the labelers:</p>
<ul>
  <li>Plain: an arbitrary task, while ensuring the tasks had sufficient diversity.</li>
  <li>Few-shot: an instruction, and multiple query/response pairs for that instruction.</li>
  <li>User-based: a prompt corresponding to the use cases stated in waitlist applications to the OpenAI API.</li>
</ul>

<h4 id="datasets-for-fine-tuning">Datasets for Fine-Tuning</h4>

<p>Three different datasets used in the fine-tuning procedure are built from the prompt dataset.</p>

<ol>
  <li><strong>SFT:</strong> A prompt is sampled from the prompt dataset, and a labeler writes an answer to this prompt, supervised learning (13k prompts)</li>
  <li><strong>RM:</strong> A prompt and several model outputs are sampled, and a labeler ranks the outputs from the best to worst. This data is used to train the reward model. (33k prompts)</li>
  <li><strong>PPO:</strong> Another prompt dataset from the API. This data is used to train PPO with the RM. (31k prompts)</li>
</ol>

<h4 id="human-data-collection">Human Data Collection</h4>

<p>Hired a team of about 40 labelers.</p>

<p>During training and evaluation, the alignment criteria may come into conflict.</p>

<ul>
  <li>During training helpfulness to the user is prioritized</li>
  <li>In the final evaluation truthfulness and harmlessness are prioritized.</li>
</ul>

<h3 id="models">Models</h3>

<h4 id="sft">SFT</h4>

<ul>
  <li>Fine-tune GPT-3 on the labeler demonstrations using supervised learning.</li>
  <li>Select the final SFT model based on the RM score on the validation set.</li>
</ul>

<h4 id="rm">RM</h4>

<ul>
  <li>Start from the SFT model with the final unembedding layer removed.</li>
  <li>Train a model to take in a prompt and response, and output a scalar reward.</li>
</ul>

<p>Model size: The 6B RMs are used because they save computation and the training of 175B RM could be unstable.</p>

<p>Loss function:</p>

\[L(\theta)=-\frac{1}{K \choose 2}\mathbb{E}_{(x,y_w,y_l)\sim D}[\log(\sigma(r_\theta(x,y_w)- r_\theta(x,y_l)))],\]

<p>where \(r_\theta(x,y)\) is the score outputed by the RM for prompt \(x\) and completion \(y\) with parameters \(\theta\), \(y_w\) is the preferred completion out of the pair of \(y_w\) and \(y_l\), and \(D\) is the dataset of human comparisons.</p>

<h4 id="rl">RL</h4>

<p>Fine-tune the SFT model using PPO.</p>

<p>The environment is a bandit environment that presents a random customer prompt and expects a response to the prompt. Given the prompt and response, it produces a reward determined by the reward model and ends the episode.</p>

<p>In addition, a per-token KL penalty from the SFT model is added at each token to mitigate overoptimization of the reward model.</p>

<p>The value function is initialized from the RM.</p>

<p>An improved algorithm: <strong>PPO-ptx</strong></p>

<ul>
  <li>Mixing the pretraining gradients into the PPO gradients.</li>
  <li>
    <p>Maximize the following combined objective function in RL training</p>

\[\mathbb{E}_{(x,y)\sim D_{\pi_{\phi}^\text{RL}}}[r_{\theta}(x,y)-\beta\log(\pi_\phi^\text{RL}(y|x)/\pi^\text{SFT}(y|x))]+\gamma \mathbb{E}_{x\sim D_\text{pretrain}}[\log(\pi_\phi^\text{RL}(x))],\]

    <p>where \(\pi_\phi^\text{RL}\) is the learned RL policy, \(\pi^\text{SFT}\) is the supervised trained model, and \(D_\text{pretrain}\) is the pretraining distribution.</p>
  </li>
  <li>Falling back into PPO when \(\gamma=0\).</li>
</ul>

<h4 id="baselines">Baselines</h4>

<ul>
  <li>GPT-3</li>
  <li>GPT-3-prompted: provide a few-shot prefix to “prompt” GPT-3 into an instruction-following mode</li>
  <li>GPT-3-FT: fine-tune GPT-3 on a variety of NLP tasks</li>
</ul>

<h2 id="experiments">Experiments</h2>

<h3 id="evaluation">Evaluation</h3>

<h4 id="definition-of-alignment">Definition of Alignment</h4>

<ul>
  <li><strong>Helpfulness:</strong>
    <ul>
      <li>The model should follow instructions, but also infer intention from a few-shot prompt or another interpretable pattern.</li>
      <li>The main metric is labeler preference ratings.</li>
      <li>However, since the labelers are not the users who generated the prompts, there could be a divergence between what a user actually intended and what the labeler thought was intended from only reading the prompt.</li>
    </ul>
  </li>
  <li><strong>Truthfulness (honesty):</strong>
    <ul>
      <li>Whether the model’s statements about the world are true.</li>
      <li>Evaluate the model’s tendency to make up information on closed-domain tasks (“hallucinations”)</li>
      <li>Use the TruthfulQA dataset.</li>
    </ul>
  </li>
  <li><strong>Harmlessness:</strong>
    <ul>
      <li>The harms from language models depend on how their outputs are used in the real world.</li>
      <li>The labelers evaluate whether the output is inappropriate in the context of a customer assistant, denigrates a protected class, or contains.</li>
      <li>Benchmark the models on datasets intended to measure bias and toxicity.</li>
    </ul>
  </li>
</ul>

<h4 id="evaluations-on-api-distribution">Evaluations on API Distribution</h4>

<p>The main metric is <em>human preference ratings</em> on a held-out set of prompts from the same source as the training distribution.</p>

<ul>
  <li>How often the model’s outputs are preferred to a baseline policy (175B SFT model)?</li>
  <li>The overall quality of each response on a 1-7 Likert scale (given by the labelers).</li>
</ul>

<h4 id="evaluations-on-public-nlp-datasets">Evaluations on Public NLP Datasets</h4>

<ul>
  <li>Datasets that capture an aspect of LM safety, particularly truthfulness, toxicity, and bias.</li>
  <li>Datasets that capture zero-shot performance on traditional NLP tasks, e.g.,
    <ul>
      <li>question answering,</li>
      <li>reading comprehension,</li>
      <li>summarization.</li>
    </ul>
  </li>
  <li>The RealToxicityPrompts dataset for human evaluations of toxicity.</li>
</ul>

<h3 id="results-on-the-api-distribution">Results on the API Distribution</h3>

<ul>
  <li><strong>Labelers significantly prefer InstructGPT outputs over outputs from GPT-3.</strong>
    <ul>
      <li>Preference order: PPO-ptx ~ PPO &gt; SFT &gt; GPT-3 (prompted) &gt; GPT-3</li>
      <li>Outputs from the 1.3B InstructGPT are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters.</li>
      <li>Results do not change significantly when evaluated on prompts submitted to GPT-3 models on the API.</li>
      <li>InstructGPT outperforms GPT along several more concrete axes:
        <ol>
          <li>“Attempts correct instruction”</li>
          <li>“Follows explicit constraints”</li>
          <li>“Hallucinations”: making up facts</li>
          <li>“Uses language appropriate for customer assistant”</li>
        </ol>
      </li>
    </ul>
  </li>
  <li><strong>InstructGPT generalizes to the preferences of “held-out” labelers.</strong>
    <ul>
      <li>Held-out labelers (who did not produce any training data) have similar ranking preferences as workers who produce training data.</li>
    </ul>
  </li>
  <li><strong>Public NLP datasets are not reflective of how our language models are used.</strong>
    <ul>
      <li>Fine-tune GPT-3 on a variety of NLP tasks (GPT-3-FT)</li>
      <li>Likert scores ranking: PPO-ptx &gt; SFT &gt; GPT-3 (prompted) ~ GPT-3-FT &gt; GPT-3</li>
      <li>InstructGPT model outperforms GPT-3-FT for two reasons.
        <ol>
          <li>Public NLP datasets are designed to capture tasks that are easy to evaluate with automatic metrics, whereas more than half of the prompts in the API distribution are open-ended generation and brainstorming.</li>
          <li>It can be difficult for public NLP datasets to obtain a very high diversity of inputs.</li>
        </ol>
      </li>
    </ul>
  </li>
</ul>

<h3 id="results-on-public-nlp-datasets">Results on Public NLP Datasets</h3>

<ul>
  <li><strong>InstructGPT models show improvements in truthfulness over GPT-3.</strong>
    <ul>
      <li>InstructGPT generates truthful and informative answers more often than GPT-3.</li>
      <li>InstructGPT does not have to be specifically instructed to tell the truth to exhibit improved truthfulness.</li>
      <li>InstructGPT hallucinate (i.e. fabricate information) less often than GPT-3 on closed-domain tasks from our API distribution.</li>
    </ul>
  </li>
  <li><strong>InstructGPT shows small improvements in toxicity over GPT-3, but not bias.</strong>
    <ul>
      <li>Obtain automatic toxicity scores of models outputs using  <a href="https://www.perspectiveapi.com">the Perspective API</a></li>
      <li>Human evaluation: absolute toxicity, toxicity relative to the prompt, continuity, and overall output preference.</li>
      <li>When instructed to produce a safe and respectful output, InstructGPT models generate less toxic outputs than those from GPT-3. This advantage disappears when the respectful prompt is removed.</li>
      <li>When explicitly prompted to produce a toxic output, InstructGPT outputs are much more toxic than those from GPT-3.</li>
      <li>In terms of the propensity to generate biased speech, InstructGPT is <em>not</em> less biased than GPT-3. But when instructed to act respectfully InstructGPT exhibits higher bias.</li>
    </ul>
  </li>
  <li><strong>The performance regressions on public NLP datasets can be minimized by modifying the RLHF fine-tuning procedure.</strong>
    <ul>
      <li>InstructGPT (PPO model trained on the API distribution) suffers from performance regressions on several public NLP datasets (alignment tax).</li>
      <li>Adding pretraining updates (PPO-ptx) mitigates these performance regressions on all datasets.</li>
      <li>Mixing in pretraining updates performs better than the simpler solution of increasing the KL coefficient.</li>
    </ul>
  </li>
</ul>

<h3 id="qualitative-results">Qualitative results</h3>

<ul>
  <li><strong>InstructGPT shows promising generalization to instructions outside of the RLHF finetuning distribution.</strong>
    <ul>
      <li>InstructGPT shows the ability to follow instructions in non-English languages, and perform summarization and question-answering for code, although non-English languages and code form a tiny minority of the fine-tuning data.</li>
    </ul>
  </li>
  <li><strong>InstructGPT still makes simple mistakes.</strong>
    <ul>
      <li>When given an instruction with a false premise, the model sometimes incorrectly assumes the premise is true.</li>
      <li>The model can overly hedge; when given a simple question, it can sometimes say that there is no one answer to the question and give multiple possible answers, even when there is one fairly clear answer from the context.</li>
      <li>The model’s performance degrades when instructions contain multiple explicit constraints or when constraints can be challenging for language models.</li>
    </ul>
  </li>
</ul>

<h2 id="discussion">Discussion</h2>

<h3 id="implications-for-alignment-research">Implications for Alignment Research</h3>

<p>This research is part of a broader research program to align AI systems with human intentions.</p>

<p>Lessons for alignment research more generally:</p>

<ul>
  <li>The cost of increasing model alignment is modest relative to pretraining.</li>
  <li>There is some evidence that InstructGPT generalizes “following instructions” to settings that people don’t supervise it in.</li>
  <li>Most of the performance degradations introduced by the fine-tuning can be mitigated.</li>
  <li>This work has validated alignment techniques from research in the real world.</li>
</ul>

<h3 id="who-are-we-aligning-to">Who are We Aligning to?</h3>

<p>Factors that influence the fine-tuning data that ultimately determine what and who the models are aligning to.</p>

<ul>
  <li>Aligning to demonstrations and preferences provided by our training labelers.</li>
  <li>Aligning to the preferences of the researchers who designed this study.</li>
  <li>Aligning to what customers think is valuable and what their end-users think is valuable to currently use the API for.</li>
</ul>

<h3 id="limitations">Limitations</h3>

<h4 id="limitations-of-methodology">Limitations of Methodology</h4>

<ul>
  <li>The behavior of InstructGPT models is determined in part by the human feedback obtained from the labelers.</li>
  <li>Some of the labeling tasks rely on value judgments that may be impacted by the identity of the labelers.</li>
  <li>Most comparisons are only labeled by 1 labeler for cost reasons.</li>
</ul>

<h4 id="limitations-of-models">Limitations of Models</h4>

<ul>
  <li>The models are neither fully aligned nor fully safe: still generate toxic or biased outputs, make up facts, and generate sexual and violent content without explicit prompting.</li>
  <li>The model also fails to generate reasonable outputs on some inputs.</li>
  <li>In most cases, the model follows the user’s instruction, even if that could lead to harm in the real world.</li>
</ul>

<h3 id="open-questions">Open Questions</h3>

<ul>
  <li>Further decrease the models’ propensity to generate toxic, biased, or otherwise harmful outputs.</li>
  <li>Training the model to be harmless despite user instructions is important but is also difficult because whether an output is harmful depends on the context in which it’s deployed.</li>
  <li>A promising future path is combining RLHF with other methods of steerability.</li>
  <li>In addition to RLHF, there are many other algorithms that could be used to train policies on the demonstration and comparison data to get even better results.</li>
  <li>Comparisons are not necessarily the most efficient way of providing an alignment signal.</li>
  <li>The proposal for mitigating the alignment tax, by incorporating pretraining data into RLHF finetuning (PPO-ptx), does not completely mitigate performance regressions and may make certain undesirable behaviors more likely for some tasks.</li>
  <li>A principle-based approach to alignment, i,e, identifying “fair principles” for alignment.</li>
</ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Training language models to follow instructions with human feedback]]></summary></entry><entry><title type="html">CLIP: Contrastive Language-Image Pre-training</title><link href="https://daviddmc.github.io/blog/2021/CLIP/" rel="alternate" type="text/html" title="CLIP: Contrastive Language-Image Pre-training" /><published>2021-02-26T00:00:00+00:00</published><updated>2021-02-26T00:00:00+00:00</updated><id>https://daviddmc.github.io/blog/2021/CLIP</id><content type="html" xml:base="https://daviddmc.github.io/blog/2021/CLIP/"><![CDATA[<h2 id="takeaways">Takeaways</h2>

<ul>
  <li>CLIP consists of an image encoder (ResNet or ViT) and a text encoder (Transformer).</li>
  <li>CLIP is pre-trained to predict which caption goes with which image using <strong>contrastive learning</strong>.</li>
  <li>CLIP enables <strong>zero-shot transfer</strong> to other image classification task (using the class name as input text).</li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>The recent development of modern pre-training methods in NLP (e.g., T5<d-cite key="T5"></d-cite>, GPT-3<d-cite key="GPT-3"></d-cite>) suggests that the aggregate supervision within web-scale collections of text surpasses that of high-quality crowd-labeled NLP datasets.</p>

<p>Using <strong>natural language supervision</strong> for image representation:</p>

<ul>
  <li>Exciting as proofs of concept, but is still rare. This is likely because the performance on common benchmarks is much lower than SOTA.</li>
  <li>This line of work represents the middle ground between learning from a limited amount of supervised “gold labels” and learning from practically unlimited amounts of raw text.</li>
  <li>Compromise: use static softmax classifiers (fixed # of output classes) and lack a mechanism for dynamic outputs, which limits their “zero-shot” capabilities.</li>
</ul>

<p>In this work, the authors</p>

<ul>
  <li>create a new dataset of <strong>400 million (image, text) pairs</strong>;</li>
  <li>demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch;</li>
  <li>transfer the model to downstream tasks in a <strong>zero-shot</strong> manner.</li>
</ul>

<h2 id="methods">Methods</h2>

<div class="l-page" style="text-align:center;">
  <img src="https://raw.githubusercontent.com/openai/CLIP/main/CLIP.png" width="100%" style="margin-bottom: 12px; background-color: white;" />
  <p>Summary of CLIP.</p>
</div>

<h3 id="natural-language-supervision">Natural Language Supervision</h3>

<p>At the core of CLIP is the idea of learning perception from supervision contained in natural language.</p>

<p>Learning from natural language has several potential strengths over other training methods.</p>

<ul>
  <li>Easier to scale natural language supervision compared to standard crowd-sourced labeling for image classification</li>
  <li>Learning from natural language also has an important advantage over most unsupervised or self-supervised learning approaches in that it doesn’t “just” learn a representation but also connects that representation to language which enables flexible zero-shot transfer.</li>
</ul>

<h3 id="dataset">Dataset</h3>

<p>Create a new dataset: WebImageText (WIT)</p>

<ul>
  <li><strong>400M image-text pairs</strong>,</li>
  <li>Collect from publicly available sources on the Internet,</li>
  <li>Search for (image, text) pairs whose text includes one of a set of 500,000 queries (frequent words in Wikipedia),</li>
  <li>Class balance the results by including up to 20,000 (image, text),</li>
  <li>The text data has a similar word count as the WebText dataset used to train GPT-2<d-cite key="GPT-2"></d-cite>.</li>
</ul>

<h3 id="efficient-pre-training-method">Efficient Pre-Training Method</h3>

<h4 id="initial-approach">Initial Approach</h4>

<p>Jointly train an image CNN and text transformer from scratch to predict the caption of an image.</p>

<h4 id="problem">Problem</h4>

<p>This approach learns to recognize ImageNet classes three times slower than a much simpler baseline that predicts a bag-of-words encoding of the same text.</p>

<h4 id="reason">Reason</h4>

<p>The models try to predict the <em>exact words</em> of the text accompanying each image. This is a <em>difficult task</em> due to the wide variety of descriptions, comments, and related text that co-occur with images.</p>

<h4 id="solution-contrastive-representation-learning">Solution: Contrastive Representation Learning</h4>

<p>Given a batch of \(N\) (image, text) pairs, CLIP is trained to predict which of the \(N\times N\) possible (image, text) pairings across a batch actually occurred.</p>

<p>To do this, CLIP learns a multi-modal embedding space by jointly training an image encoder and text encoder to maximize the cosine similarity of the image and text embeddings of the \(N\) real pairs in the batch while minimizing the cosine similarity of the embeddings of the \(N^2 - N\) incorrect pairings (see the pseudocode below).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># image_encoder - ResNet or Vision Transformer
</span>  <span class="c1"># text_encoder - CBOW or Text Transformer
</span>  <span class="c1"># I[n, h, w, c] - minibatch of aligned images
</span>  <span class="c1"># T[n, l] - minibatch of aligned texts
</span>  <span class="c1"># W_i[d_i, d_e] - learned proj of image to embed
</span>  <span class="c1"># W_t[d_t, d_e] - learned proj of text to embed
</span>  <span class="c1"># t - learned temperature parameter
</span>  <span class="c1"># extract feature representations of each modality
</span>  <span class="n">I_f</span> <span class="o">=</span> <span class="nf">image_encoder</span><span class="p">(</span><span class="n">I</span><span class="p">)</span> <span class="c1">#[n, d_i]
</span>  <span class="n">T_f</span> <span class="o">=</span> <span class="nf">text_encoder</span><span class="p">(</span><span class="n">T</span><span class="p">)</span> <span class="c1">#[n, d_t]
</span>  <span class="c1"># joint multimodal embedding [n, d_e]
</span>  <span class="n">I_e</span> <span class="o">=</span> <span class="nf">l2_normalize</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">I_f</span><span class="p">,</span> <span class="n">W_i</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">T_e</span> <span class="o">=</span> <span class="nf">l2_normalize</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">T_f</span><span class="p">,</span> <span class="n">W_t</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="c1"># scaled pairwise cosine similarities [n, n]
</span>  <span class="n">logits</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">I_e</span><span class="p">,</span> <span class="n">T_e</span><span class="p">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
  <span class="c1"># symmetric loss function
</span>  <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
  <span class="n">loss_i</span> <span class="o">=</span> <span class="nf">cross_entropy_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">loss_t</span> <span class="o">=</span> <span class="nf">cross_entropy_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss_i</span> <span class="o">+</span> <span class="n">loss_t</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
</code></pre></div></div>

<p>CLIP was trained from scratch without initializing the image encoder with ImageNet weights or the text encoder with pre-trained weights.</p>

<p>Only used linear projection to map image/text representation to the multi-model embedding space (instead of non-linear projection)</p>

<h4 id="model">Model</h4>

<ul>
  <li><strong>Image encoder:</strong> ResNet / ViT</li>
  <li><strong>Text encoder:</strong> Transformer<d-cite key="Transformer"></d-cite></li>
</ul>

<h2 id="experiments">Experiments</h2>

<h3 id="zero-shot-transfer">Zero-Shot Transfer</h3>

<h4 id="zero-shot-classification">Zero-Shot Classification</h4>

<p>Using class name as input text and predict the most probable pair (maximizing cosine similarity)</p>

<p>Results:</p>

<ul>
  <li>CLIP outperforms Visual N-Grams.</li>
  <li>The best <strong>zero-shot</strong> CLIP model achieves an accuracy of 76.2% on ImageNet, matching the performance of the original <strong>supervised</strong> ResNet-50.</li>
</ul>

<h4 id="prompt-engineering">Prompt Engineering</h4>

<p>The input texts during training are sentences while the label of an image is just a word. To help bridge this distribution gap, the authors use the prompt template <code class="language-plaintext highlighter-rouge">"A photo of a {label}"</code>. Also, zero-shot performance can be significantly improved by customizing the prompt text for each task.</p>

<h3 id="representation-learning">Representation Learning</h3>

<p>Methods to evaluate the quality of learned representation:</p>

<ol>
  <li>Fitting a linear classifier on the representation extracted from the model and measuring its performance on various datasets.</li>
  <li>Fine-tunning end-to-end</li>
</ol>

<p>The first method is used in CLIP since fine-tuning end-to-end would change the representation and potentially mask some failures.</p>

<h3 id="robustness-to-natural-distribution-shift">Robustness to Natural Distribution Shift</h3>

<p>DL models are exceedingly adept at finding correlations and patterns which hold across their training dataset and thus improve in-distribution performance. However many of these correlations and patterns are actually spurious and do not hold for other distributions and result in large drops in performance on other datasets.</p>

<ul>
  <li>
    <p><em>Effective robustness</em> measures improvements in accuracy under distribution shift above what is predicted by the documented relationship between in-distribution and out-of-distribution accuracy.</p>
  </li>
  <li>
    <p><em>Relative robustness</em> captures any improvement in out-of-distribution accuracy.</p>
  </li>
</ul>

<h4 id="zero-shot">Zero-Shot</h4>

<p>Intuitively, a zero-shot model should not be able to exploit spurious correlations or patterns that hold only on a specific distribution, since it is not trained on that distribution.</p>

<p>Results show that zero-shot models can be much more robust, however, they do not necessarily mean that supervised learning on ImageNet causes a robustness gap. Other details of CLIP, such as its large and diverse pre-training dataset or use of natural language supervision could also result in much more robust models regardless of whether they are zero-shot or fine-tuned.</p>

<h4 id="fine-tune-on-imagenet">Fine-Tune on ImageNet</h4>

<p>Although adapting CLIP to the ImageNet distribution increases its ImageNet accuracy by 9.2% to 85.4% overall, and ties the accuracy of the 2018 SOTA, <em>average accuracy under distribution shift slightly decreases</em>.</p>

<h4 id="flexible-classes">Flexible Classes</h4>

<p>The target classes across the transfer datasets are not always perfectly aligned with those of ImageNet. With CLIP we can instead generate a custom zero-shot classifier for each dataset directly based on its class names.</p>

<p>Results: This improves average effective robustness by 5% but is concentrated in large improvements on only a few datasets.</p>

<h4 id="few-shot">Few-Shot</h4>

<ul>
  <li>Few-shot CLIP also increases effective robustness compared to existing ImageNet models.</li>
  <li>But few-shot CLIP is less robust than zero-shot CLIP.</li>
  <li>Minimizing the amount of ImageNet training data used for adaption increases effective robustness at the cost of decreasing relative robustness.</li>
</ul>

<h2 id="limitations">Limitations</h2>

<ul>
  <li>The zero-shot CLIP is on average competitive with the simple <em>supervised</em> baseline of a <em>linear classifier</em> on top of ResNet-50 features, and is well below the overall SOTA. Significant work is still needed to improve the task-learning and transfer capabilities of CLIP.</li>
  <li>The performance of CLIP is poor on fine-grained classification (e.g., differentiating models of cars), abstract and systematic tasks (e.g., number counting), and novel tasks which are unlikely to be included in the pre-training dataset.</li>
  <li>The zero-shot CLIP still generalizes poorly to data that is truly out-of-distribution for it. This suggests CLIP does little to address the underlying problem of brittle generalization of deep learning models. Instead, CLIP tries to circumvent the problem and hopes that by training on such a large and varied dataset that all data will be effectively in-distribution.</li>
  <li>CLIP is still limited to choosing from only those concepts in a given zero-shot classifier. This is a significant restriction compared to a truly flexible approach like image captioning which could generate novel outputs.</li>
  <li>CLIP also does not address the poor data efficiency of deep learning.</li>
  <li>Despite our focus on zero-shot transfer, we repeatedly queried performance on full validation sets to guide the development of CLIP.</li>
  <li>Many complex tasks and visual concepts can be difficult to specify just through text.</li>
  <li>CLIP does not optimize for few-shot performance directly, resulting in a counter-intuitive drop in performance when transitioning from a zero-shot to a few-shot setting. This is different from the human performance which shows a large increase from a zero to a one-shot setting.</li>
</ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Learning Transferable Visual Models From Natural Language Supervision]]></summary></entry><entry><title type="html">DDIM</title><link href="https://daviddmc.github.io/blog/2020/DDIM/" rel="alternate" type="text/html" title="DDIM" /><published>2020-10-06T00:00:00+00:00</published><updated>2020-10-06T00:00:00+00:00</updated><id>https://daviddmc.github.io/blog/2020/DDIM</id><content type="html" xml:base="https://daviddmc.github.io/blog/2020/DDIM/"><![CDATA[<h2 id="takeaways">Takeaways</h2>

<ul>
  <li>This work generalizes DDPMs<d-cite key="DDPM"></d-cite> via a class of <strong>non-Markovian</strong> diffusion processes that lead to the same training objective.</li>
  <li>These non-Markovian processes interpolate between the original DDPMs and implicit models (DDIM) that have deterministic generative processes.</li>
  <li>With the same training procedure as DDPMs, this work provides a more efficient way for sampling by only considering a subsequence of latent variables.</li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Problems with DDPMs:</p>

<ul>
  <li>DDPMs require many iterations (~1000, the same as the number of forward steps) to produce a high-quality sample.</li>
  <li>The DDPM objective only depends on the “marginals” \(q(x_t\vert x_0)\), but not directly on the “joint” \(q(x_{1:T}\vert x_0)\). There are many inference distributions (joints) with the same marginals</li>
</ul>

<p>Ideas:</p>

<ul>
  <li>Explore non-Markovian inference processes, for which we are still able to design suitable reverse generative Markov chains.</li>
  <li>Show that the resulting variational training objectives have a shared surrogate objective, which is exactly the objective used to train DDPM.</li>
  <li>The shared objective allows us to choose from a large family of generative models using the same neural network simply by choosing a different, non-Markovian diffusion process.</li>
  <li>We are able to use non-Markovian diffusion processes which lead to “short” generative Markov chains that can be simulated in a small number of steps.</li>
</ul>

<h2 id="methods">Methods</h2>

<h3 id="non-markovian-forward-processes">Non-Markovian Forward Processes</h3>

<p>Define a family \(\mathcal{Q}\) of (inference) distributions, indexed by vector \(\sigma\in\mathbb{R}^T_{\ge0}\):</p>

\[q_\sigma(x_{1:T}\vert x_0):=q_\sigma(x_T\vert x_0)\prod_{t=2}^T q_\sigma(x_{t-1}\vert x_t,x_0)\]

<p>where</p>

\[\begin{aligned}
q_\sigma(x_T\vert x_0)&amp;=\mathcal{N}(\sqrt{\alpha_T}x_0,(1-\alpha_T)I),\\
q_\sigma(x_{t-1}\vert x_t, x_0)&amp;=\mathcal{N}(\sqrt{\alpha_{t-1}}x_0+\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot\frac{x_t-\sqrt{\alpha_t}x_0}{\sqrt{1-\alpha_t}},\sigma_t^2I)\quad\text{for all } t&gt;1.
\end{aligned}\]

<p>Remarks:</p>

<ol>
  <li>The mean is chosen so that \(q_\sigma(x_t\vert x_0)=\mathcal{N}(\sqrt{\alpha_t}x_0,(1-\alpha_t)I)\) (see <a href="#the-inference-distribution">Appendix</a>).</li>
  <li>The joint is factorized in reverse order.</li>
  <li>The forward process \(q_\sigma(x_t\vert x_{t-1}, x_0)\) can be derived from Bayes’rule, which is also Gaussian, but is non-Markovian in comparison to DDPM<d-cite key="DDPM"></d-cite>.</li>
  <li>The variance \(\sigma_t^2\) of \(q_\sigma(x_{t-1}\vert x_t, x_0)\) is a hyperparameter that can be choosen. In contrast, the variance of \(q(x_{t-1}\vert x_t, x_0)\) in DDPM is determined by \(\alpha\) due to the Markovian model.</li>
</ol>

<h3 id="generative-process">Generative Process</h3>

<p>Define a trainable generative process</p>

\[p_\theta(x_{0:T}):=p_\theta(x_T)\prod_{i=1}^Tp_\theta^{(t)}(x_{t-1}\vert x_t),\]

<p>where each \(p_\theta^{(t)}(x_{t-1}\vert x_t)\) leverages knowledge of \(q_\sigma(x_{t-1}\vert x_t, x_0)\).</p>

<ol>
  <li>Given a noisy observation \(X_t\), e.g., \(X_t=\sqrt{\alpha_t}X_0+\sqrt{1-\alpha_t}\epsilon_t\) with \(X_0\sim q(x_0)\) and \(\epsilon_t\sim\mathcal{N}(0,I)\).</li>
  <li>
    <p>Make a prediction of the corresponding \(X_0\): The model \(\epsilon_\theta^{(t)}(x_t)\) predicts \(\epsilon_t\) from \(X_t\), without knowing \(X_0\). Then we can predict the <em>denoised observation</em>, which is a prediction of \(X_0\) given \(X_t\),</p>

\[f_\theta^{(t)}(x_t):=\frac{1}{\sqrt{\alpha_t}}(x_t-\sqrt{1-\alpha_t}\epsilon_\theta^{(t)}(x_t)).\]
  </li>
  <li>
    <p>Use the prediction to obtain to sample \(X_{t-1}\) from the reverse conditional distribution \(q_\sigma(x_{t-1}\vert x_t,x_0)\): we can define the generative process with a fixed prior \(p_\theta(x_T)=\mathcal{N}(0,I)\) and</p>

\[p_\theta^{(t)}(x_{t-1}\vert x_t)=\left\{
 \begin{aligned}
 &amp;\mathcal{N}(f_\theta^{(1)}(x_1),\sigma_1^2 I)\quad&amp;\text{if }t=1,\\
 &amp;q_\sigma(x_{t-1}\vert x_t,f_\theta^{(t)}(x_t))\quad&amp;\text{if }t\ge1,
 \end{aligned}
 \right.\]

    <p>where Gaussian noise is added to the case of \(t=1\) to ensure that the generative process is supported everywhere.</p>
  </li>
</ol>

<p>Remarks: This generative process is basically the same as DDPM with some minor differences</p>

<ul>
  <li>This work uses \(q_\sigma(x_{t-1}\vert x_t,x_0)\) while DDPM uses \(q(x_{t-1}\vert x_t, x_0)\).</li>
  <li>This work uses the same variance \(\sigma_t^2\) for \(q_\sigma(x_{t-1}\vert x_t,x_0)\) and \(p_\theta^{(t)}(x_{t-1}\vert x_t)\) while DDPM introduce \(\sigma_t^2\) in \(p_\theta^{(t)}(x_{t-1}\vert x_t)\), which might be different from the \(\tilde\beta_t\) in \(q(x_{t-1}\vert x_t, x_0)\).</li>
</ul>

<h3 id="unified-variational-inference-objective">Unified Variational Inference Objective</h3>

<p>The parameters \(\theta\) are optimized via the variational inference objective</p>

\[J_\sigma(\epsilon_\theta):=\mathbb{E}_{q_\sigma}[\log q_\sigma(X_{1:T}\vert X_0)-\log p_\theta(X_{0:T})]\]

<p>In comparison, DDPM optimizes the following objective:</p>

\[L_\gamma(\epsilon_\theta):=\sum_{t=1}^T\gamma_t \mathbb{E}_{X_0,\epsilon_t}\left[\|\epsilon_\theta^{(t)}(\sqrt{\alpha_t}X_0+\sqrt{1-\alpha_t}\epsilon_t)-\epsilon_t\|_2^2\right],\]

<p>where \(\gamma\in\mathbb{R}^T_{&gt;0}\) is a vector of positive coefficients in the objective that depends on \(\alpha_{1:T}\). In DDPM The objective with \(\gamma=1\) is optimized instead to maximize the generation performance of the trained model.</p>

<p><strong>Theorem 1.</strong> For all \(\sigma\in\mathbb{R}^T_{&gt;0}\), there exists \(\gamma\in\mathbb{R}^T_{&gt;0}\) and \(C\in\mathbb{R}\), such that \(J_\sigma=L_\gamma+C\). (see <a href="#proof-of-theorem-1">Appendix</a> for the proof)</p>

<p>Discussion:</p>

<ul>
  <li>If parameters \(\theta\) are not shared across different \(t\), the optimal solution to \(L_\gamma\) will not depend on the weights \(\gamma\) as  global
optimum is achieved by separately maximizing each term in the sum</li>
  <li>This property justified the use of \(L_1\) (i.e., \(\gamma=1\)) as a surrogate objective function for the variational lower bound in DDPMs.</li>
  <li>Since \(J_\sigma\) is equivalent to some \(L_\gamma\) from Theorem 1, the optimal solution of \(J\sigma\) is also the same as that of \(L_1\).</li>
  <li>Therefore, if parameters are not shared across \(t\), then the \(L_1\) objective used by DDPMs can be used as a surrogate objective for the variational objective \(J_\sigma\) as well.</li>
</ul>

<h3 id="sampling-from-generalized-generative-processes">Sampling from Generalized Generative Processes</h3>

<p>With \(L_1\) as the objective (\(\sigma\) does not appear in the loss), we are not only learning a generative process for the Markovian inference process considered DDPM, but also generative processes for many non-Markovian forward processes parametrized by \(\sigma\) that described above.</p>

<p>Use pre-trained DDPM models as the solutions to the new objectives, and focus on finding a generative process that is better at producing samples subject to our needs by changing \(\sigma\).</p>

<h4 id="denoising-diffusion-implicit-models">Denoising Diffusion Implicit Models</h4>

<p>Generate a sample \(x_{t-1}\) from a sample \(x_t\):</p>

\[x_{t-1}=
\underbrace{\sqrt{\alpha_{t-1}}\left(\frac{x_t-\sqrt{1-\alpha_t}\epsilon_\theta^{(t)}(x_t)}{\sqrt{\alpha_t}}\right)}_{\text{predicted }x_0}
+
\underbrace{\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot\epsilon_\theta^{(t)}(x_t)}_{\text{direction pointing to }x_t}
+
\underbrace{\sigma_t\epsilon_t}_\text{random noise}\]

<p>where \(\epsilon_t\sim\mathcal{N}(0, I)\). Different choices of \(\sigma\) result in different generative processes, all while using the same model \(\epsilon_\theta\), so re-training the model is unnecessary.</p>

<ul>
  <li>DDPM
    <ul>
      <li>Set \(\sigma_t=\sqrt{(1-\alpha_{t-1})/(1-\alpha_t)}\sqrt{1-\alpha_t/\alpha_{t-1}}\).</li>
      <li>The forward process becomes Markovian.</li>
    </ul>
  </li>
  <li>DDIM (denoising diffusion implict model)
    <ul>
      <li>Set \(\sigma_t=0\) for all \(t\).</li>
      <li>The forward process becomes deterministic given \(x_{t-1}\) and \(x_{0}\).</li>
      <li>Samples are generated from latent variables with a fixed procedure (from \(x_T\) to \(x_0\)).</li>
    </ul>
  </li>
</ul>

<h4 id="accelerated-generation-processes">Accelerated Generation Processes</h4>

<p>The generative process is considered as the approximation to the reverse
process, and therefore, they should have the same number of time steps \(T\).</p>

<p>However, as \(L_1\) does not depend on the specific forward procedure as long as \(q_\sigma(x_t\vert x_0)\) is fixed, we may also consider forward processes with lengths smaller than \(T\), which accelerates the corresponding generative processes without having to train a different model.</p>

<ol>
  <li>Consider a subset \(\{X_{\tau_1},\dots X_{\tau_S}\}\), where \(\tau\) is an increasing sub-sequence of \([1,\dots, T]\) of length \(S\).</li>
  <li>Define the a forward process over \(X_\tau\) such that \(q(x_{\tau_i}\vert x_0)=\mathcal{N}(\sqrt{\alpha_{\tau_i}}x_0, (1-\alpha_{\tau_i})I)\) matches the “marginals”.</li>
  <li>The generative process now sampled latent variable according to reversed \(\tau\) (<em>sampling trajectory</em>).</li>
</ol>

<p>Details can be found in the <a href="#accelerated-sampling-processes">Appendix</a></p>

<p>Insight:</p>

<ul>
  <li>In principle, we can train a model with an arbitrary number of forward steps but only sample from some of them in the generative process.</li>
  <li>Therefore, the trained model could consider many more steps or even a continuous time variable \(t\)</li>
</ul>

<h4 id="relevance-to-neural-odes">Relevance to Neural ODEs</h4>

<p>The DDIM iterate (i.e., \(\sigma_t=0\)):</p>

\[x_{t-1}=
\sqrt{\alpha_{t-1}}\left(\frac{x_t-\sqrt{1-\alpha_t}\epsilon_\theta^{(t)}(x_t)}{\sqrt{\alpha_t}}\right)
+
\sqrt{1-\alpha_{t-1}}\cdot\epsilon_\theta^{(t)}(x_t)\]

<p>can be rewritten as</p>

\[\frac{x_{t-\Delta t}}{\sqrt{\alpha_{t-\Delta t}}}=\frac{x_t}{\sqrt{\alpha_t}}+\left(\sqrt{\frac{1-\alpha_{t-\Delta t}}{\alpha_{t-\Delta t}}}-\sqrt{\frac{1-\alpha_t}{\alpha_t}}\right)\epsilon_\theta^{(t)}(x_t)\]

<p>We can reparameterize \(\sqrt{(1-\alpha)/\alpha}\) with \(\omega\) and \(x/\sqrt{\alpha}\) with \(\bar{x}\). When \(\Delta t\rightarrow 0\), \(\omega\) and \(\bar{x}\) are functions of \(t\), where \(\omega\) is continous, increasing with \(\omega(0)=0\). The above iteration can be treated as an Euler method over the following ODE:</p>

\[\text{d}\bar{x}(t)=\epsilon^{(t)}_\theta\left(\frac{\bar{x}(t)}{\sqrt{\omega^2+1}}\right)\text{d}\omega(t),\]

<p>where the initial conditions is \(\bar{x}(T)=x(T)/\sqrt{\alpha(T)}\sim\mathcal{N}(0,1/\alpha(T))\). Since \(\alpha(T)\approx 0\), The variance \(1/\alpha(T)\) would be very large.</p>

<h2 id="experiments">Experiments</h2>

<p>Key results:</p>

<ul>
  <li>DDIMs outperform DDPMs in terms of image generation <em>when fewer iterations are considered</em>, giving speed-ups of 10x to 100x over the original DDPM generation process.</li>
  <li>Unlike DDPMs, once the initial latent variables \(x_T\) are fixed, DDIMs retain high-level image features regardless of the generation trajectory (different sub-sequences), so they are able to perform interpolation directly from the latent space.</li>
  <li>DDIMs can also be used to encode samples that reconstruct them from the latent code, which DDPMs cannot do due to the stochastic sampling process.</li>
</ul>

<p>Setup:</p>

<ul>
  <li>Use the <strong>same trained model</strong>
    <ul>
      <li>number of time steps \(T=1000\)</li>
      <li>trained with \(L_1\)</li>
    </ul>
  </li>
  <li><strong>The only change</strong> is how to produce samples from the model by controlling
    <ul>
      <li>how fast the samples are obtained, \(τ\)</li>
      <li>and sample variance \(\sigma_t^2\), which interpolates between the deterministic DDIM and the stochastic DDPM.
\(\sigma_{\tau_i}(\eta)=\eta\sqrt{\frac{1-\alpha_{\tau_{i-1}}}{1-\alpha_{\tau_i}}}\sqrt{1-\frac{\alpha_{\tau_i}}{\alpha_{\tau_{i-1}}}}\)
where \(\eta&gt;0\) is a hyperparameter. This includes DDPM (\(\eta=1\)), DDIM (\(\eta=0\)), and DDPM with larger variance (denoted as \(\hat{\sigma}:\hat{\sigma}_{\tau_i}=\sqrt{1-\alpha_{\tau_i}/\alpha_{\tau_{i-1}}}\)).</li>
    </ul>
  </li>
</ul>

<h3 id="sample-quality-and-efficiency">Sample Quality and Efficiency</h3>

<p>Vary the number of timesteps used to generate a sample (\(S=\text{dim}(\tau)\)) and the stochasticity of the process \(\eta\), and present a tradeoff between sample quality and computational costs.</p>

<p>Results:</p>

<ul>
  <li>DDIM (\(\eta=0\)) achieves the best sample quality when \(S\) is small.</li>
  <li>DDPM (\(\eta=1\) and \(\hat{\sigma}\)) typically has worse sample quality compared to its less stochastic counterparts with the same \(S\). (when \(S&lt;T\))</li>
  <li>In the case with \(S=T=1000\), DDPM (\(\hat{\sigma}\)) is better than DDIM.</li>
  <li>The sample quality of DDPM (\(\hat{\sigma}\)) becomes much worse for smaller \(S\), which suggests that it is ill-suited for shorter trajectories.</li>
  <li>DDIM achieves high sample quality much more consistently.</li>
  <li>DDIM is able to produce samples with quality comparable to 1000 step models within 20 to 100 steps.</li>
</ul>

<h3 id="sample-consistency-in-ddims">Sample Consistency in DDIMs</h3>

<p>For DDIM, the generative process is deterministic, and \(x_0\) would depend only on the initial state \(x_T\).</p>

<p>Compare generated images under different generative trajectories (i.e. different \(\tau\)) while starting with the same initial \(x_T\)</p>

<p>Results:</p>

<ul>
  <li>For the generated images with the same initial \(x_T\), most high-level features are similar, regardless of the generative trajectory.</li>
  <li>It indicates that \(x_T\) alone would be an informative latent encoding of the image.</li>
  <li>Minor details that affect sample quality are encoded in the parameters.</li>
</ul>

<h3 id="interpolation-in-deterministic-generative-processes">Interpolation in Deterministic Generative Processes</h3>

<p>Since the high-level features of the DDIM sample are encoded by \(x_T\), it might be used for semantic interpolation.</p>

<p>This is different from the interpolation procedure in DDPM, where the same \(x_T\) would lead to highly diverse \(x_0\) due to the stochastic generative process</p>

<p>DDIM is able to control the generated images on a high level directly through the latent variables, which DDPMs cannot.</p>

<h3 id="reconstruction-from-latent-space">Reconstruction from Latent Space</h3>

<p>As DDIM is the Euler integration for a particular ODE, it should be able to encode from \(x_0\) to \(x_T\) (reverse of the ODE) and reconstruct \(x_0\) from the resulting \(x_T\) (forward of the ODE).</p>

<p>Results: DDIMs have lower reconstruction error for larger \(S\) and have properties similar to Neural ODEs and normalizing flows. The same cannot be said for DDPMs due to their stochastic nature.</p>

<h2 id="appendix">Appendix</h2>

<h3 id="comparision-between-notations-in-ddpm-and-ddim">Comparision between Notations in DDPM and DDIM</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">meaning</th>
      <th style="text-align: center">DDPM<d-cite key="DDPM"></d-cite></th>
      <th style="text-align: center">DDIM (this work)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">diffusion rate</td>
      <td style="text-align: center">\(\beta_t\)</td>
      <td style="text-align: center">\(1-\alpha_t/\alpha_{t-1}\)</td>
    </tr>
    <tr>
      <td style="text-align: center">1-diffusion rate</td>
      <td style="text-align: center">\(\alpha_t\)</td>
      <td style="text-align: center">\(\alpha_t/\alpha_{t-1}\)</td>
    </tr>
    <tr>
      <td style="text-align: center">product of 1-diffusion rate</td>
      <td style="text-align: center">\(\overline{\alpha}_t\)</td>
      <td style="text-align: center">\(\alpha_t\)</td>
    </tr>
  </tbody>
</table>

<h3 id="marginal-and-conditional-gaussians">Marginal and Conditional Gaussians</h3>

<p>The materials in this section are from Pattern Recognition and Machine Learning (Bishop, 2006)  Section 2.3.3.</p>

<p>Given a marginal Gaussian distribution for \(x\) and a conditional Gaussian distribution for \(y\) given \(x\) in the form</p>

\[\begin{aligned}
p(x)&amp;=\mathcal{N}(x;\mu,\Lambda^{-1})\\
p(y\vert x)&amp;=\mathcal{N}(y; Ax+b,L^{-1}) \\
\end{aligned}\]

<p>The marginal distribution of \(y\) and the conditional distribution of \(x\) given \(y\) are given by</p>

\[\begin{aligned}
p(y) &amp;= \mathcal{N}(y; A\mu+b, L^{-1}+A\Lambda^{-1}A^T)\\
p(x|y) &amp;= \mathcal{N}(x; \Sigma\{A^TL(y-b)+\Lambda\mu\}, \Sigma)
\end{aligned}\]

<p>where \(\Sigma=(\Lambda + A^TLA)^{-1}\).</p>

<h3 id="the-inference-distribution">The Inference Distribution</h3>

<p>The core of the inference distribution \(q_\sigma\) is the conditional distribution of \(X_{t-1}\) given \(X_t\) and \(X_0\), i.e.,</p>

\[q_\sigma(x_{t-1}\vert x_t, x_0)=\mathcal{N}(\tilde{\mu}_t(x_t,x_0),\sigma_t^2I),\]

<p>where \(\tilde\mu_t\) is the mean function. Assuming it takes a linear form, i.e., \(\tilde\mu_t(x_t, x_0)=ax_t+bx_0\), where \(a\) and \(b\) are constants to be determined. We want the proposed joint distribution to match the “marginals” of the original DM. Specifically, suppose \(q_\sigma(x_t\vert x_0)=\mathcal{N}(\sqrt{\alpha_t} x_0, (1-\alpha_t)I)\), we want \(q_\sigma(x_{t-1}\vert x_0)=\mathcal{N}(\sqrt{\alpha_{t-1}} x_0, (1-\alpha_{t-1})I)\). We can compute \(q_\sigma(x_{t-1}\vert x_0)\) from \(q_\sigma(x_{t}\vert x_0)\) and \(q_\sigma(x_{t-1}\vert x_t,x_0)\) as follows. (see <a href="#marginal-and-conditional-gaussians">this section</a>)</p>

\[q_\sigma(x_{t-1}\vert x_0)=\mathcal{N}(a\sqrt{\alpha_t}x_0+bx_0,[\sigma_t^2+(1-\alpha_{t})a^2]I)\]

<p>We solve the following equations to match the mean and the variance:
\(\begin{aligned}
a\sqrt{\alpha_t}+b&amp;=\sqrt{\alpha_{t-1}}\\
\sigma_t^2+(1-\alpha_{t})a^2&amp;=1-\alpha_{t-1}
\end{aligned}\)</p>

<p>which givens</p>

\[\begin{aligned}
a&amp;=\frac{\sqrt{1-\alpha_{t-1}-\sigma_t^2}}{\sqrt{1-\alpha_{t}}}\\
b&amp;=\sqrt{\alpha_{t-1}}-\frac{\sqrt{\alpha_t}\sqrt{1-\alpha_{t-1}-\sigma_t^2}}{\sqrt{1-\alpha_{t}}}
\end{aligned}\]

<p>Therefore,</p>

\[\tilde\mu_t=a x_t+bx_0=\sqrt{\alpha_{t-1}}x_0+\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot\frac{x_t-\sqrt{\alpha_t}x_0}{\sqrt{1-\alpha_{t}}}\]

<p>and</p>

\[q_\sigma(x_{t-1}\vert x_t, x_0)=\mathcal{N}(\sqrt{\alpha_{t-1}}x_0+\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot\frac{x_t-\sqrt{\alpha_t}x_0}{\sqrt{1-\alpha_t}},\sigma_t^2I)\]

<p>In comparison, DDPM uses different mean and variance for \(q(x_{t-1}\vert x_t, x_0)\):</p>

\[\begin{aligned}
q(x_{t-1}\vert x_t, x_0)&amp;=\mathcal{N}\left(\frac{\sqrt{\alpha_{t-1}}}{1-\alpha_{t}}\left(1-\frac{\alpha_t}{\alpha_{t-1}}\right)x_0 + \frac{\sqrt{\alpha_t}(1-\alpha_{t-1})}{\sqrt{\alpha_{t-1}}(1-\alpha_t)}x_t, \frac{1-\alpha_{t-1}}{1-\alpha_t}(1-\frac{\alpha_t}{\alpha_{t-1}})I\right) \\
&amp;=\mathcal{N}\left(\sqrt{\alpha_{t-1}}x_0+\frac{\sqrt{\alpha_t}(1-\alpha_{t-1})}{\sqrt{\alpha_{t-1}}\sqrt{1-\alpha_t}}\cdot\frac{x_t-\sqrt{\alpha_t}x_0}{\sqrt{1-\alpha_t}},\frac{1-\alpha_{t-1}}{1-\alpha_t}(1-\frac{\alpha_t}{\alpha_{t-1}})I \right)
\end{aligned}\]

<p>If we set</p>

\[\sigma_t^2=\frac{1-\alpha_{t-1}}{1-\alpha_t}\left(1-\frac{\alpha_t}{\alpha_{t-1}}\right),\]

<p>then \(q_\sigma(x_{t-1}\vert x_t, x_0)=q(x_{t-1}\vert x_t, x_0)\) and the model becomes DDPM.</p>

<h3 id="proof-of-theorem-1">Proof of Theorem 1</h3>

<p>For all \(\sigma\in\mathbb{R}^T_{&gt;0}\), there exists \(\gamma\in\mathbb{R}^T_{&gt;0}\) and \(C\in\mathbb{R}\), such that \(J_\sigma=L_\gamma+C\).</p>

<p>Proof:</p>

<p>Following the derivation of DDPM (where \(\equiv\) denotes “equal up to a value that does not depend on \(\theta\), but may depend on \(q_\sigma\)”).</p>

\[\begin{aligned}
J_\sigma(\epsilon_\theta)&amp;:=\mathbb{E}_{q_\sigma}[\log q_\sigma(X_{1:T}\vert X_0)-\log p_\theta(X_{0:T})] \\
&amp;\equiv\mathbb{E}_{q_\sigma}\left[\sum_{t=2}^T D_\text{KL}(q_\sigma(x_{t-1}|X_t,X_0)||p_\theta^{(t)}(x_{t-1}|X_t)) -\log p_\theta^{1}(X_0|X_1)\right]\\
\end{aligned}\]

<p>For \(t&gt;1\):</p>

\[\begin{aligned}
\mathbb{E}_{q_\sigma}\left[ D_\text{KL}(q_\sigma(x_{t-1}|X_t,X_0)||p_\theta^{(t)}(x_{t-1}|X_t))\right]&amp;=\mathbb{E}_{X_0,X_t}\left[ D_\text{KL}(q_\sigma(x_{t-1}|X_t,X_0)||q_\sigma(x_{t-1}|X_t,f_\theta^{t}(X_t)))\right]\\
&amp;\equiv\mathbb{E}_{X_0,X_t}\left[\frac{\|\tilde{\mu}_t(X_t,X_0)-\tilde{\mu}_t(X_t,f_\theta^{(t)}(X_t))\|_2^2}{2\sigma_t^2}\right]\\
&amp;=\mathbb{E}_{X_0,X_t}\left[\frac{b_t^2}{2\sigma_t^2}\|X_0-f_\theta^{(t)}(X_t)\|_2^2\right]\\
&amp;=\mathbb{E}_{X_0,\epsilon}\left[\frac{b_t^2(1-\alpha_t)}{2\sigma_t^2\alpha_t}\|\epsilon-\epsilon_\theta^{(t)}(X_t)\|_2^2\right]\\
\end{aligned}\]

<p>For \(t=1\):</p>

\[\begin{aligned}
\mathbb{E}_{q_\sigma}\left[ -\log p_\theta^{1}(X_0|X_1)\right]&amp;\equiv\mathbb{E}_{X_0,X_t}\left[\frac{1}{2\sigma_t^2}\|X_0-f_\theta^{(t)}(X_1)\|_2^2\right]\\
&amp;=\mathbb{E}_{X_0,\epsilon}\left[\frac{(1-\alpha_t)}{2\sigma_t^2\alpha_t}\|\epsilon-\epsilon_\theta^{(t)}(X_1)\|_2^2\right]\\
\end{aligned}\]

<p>Choosing \(\gamma_1=(1-\alpha_t)/(2\sigma_t^2\alpha_t)\) and \(\gamma_t=(1-\alpha_t)b_t^2/(2\sigma_t^2\alpha_t)\) for \(t&gt;1\), we have \(J_\sigma(\epsilon_\theta)\equiv L_\gamma(\epsilon_\theta)\).</p>

<h3 id="accelerated-sampling-processes">Accelerated Sampling Processes</h3>

<p>The inference process in the accelerated case is given by</p>

\[q_{\sigma,\tau}(x_{1:T}\vert x_0)=q_{\sigma, \tau}(x_{\tau_S}\vert x_0)\prod_{i=2}^S q_{\sigma, \tau}(x_{\tau_{i-1}}\vert x_{\tau_i}, x_0)\prod_{t\in\overline\tau}q_{\sigma, \tau}(x_t|x_0),\]

<p>where \(\tau\) is a sub-sequence of \([1,\dots, T]\) of length \(S\) with \(\tau_S=T\), and \(\overline\tau:=\{1,\dots, T\}\backslash \tau\), i.e., the graphical model of \(\{X_{\tau_i}\}_{i=1}^S\) and \(X_0\) form a chain, whereas the graphical model of \(\{X_t\}_{t\in\overline\tau}\) and \(X_0\) form a star graph.</p>

<p>Define:</p>

\[\begin{aligned}
q_{\sigma,\tau}(x_t\vert x_0)&amp;=\mathcal{N}(\sqrt{\alpha_t}x_0,(1-\alpha_t)I)\quad\forall t\in\overline\tau\cup\{T\}\\
q_{\sigma,\tau}(x_{\tau_{i-1}}\vert x_{\tau_i},x_0)&amp;=\mathcal{N}(\sqrt{\alpha_{\tau_{i-1}}}x_0+\sqrt{1-\alpha_{\tau_{i-1}}-\sigma_{\tau_i}^2}\cdot\frac{x_{\tau_i}-\sqrt{\alpha_{\tau_i}}x_0}{\sqrt{1-\alpha_{\tau_i}}},\sigma_{\tau_i}^2I),\quad 2\le i\le S
\end{aligned}\]

<p>where the coefficients are chosen such that:</p>

\[q_{\sigma,\tau}(x_{\tau_i}|x_0)=\mathcal{N}(\sqrt{\alpha_{\tau_i}}x_0,(1-\alpha_{\tau_i})I)\quad 1\le i\le S,\]

<p>i.e., the “marginals” match.</p>

<p>The corresponding “generative process” is defined as:</p>

\[p_\theta(x_{0:T}):=
\underbrace{p_\theta(x_T)\prod_{i=1}^Sp_{\theta}^{(\tau_i)}(x_{\tau_{i-1}}\vert x_{\tau_i})}_\text{use to produce samples}
\times
\underbrace{\prod_{t\in\overline\tau}p_\theta^{(t)}(x_0\vert x_t)}_\text{use in objective},\]

<p>where only part of the models are actually being used to produce samples (define \(\tau_0=0\)). The conditionals are:</p>

\[\begin{aligned}
p_\theta^{\tau_{i}}(x_{\tau_{i-1}}\vert x_{\tau_i})&amp;=q_{\sigma,\tau}(x_{\tau_{i-1}}\vert x_{\tau_i}, f_\theta^{(\tau_i)}(x_{\tau_i}))\quad\text{if }i\in\{2,\dots, S\}\\
p_\theta^{(t)}(x_0\vert x_t)&amp;=\mathcal{N}(f_\theta^{(t)}(x_{t}),\sigma_{t}^2I),\quad\text{if }t\in\overline\tau\cup\{\tau_1\},
\end{aligned}\]

<p>which leverages \(q_{\sigma,\tau}(x_{\tau_{i-1}}\vert x_{\tau_i}, x_0)\) as part of the inference process.</p>

<p>The resulting variational objective becomes (define \(x_{\tau_{L+}}\))</p>

\[\begin{aligned}
J_{\sigma,\tau}(\epsilon_\theta)&amp;=\mathbb{E}_{q_{\sigma,\tau}}[\log q_{\sigma,\tau}(X_{1:T}\vert X_0)-\log p_\theta(X_{0:T})]\\
&amp;\equiv\mathbb{E}_{q_{\sigma,\tau}}\left[\sum_{i=2}^S D_\text{KL}(q_{\sigma,\tau}(x_{\tau_{i-1}}|X_{\tau_i},X_0)||p_\theta^{(\tau_i)}(x_{\tau_{i-1}}|X_{\tau_i})) -\log p_\theta^{(\tau_1)}(X_0|X_{\tau_i})\right.\\
&amp;\qquad+\left. \sum_{t\in\overline\tau} -\log p_\theta^{(t)}(X_0|X_{t}) \right]\\

\end{aligned}\]

<p>A similar argument to the proof used in <a href="#proof-of-theorem-1">Theorem 1</a> can show that \(J_{\sigma,\tau}\) can also be converted to an objective of the form \(L_\gamma\).</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Denoising Diffusion Implicit Models]]></summary></entry><entry><title type="html">GPT-3</title><link href="https://daviddmc.github.io/blog/2020/GPT-3/" rel="alternate" type="text/html" title="GPT-3" /><published>2020-07-22T00:00:00+00:00</published><updated>2020-07-22T00:00:00+00:00</updated><id>https://daviddmc.github.io/blog/2020/GPT-3</id><content type="html" xml:base="https://daviddmc.github.io/blog/2020/GPT-3/"><![CDATA[<h2 id="takeaways">Takeaways</h2>

<ul>
  <li>Although recent large pre-training models are <strong>task-agnostic in architecture</strong>, they still require <strong>task-specific fine-tuning</strong> datasets of thousands of examples.</li>
  <li>The author train GPT-3, an autoregressive LM with 175B parameters, and test its performance in the <strong>few-shot</strong> setting, i.e., providing task descriptions and few-shot demonstrations purely via text interaction (prompt), and without any gradient updates.</li>
  <li>This work shows that scaling up LM greatly improves <strong>task-agnostic</strong>, <strong>few-shot</strong> performance, sometimes even reaching competitiveness with prior SOTA finetuning approaches.</li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>The trend of pre-trained language representation NLP</p>
<ol>
  <li>single-layer pre-trained word embedding + task-specific architectures</li>
  <li>multiple layers of representations (e.g. RNN) + task-specific architectures</li>
  <li>pre-train RNNs or Transformers, and then directly fine-tune, them without task-specific architectures.</li>
</ol>

<h3 id="problems-with-pre-training--fine-tune">Problems with Pre-training + Fine-tune</h3>

<ul>
  <li>Although the last paradigm uses <strong>task-agnostic</strong> architectures, they still require <strong>task-specific</strong> fine-tuning.</li>
  <li>The need for a large dataset of labeled examples for every new task limits the applicability of LMs.</li>
  <li>The potential to exploit spurious correlations in training data fundamentally grows with the expressiveness of the model and the narrowness of the training distribution.</li>
  <li>Humans do not require large supervised datasets to learn most language tasks. A brief directive in natural language + a tiny number of examples is often sufficient.</li>
</ul>

<h3 id="meta-learning">Meta Learning</h3>

<p>In the context of LMs, Meta learning means the model develops a broad set of skills at training time and then uses those abilities at inference time to rapidly adapt to or recognize the desired task.</p>

<p>GPT-2<d-cite key="GPT-2"></d-cite> attempts to do this via what “<em>in-context learning</em>”: the model is conditioned on natural language instruction and/or a few demonstrations of the task.</p>

<p>While it has shown some initial promise, this approach still achieves results far inferior to fine-tuning</p>

<p>This work shows that scaling up language models greatly improves <em>task-agnostic</em>, <em>few-shot</em> performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches.</p>

<h3 id="model-scale">Model Scale</h3>

<table>
  <tbody>
    <tr>
      <td>Model</td>
      <td>GPT<d-cite key="GPT"></d-cite></td>
      <td>BERT<d-cite key="BERT"></d-cite></td>
      <td>GPT-2<d-cite key="GPT-2"></d-cite></td>
      <td>Megatron-LM<d-cite key="Megatron-LM"></d-cite></td>
      <td>T5<d-cite key="T5"></d-cite></td>
      <td>Turing-NLG<d-cite key="Turing-NLG"></d-cite></td>
    </tr>
    <tr>
      <td># of parameters</td>
      <td>100M</td>
      <td>300M</td>
      <td>1.5B</td>
      <td>8B</td>
      <td>11B</td>
      <td>17B</td>
    </tr>
  </tbody>
</table>

<p>There is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a smooth trend of improvement with scale. Since in-context learning involves absorbing many skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale.</p>

<p>The authors test this hypothesis by training a <strong>175B</strong> parameter autoregressive language model (GPT-3) and measuring its in-context learning abilities (few-shot, one-shot, and zero-shot).</p>

<h2 id="methods">Methods</h2>

<h3 id="different-settings">Different Settings</h3>

<h4 id="fine-tuning-ft">Fine-Tuning (FT)</h4>

<ul>
  <li>A pre-trained model by training on a supervised dataset specific to the desired task. Typically thousands to hundreds of thousands of labeled examples are used.</li>
  <li><em>The main advantage</em> is strong performance on many benchmarks.</li>
  <li><em>The main disadvantages</em> are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution, and the potential to exploit spurious features of the training data, potentially resulting in an unfair comparison with human performance.</li>
  <li>In this work, the authors do not fine-tune GPT-3.</li>
</ul>

<h4 id="few-shot-fs">Few-Shot (FS)</h4>

<ul>
  <li>Refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning, but no weight updates are allowed.</li>
  <li>The number of samples \(K\) is in the range of 10 to 100 as this is how many examples can fit in the model’s context window (<code class="language-plaintext highlighter-rouge">nctx = 2048</code>).</li>
  <li>The main advantages: A major reduction in the need for task-specific data and reduced potential to learn an overly narrow distribution from a large but narrow fine-tuning dataset.</li>
  <li>The main disadvantage: Results from this method have so far been much worse than SOTA fine-tuned models. Also, a small amount of task-specific data is still required.</li>
</ul>

<h4 id="one-shot-1s">One-Shot (1S)</h4>

<ul>
  <li>It is the same as few-shot except that only one demonstration is allowed, in addition to a natural
language description of the task.</li>
  <li>The reason to distinguish one-shot from few-shot and zero-shot (below) is that it most closely matches the way in which some tasks are communicated to humans.</li>
</ul>

<h4 id="zero-shot-0s">Zero-Shot (0S)</h4>

<ul>
  <li>No demonstrations are allowed, and the model is only given a natural language instruction describing the task.</li>
  <li>This method provides maximum convenience, potential for robustness, and avoidance of spurious correlations.</li>
  <li>But it is also the most challenging setting.</li>
</ul>

<h3 id="model">Model</h3>

<p>The same model and architecture as GPT-2, with the exception that</p>

<ol>
  <li>GPT-3 uses alternating dense and locally banded sparse attention patterns in the layers of the transformer.</li>
  <li>To study the dependence of ML performance on model size, 8 different sizes of model were trained, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model called GPT-3.</li>
</ol>

<h3 id="training-dataset">Training Dataset</h3>

<p>Unfiltered or lightly filtered versions of Common Crawl tend to have lower quality than more curated datasets. Therefore, the authors took 3 steps to improve the average quality of the datasets:</p>

<ol>
  <li>Downloaded and filtered a version of CommonCrawl based on similarity to a range of high-quality reference corpora;</li>
  <li>Performed fuzzy deduplication at the document level, within and across datasets, to prevent redundancy and preserve the integrity of held-out validation set as an accurate measure of overfitting;</li>
  <li>Added known high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity.</li>
</ol>

<p>The overall training dataset has about 500B tokens.</p>

<h2 id="experiments">Experiments</h2>

<h3 id="evaluation">Evaluation</h3>

<p>For few-shot learning, the authors evaluate each example in the evaluation set by randomly drawing \(K\) examples from that task’s training set as conditioning</p>

<h4 id="multiple-choice-problems">Multiple-Choice Problems</h4>

<p>Provide \(K\) examples of context plus correct completion, followed by one example of context only, and compare the LM likelihood of each completion.</p>

<p>For most tasks, the per-token likelihood (to normalize for length) is compared. However, sometime it might be beneficial to normalize by the unconditional probability of each completion, by computing</p>

\[\frac{P(\texttt{completion}|\texttt{context})}{P(\texttt{completion}|\texttt{answer context})},\]

<p>where answer context is the string <code class="language-plaintext highlighter-rouge">"Answer: "</code> or <code class="language-plaintext highlighter-rouge">"A: "</code>.</p>

<h4 id="binary-classification">Binary Classification</h4>

<p>Give the options more semantically meaningful names (e.g. <code class="language-plaintext highlighter-rouge">"True"</code> or <code class="language-plaintext highlighter-rouge">"False"</code> rather than 0 or 1) and then treat the task like multiple choice.</p>

<h4 id="free-form-completion">Free-Form Completion</h4>

<p>Use beam search with a beam width of 4 and a length penalty of \(\alpha = 0.6\).</p>

<h3 id="language-modeling-cloze-and-completion">Language Modeling, Cloze, and Completion</h3>

<h4 id="language-modeling">Language Modeling</h4>

<p>Evaluate the zero-shot GPT-3 by computing the perplexity on the Penn Tree Bank dataset. GPT-3 sets a new SOTA compared to GPT-2.</p>

<h4 id="lambada">LAMBADA</h4>

<p>Task: The model is asked to predict the last word of sentences which requires reading a paragraph of context.</p>

<p>The authors use a fill-in-the-blank format to guide GPT-3 to predict a word rather than other valid continuations of the paragraph:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Alice was friends with Bob. Alice went to visit her friend ___. → Bob
George bought some baseball equipment, a ball, a glove, and a ___. →
</code></pre></div></div>

<p>Results:</p>

<ul>
  <li>GPT-3 achieves new SOTA on LAMBADA.</li>
  <li>Few-shot performance improves strongly with model size.</li>
  <li>The one-shot setting always performs worse than the zero-shot setting.</li>
</ul>

<h4 id="hellaswag">HellaSwag</h4>

<p>Task: Pick the best ending to a story or set of instructions.</p>

<p>Results: The performance of GPT-3 on this task is a fair amount lower than the overall SOTA.</p>

<h4 id="storycloze">StoryCloze</h4>

<p>Task: Select the correct ending sentence for a five-sentence long story.</p>

<p>Results: GPT-3 is better than previous zero-shot results but still underperforms fine-tuned SOTA.</p>

<h3 id="question-answering">Question Answering</h3>

<p><em>open-book QA:</em> use an information retrieval system to find relevant text and train a model to generate an answer given the question and the retrieved text.</p>

<p><em>closed-book QA:</em> train a model to answer the questions directly.</p>

<p>Results:</p>

<ul>
  <li>Overall, on one of the three datasets GPT-3’s one-shot matches the open-book fine-tuning SOTA.</li>
  <li>On the other two datasets, it approaches the performance of the closed-book SOTA despite not using fine-tuning.</li>
</ul>

<h3 id="translation">Translation</h3>

<p>For GPT-2 a filter was used on a multilingual collection of documents to produce an English-only dataset due to capacity concerns. Since the capacity increases by over two orders of magnitude from GPT-2 to GPT-3, the scope of the training dataset is also expanded to include more representation of other languages. the majority of the data is derived from raw Common Crawl with only quality-based filtering. Although GPT-3’s training data is still primarily English (93% by word count), it also includes 7% of text in other languages.</p>

<p>Zero-shot/one-shot/few-shot GPT-3 underperforms, nears competitive performance, and achieves similar average performance to prior unsupervised NMT work.</p>

<p>GPT-3 has a noticeable skew in its performance depending on language direction. GPT-3 significantly outperforms prior unsupervised NMT work when translating into English but underperforms when translating in the other direction.</p>

<h3 id="winograd-style-tasks">Winograd-Style Tasks</h3>

<p>Task: Determine which word a pronoun refers to, when the pronoun is grammatically ambiguous but semantically unambiguous to a human.</p>

<h3 id="common-sense-reasoning">Common Sense Reasoning</h3>

<p>Task: Capture physical or scientific reasoning</p>

<p>Results: Overall, in-context learning with GPT-3 shows mixed results on commonsense reasoning tasks.</p>

<h3 id="reading-comprehension">Reading Comprehension</h3>

<p>Results:</p>

<ul>
  <li>A wide spread is observed in GPT-3’s performance across 5 datasets suggestive of varying capability with different answer formats.</li>
  <li>In general, GPT-3 is on par with initial baselines and early results trained using contextual representations on each respective dataset.</li>
</ul>

<h3 id="superglue">SuperGLUE</h3>

<p>Results: The average performance of few-shot GPT-3 matches that of a fine-tuned BERT model.</p>

<h3 id="natural-language-inference">Natural Language Inference</h3>

<p>NLI concerns the ability to understand the relationship between two sentences.</p>

<p>Task: a two or three class classification problem where the model classifies whether the second sentence logically follows from the first, contradicts the first sentence, or is possibly true (neutral)</p>

<p>Results: NLI is still a very difficult task for language models and they are only just beginning to show signs of progress.</p>

<h3 id="synthetic-and-qualitative-tasks">Synthetic and Qualitative Tasks</h3>

<h4 id="arithmetic">Arithmetic</h4>

<p>Results: Overall, GPT-3 displays reasonable proficiency at moderately complex arithmetic in few-shot, one-shot, and even zero-shot settings.</p>

<h4 id="word-scrambling-and-manipulation-tasks">Word Scrambling and Manipulation Tasks</h4>

<p>Each task involves giving the model a word distorted by some combination of scrambling, addition, or deletion of characters, and asking it to recover the original word.</p>

<p>Results:</p>

<ul>
  <li>The one-shot performance is significantly weaker than the few-shot setting.</li>
  <li>In the zero-shot setting, the model can rarely perform any of the tasks. This suggests that the model really does appear to learn these tasks at test time, as the model cannot perform them zero-shot and their artificial nature makes them unlikely to appear in the pre-training data.</li>
</ul>

<h4 id="news-article-generation">News Article Generation</h4>

<p>Few-shot learning: Provide three previous news articles and the title and subtitle of a proposed next article in the model’s context to condition it.</p>

<p>Results:</p>

<ul>
  <li>With prompt, the model is able to reliably generate short articles in the “news” genre.</li>
  <li>Human abilities to detect model-generated text appear to decrease as model size increases.</li>
</ul>

<h4 id="learning-and-using-novel-words">Learning and Using Novel Words</h4>

<p>Task: using a word in a sentence after seeing it defined only once.</p>

<p>Results: Overall, GPT-3 appears to be at least proficient at the task of using novel words in a sentence.</p>

<h4 id="correcting-english-grammar">Correcting English Grammar</h4>

<p>Prompt: <code class="language-plaintext highlighter-rouge">Poor English Input: &lt;sentence&gt;\n Good English Output: &lt;sentence&gt;</code>.</p>

<h2 id="limitations">Limitations</h2>

<ul>
  <li>Despite the strong quantitative and qualitative improvements of GPT-3, it still has notable weaknesses in text synthesis and several NLP tasks</li>
  <li>GPT-3 has several structural and algorithmic limitations: do not include any bidirectional architectures or other training objectives such as denoising.</li>
  <li>Scaling up any LM-like model may eventually run into (or could already be running into) the limits of the pretraining objective.</li>
  <li>Poor sample efficiency during pre-training.</li>
  <li>Ambiguity about whether few-shot learning actually learns new tasks “from scratch” at inference time, or if it simply recognizes and identifies tasks that it has learned during training.</li>
</ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Language Models are Few-Shot Learners]]></summary></entry><entry><title type="html">DDPM</title><link href="https://daviddmc.github.io/blog/2020/DDPM/" rel="alternate" type="text/html" title="DDPM" /><published>2020-06-19T00:00:00+00:00</published><updated>2020-06-19T00:00:00+00:00</updated><id>https://daviddmc.github.io/blog/2020/DDPM</id><content type="html" xml:base="https://daviddmc.github.io/blog/2020/DDPM/"><![CDATA[<h2 id="takeaways">Takeaways</h2>

<ul>
  <li>A diffusion probabilistic model is a parameterized Markov chain trained using variational inference to produce samples matching the data after finite time.</li>
  <li>Transitions of this chain are learned to reverse a diffusion process, which is a Markov chain that gradually adds noise to the data in the opposite direction of sampling until signal is destroyed.</li>
  <li>When the diffusion rates \(\beta_t\) are small, it is sufficient to set the sampling chain transitions to conditional Gaussians too, allowing for a particularly simple neural network parameterization.</li>
  <li>DDPM shows that diffusion models are capable of generating high quality samples.</li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Let \(X_0\sim q(x_0)\) be the data distribution. The forward process (diffusion process) \(q(x_{1:T}\vert x_0)\) of diffusion models is a Markov chain that generates latent variables \(X_1,\dots,X_T\) by gradually adding Gaussian noise to the data \(X_0\) according to a variance schedule \(\beta_1,\dots,\beta_T\) where</p>

\[q(x_{1:T}|x_0):=\prod_{t=1}^T q(x_t|x_{t-1}),\quad q(x_t|x_{t-1}):=\mathcal{N}(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_t I)\]

<p>choices of \(\beta_t\)</p>

<ul>
  <li>learned by reparameterization</li>
  <li>held constant as hyperparameters</li>
</ul>

<p>The diffusion models aim to learn a generative distribution \(p_\theta\) to describe the same trajectory, but in <strong>reverse</strong>,</p>

\[p_\theta(x_{0:T}):=p(x_T)\prod_{t=1}^T p_\theta(x_{t-1}|x_t),\quad p(x_T)=\mathcal{N}(x_t;0,I)\]

<p>The reverse processes have the same functional form (Gaussian) when \(\beta_t\) are small (the time reversibility of SDE). Therefore, diffusion models learn Gaussian distribution in the reverse Markov chain:</p>

\[p_\theta(x_{t-1}|x_t):=\mathcal{N}(x_{t-1};\mu_\theta(x_t, t), \Sigma_\theta(x_t, t))\]

<p>Training is performed by optimizing the usual variational bound on negative log likelihood (see <a href="#variational-bound-on-negative-log-likelihood">Appendix</a> for details):</p>

\[\mathbb{E}_q[-\log p_\theta(X_0)] \le \mathbb{E}_q\left[-\log p(X_T)-\sum_{t=1}^T\log\frac{p_\theta(X_{t-1}|X_t)}{q(X_t|X_{t-1})} \right] =:L\\\]

<p>The loss \(L\) can be rewritten as</p>

\[L =\mathbb{E}_q\left[\underbrace{D_\text{KL}(q(x_T|X_0)||p(x_T))}_{L_T}+\sum_{t=2}^T \underbrace{D_\text{KL}(q(x_{t-1}|X_t,X_0)||p_\theta(x_{t-1}|X_t))}_{L_{t-1}} \underbrace{-\log p_\theta(X_0|X_1)}_{L_0} \right]\]

<p>When conditioned on \(X_0\), both \(X_t\) and \(X_{t-1}\) are Gaussian (see <a href="#close-form-of-forward-process">Appendix</a>):</p>

\[q(x_t|x_0)=\mathcal{N}(x_t;\sqrt{\overline{\alpha}_t}x_0,(1-\overline{\alpha}_t)I),\quad q(x_{t-1}|x_0)=\mathcal{N}(x_{t-1};\sqrt{\overline{\alpha}_{t-1}}x_0,(1-\overline{\alpha}_{t-1})I),\]

<p>where \(\alpha_t:=1-\beta_t\) and \(\overline{\alpha}_t:=\prod_{s=1}^t\alpha_s\). According to the forward model, we have the following reparameterization: \(X_t=\sqrt{\alpha_t}X_{t-1}+\sqrt{1-\alpha_t}\epsilon_t\), with \(\epsilon_t\in\mathcal{N}(0,I)\) and therefore, \(\text{Cov}(X_t,X_{t-1})=\sqrt{\alpha_t}\text{Var}(X_{t-1})\) and the posterior conditioned on \(X_0\) is</p>

\[q(x_{t-1}|x_t,x_0)=\mathcal{N}(x_{t-1};\tilde{\mu}_t(x_t,x_0),\tilde{\beta}_tI),\]

<p>where</p>

\[\tilde{\mu}_t(x_t,x_0):=\frac{\sqrt{\overline\alpha_{t-1}}\beta_t}{1-\overline\alpha_t}x_0+\frac{\sqrt{\alpha_t}(1-\overline{\alpha}_{t-1})}{1-\overline{\alpha}_t}x_t,\quad \tilde{\beta}_t:=\frac{1-\overline\alpha_{t-1}}{1-\overline\alpha_{t}}\beta_t\]

<h2 id="ddpm">DDPM</h2>

<h3 id="forward-process-and-l_t">Forward process and \(L_T\)</h3>

<p>Fix \(\beta_t\) to constants, the approximate posterior \(q\) has no learnable parameters, and \(L_T\) is a constant.</p>

<h3 id="reverse-process-and-l_1t-1">Reverse process and \(L_{1:T-1}\)</h3>

\[p_\theta(x_{t-1}|x_t):=\mathcal{N}(x_{t-1};\mu_\theta(x_t, t), \Sigma_\theta(x_t, t))\]

<h4 id="variances">Variances</h4>

<p>Set \(\Sigma_\theta(x_t,t)=\sigma_t^2I\) to untrained time-dependent constants. Experimentally, the following two choices of \(\sigma_t^2\) have similar results.</p>

<ul>
  <li>
    <p>Choice 1: \(\sigma_t^2=\beta_t\). This optimial for \(X_0\sim\mathcal{N}(0,I)\). (see <a href="#special-cases-for-optimal-posterior-variance">Appendix</a>)</p>
  </li>
  <li>
    <p>Choice 2: \(\sigma_t^2=\frac{1-\overline\alpha_{t-1}}{1-\overline\alpha_t}\beta_t\). This is optimal for \(X_0\) deterministically set to one point. (see <a href="#special-cases-for-optimal-posterior-variance">Appendix</a>)</p>
  </li>
</ul>

<h4 id="means">Means</h4>

<p>We rewrite \(L_{t-1}\) as follows. (see <a href="#special-cases-for-optimal-posterior-variance">Appendix</a>)</p>

\[L_{t-1}=\mathbb{E}_q\left[ \frac{1}{2\sigma_t^2}\| \tilde\mu_t(X_t, X_0) -\mu_\theta(X_t,t)\|_2^2 \right]+C\]

<p>where \(C\) is a constant that does not depend on \(\theta\). Since \(X_t=\sqrt{\overline\alpha_t}X_0+\sqrt{1-\overline\alpha_t}\epsilon\) for \(\epsilon\sim\mathcal{N}(0,I)\).</p>

\[\begin{aligned}
L_{t-1}-C&amp;=\mathbb{E}_{X_0,\epsilon}\left[ \frac{1}{2\sigma_t^2}\left\| \tilde\mu_t\left(X_t, \frac{1}{\sqrt{\overline\alpha_t}}(X_t-\sqrt{1-\overline\alpha_t}\epsilon)\right) -\mu_\theta(X_t,t)\right\|_2^2 \right]\\
&amp;=\mathbb{E}_{X_0,\epsilon}\left[ \frac{1}{2\sigma_t^2}\left\| \frac{1}{\sqrt{\alpha_t}}  \left(X_t-\frac{\beta_t}{\sqrt{1-\overline\alpha_t}}\epsilon\right)-\mu_\theta(X_t,t)\right\|_2^2 \right]
\end{aligned}\]

<p>Since \(X_t\) is available as input to the model, we may choose the parameterization</p>

\[\mu_\theta(X_t, t):= \frac{1}{\sqrt{\alpha_t}}  \left(X_t-\frac{\beta_t}{\sqrt{1-\overline\alpha_t}}\epsilon_\theta(X_t, t)\right)\]

<p>To sample \(X_{t-1}\sim p_\theta(x_{t-1}\vert x_t)\) is to compute</p>

\[X_{t-1}=\frac{1}{\sqrt{\alpha_t}}  \left(X_t-\frac{\beta_t}{\sqrt{1-\overline\alpha_t}}\epsilon_\theta(X_t, t)\right)+\sigma_t z,\]

<p>where \(z\sim\mathcal{N}(0,1)\). With the parameterization of \(\mu_\theta\), the loss simplifies to</p>

\[\mathbb{E}_{X_0,\epsilon}\left[ \frac{\beta_t^2}{2\sigma_t^2\alpha_t(1-\overline\alpha_t)}||\epsilon-\epsilon_\theta(\sqrt{\overline\alpha_t}X_0+\sqrt{1-\overline\alpha_t}\epsilon,t) ||_2^2 \right]\]

<h3 id="data-scaling-reverse-process-decoder-and-l_0">Data Scaling, Reverse Process Decoder, and \(L_0\)</h3>

<p>Assume that image data consists of integers in \(\{0,1,\dots,255\}\) and scale them ilnearly to \([-1,1]\). This ensures that the neural network reverse process operates on consistently scaled inputs starting from the standard normal prior \(p(x_T)\). Set the last term of the reverse process as follows:</p>

\[p_\theta(x_0|x_1)=\prod_{i=1}^D\int_{\delta_-(x_0^i)}^{\delta_+(x_0^i)}\mathcal{N}(x;\mu_\theta^i(x_1,1),\sigma_1^2)dx,\]

\[\delta_+(x)=\left\{
  \begin{aligned}
  &amp;\infty &amp;\text{if } x=1 \\
  &amp;x+\frac{1}{255} &amp;\text{if } x&lt;1
  \end{aligned}
  \right.,
\qquad
\delta_-(x)=\left\{
  \begin{aligned}
  &amp;-\infty &amp;\text{if } x=-1 \\
  &amp;x-\frac{1}{255} &amp;\text{if } x&gt;-1
  \end{aligned}
  \right.,\]

<p>where \(D\) is the data dimensionality and the \(i\) superscript indicates extraction of one coordinate. Similar to the discretized continuous distributions used in VAE decoders and autoregressive models, the choice here ensures that the variational bound is a lossless codelength of discrete data, without the need of adding noise to the data or incorporating the Jacobian of the scaling operation into the log likelihood.</p>

<p>At the end of sampling, \(\mu_\theta(X_1, 1)\) is outputed without adding noise.</p>

<h3 id="training-and-sampling">Training and Sampling</h3>

<p>It is beneficial to sample quality (and simpler to implement) to train on the
following variant of the variational bound:</p>

\[L_\text{simple}(\theta):=\mathbb{E}_{t,X_0,\epsilon}\left[ \| \epsilon-\epsilon_\theta(\sqrt{\overline\alpha_t}X_0+\sqrt{1-\overline\alpha_t}\epsilon,t) \|_2^2 \right],\]

<p>where \(t\) is uniform between \(1\) and \(T\).</p>

<ul>
  <li>The \(t=1\) case corresponds to \(L_0\)</li>
  <li>The \(t&gt;1\) cases correspond to an unweighted version of \(L_{t-1}\).</li>
  <li>The \(L_T\) term does not appear because the forward process variances \(\beta_t\) are fixed.</li>
</ul>

<p>Since the simplified objective discards the weighting, it is a <strong>weighted variational bound</strong> that emphasizes different aspects of reconstruction compared to the standard variational bound. The diffusion process setup in <a href="#experiments">the experiments</a> causes the simplified objective to down-weight loss terms corresponding to small \(t\). These terms train the network to denoise data with very small amounts of noise, so it is beneficial to down-weight them so that the network can focus on more difficult denoising tasks at larger \(t\) terms.</p>

<p>The overall training and sampling algorithms are as follows.</p>

<div class="l-body" style="text-align:center;">
  <img src="https://hojonathanho.github.io/diffusion/assets/img/algorithms.png" width="100%" style="margin-bottom: 12px; background-color: white;" />
  <p></p>
</div>

<h2 id="experiments">Experiments</h2>

<h3 id="hyperparameters">Hyperparameters</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">\(T\)</th>
      <th style="text-align: center">\(\beta_1\)</th>
      <th style="text-align: center">\(\beta_T\)</th>
      <th style="text-align: center">\(\beta\) intepolation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">1000</td>
      <td style="text-align: center">\(10^{-4}\)</td>
      <td style="text-align: center">0.02</td>
      <td style="text-align: center">Linear</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>Use a U-Net backbone with group normalization.</li>
  <li>Parameters are shared across time, which is specified to the network using the Transformer sinusoidal position embedding</li>
  <li>Use self-attention at the \(16 \times 16\) feature map resolution.</li>
</ul>

<h3 id="sample-quality">Sample Quality</h3>

<ul>
  <li>The unconditional DDPM achieves better sample quality than most models in the literature, including class conditional models.</li>
  <li>Training DDPMs on the true variational bound yields better codelengths (i.e., the negative log likelihoods) than training on the simplified objective, as expected, but the latter yields the best sample quality.</li>
</ul>

<h3 id="reverse-process-parameterization-and-training-objective-ablation">Reverse Process Parameterization and Training Objective Ablation</h3>

<ol>
  <li>The baseline option of predicting \(\tilde\mu\) works well only when trained on the true variational bound instead of unweighted mean squared error.</li>
  <li>Learning reverse process variances (by incorporating a parameterized diagonal \(\Sigma_\theta\) into the variational bound) leads to unstable training and poorer sample quality compared to fixed variances.</li>
  <li>Predicting \(\epsilon\) performs approximately as well as predicting \(\tilde\mu\) when trained on the variational bound with fixed variances, but much better when trained with the simplified objective.</li>
</ol>

<h3 id="progressive-coding">Progressive Coding</h3>

<p>The lossless codelengths of DDPMs are better than the large estimates reported for energy-based models and score matching using annealed importance sampling, they are not competitive with other types of likelihood-based generative models.</p>

<p>DDPMs have an <em>inductive bias</em> that makes them excellent <em>lossy</em> compressors.</p>

<h4 id="progressive-lossy-compression">Progressive Lossy Compression</h4>

<p>The distortion decreases steeply in the low-rate region of the rate-distortion plot, indicating that the majority of the bits are indeed allocated to imperceptible distortions</p>

<h4 id="progressive-generation">Progressive Generation</h4>

<p>Experiment: Run a progressive unconditional generation process given by progressive decompression from random bits, i.e., predicting the result of the reverse process, \(\hat{X}_0=(X_t-\sqrt{1-\overline\alpha_t}\epsilon_\theta(X_t, t))/\sqrt{\overline\alpha_t}\), while sampling from the reverse process.</p>

<p>Result: Large-scale image features appear first and details appear last</p>

<p>Experiment: Stochastic prediction by sampling images from \(p_\theta(x_0\vert x_t)\) for different time step</p>

<p>Results:  When \(t\) is small, all but fine details are preserved, and when \(t\) is large, only large-scale features are preserved. Perhaps these are hints of conceptual compression.</p>

<h4 id="connection-to-autoregressive-decoding">Connection to Autoregressive Decoding</h4>

<p>The variational bound \(L\) can be rewritten as (see <a href="#variational-bound-on-negative-log-likelihood">Appendix</a>):</p>

\[L=D_\text{KL}(q(x_T)||p(x_T)) + \mathbb{E}_q\left[\sum_{t=1}^TD_\text{KL}(q(x_{t-1}|X_t)||p_\theta(x_{t-1}|X_t))\right] + H(X_0)\]

<p>An autoregressive model can be considered as a special diffusion model:</p>

<ol>
  <li>Set the diffusion process length \(T\) to the dimensionality of the data.</li>
  <li>Define the forward process so that \(q(x_t\vert x_0)\) places all probability mass on \(x_0\) with the first \(t\) coordinates masked out (i.e., \(q(x_t\vert x_{t-1})\) masks out the \(t\)-th coordinate).</li>
  <li>Set \(p(x_t)=q(x_t)\) to place all mass on a blank image, and therefore \(D_\text{KL}(q(x_T)\|p(x_T))=0\)</li>
  <li>Take \(p_\theta(x_{t-1}\vert x_t)\) to be a fully expressive conditional distribution.</li>
  <li>With these choices, minimizing \(D_\text{KL}(q(x_{t-1}\vert x_t)\|p_\theta(x_{t-1}\vert x_t))\) trains \(p_\theta\) to copy coordinates \(t+1,\dots,T\) unchanged and to predict the \(t\)-th coordinate given \(t+1,\dots,T\)</li>
</ol>

<p>Insights:</p>

<ul>
  <li>
    <p>Interpret the Gaussian DM as a kind of autoregressive model with a <strong>generalized bit ordering</strong> that cannot be expressed by reordering data coordinates.</p>
  </li>
  <li>
    <p>Prior work has shown that such reorderings introduce inductive biases that have an impact on sample quality.</p>
  </li>
  <li>
    <p>Gaussian diffusion might serve a similar purpose, perhaps to greater effect since Gaussian noise might be more natural to add to images compared to masking noise. Gaussian diffusion length is not restricted to equal the data dimension: Gaussian diffusions can be made shorter for fast sampling or longer for model expressiveness.</p>
  </li>
</ul>

<h3 id="interpolation">Interpolation</h3>

<p>Interpolate images in the latent space</p>

<ol>
  <li>Given two images from the data distribution \(X_0,X_0'\sim q(x_0)\).</li>
  <li>Use \(q\) as a stochastic encoder to encoder \(X_0\) and \(X_0'\) into \(X_t\) and \(X_t'\) respectively, where \(X_t,X_t'\sim q(x_t\vert x_0)\).</li>
  <li>Linearly interpolate in the latent space: \(\bar{X}_t=(1-\lambda)X_t+\lambda X_t'\).</li>
  <li>Decode \(\bar{X}_t\) into the image space by the reverse process, \(\bar{X}_0\sim p(x_0\vert\bar{x}_t)\)</li>
</ol>

<p>Results:  The reverse process produces high-quality reconstructions and plausible interpolations that smoothly vary attributes such as pose, skin tone, hairstyle, expression, and background, but not eyewear. Larger \(t\) results in coarser and more varied interpolations.</p>
<h2 id="appendix">Appendix</h2>

<h3 id="variational-bound-on-negative-log-likelihood">Variational Bound on Negative log Likelihood</h3>

\[\begin{aligned}
\mathbb{E}_q[-\log p_\theta(X_0)] &amp;=\int -q(x_0)\log p_\theta(x_0) d x_0 \\
&amp; = \int -q(x_0)\log\left[\int\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)}q(x_{1:T}|x_0) d x_{1:T} \right]d x_0 \\
&amp;\le \int -q(x_0)\int\log\left[\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)}\right]q(x_{1:T}|x_0) d x_{1:T}  d x_0 \\
&amp; = \int -q(x_{0:T})\log\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)} dx_{0:T} \\
&amp; = \mathbb{E}_q \left[-\log\frac{p_\theta(X_{0:T})}{q(X_{1:T}|X_0)}\right]  \\
&amp; = \mathbb{E}_q \left[-\log\frac{p(X_T)\prod_{t=1}^T p_\theta(X_{t-1}|X_t)}{\prod_{t=1}^Tq(X_t|X_{t-1})}\right] \\
&amp; = \mathbb{E}_q\left[-\log p(X_T)-\sum_{t=1}^T\log\frac{p_\theta(X_{t-1}|X_t)}{q(X_t|X_{t-1})} \right] =:L\\
\end{aligned}\]

<p>To derive the objective for DDPM, \(L\) can be rewritten as</p>

\[\begin{aligned}
L &amp; = \mathbb{E}_q\left[-\log p(X_T)-\sum_{t=1}^T\log\frac{p_\theta(X_{t-1}|X_t)}{q(X_t|X_{t-1})} \right]\\
&amp; =  \mathbb{E}_q\left[-\log p(X_T)-\sum_{t=2}^T\log\frac{p_\theta(X_{t-1}|X_t)}{q(X_t|X_{t-1})} - \log\frac{p_\theta(X_0|X_1)}{q(X_1|X_0)} \right] \\
&amp; =  \mathbb{E}_q\left[-\log p(X_T)-\sum_{t=2}^T\log\frac{p_\theta(X_{t-1}|X_t)}{q(X_t|X_{t-1}, X_0)} - \log\frac{p_\theta(X_0|X_1)}{q(X_1|X_0)} \right] \\
&amp; =  \mathbb{E}_q\left[-\log p(X_T)-\sum_{t=2}^T\log\frac{p_\theta(X_{t-1}|X_t)q(X_{t-1}|X_0)}{q(X_{t-1}|X_{t}, X_0)q(X_t|X_0)} - \log\frac{p_\theta(X_0|X_1)}{q(X_1|X_0)} \right] \\
&amp; =  \mathbb{E}_q\left[-\log \frac{p(X_T)}{q(X_T|X_0)}-\sum_{t=2}^T\log\frac{p_\theta(X_{t-1}|X_t)}{q(X_{t-1}|X_{t}, X_0)} - \log p_\theta(X_0|X_1)\right] \\
&amp; = \mathbb{E}_q\left[\underbrace{D_\text{KL}(q(x_T|X_0)||p(x_T))}_{L_T}+\sum_{t=2}^T \underbrace{D_\text{KL}(q(x_{t-1}|X_t,X_0)||p_\theta(x_{t-1}|X_t))}_{L_{t-1}} \underbrace{-\log p_\theta(X_0|X_1)}_{L_0} \right]
\end{aligned}\]

<p>Here is another way to rewrite \(L\) which is helpful for analysis.</p>

\[\begin{aligned}
L &amp; = \mathbb{E}_q\left[-\log p(X_T)-\sum_{t=1}^T\log\frac{p_\theta(X_{t-1}|X_t)}{q(X_t|X_{t-1})} \right]\\
&amp;=\mathbb{E}_q\left[-\log p(X_T)-\sum_{t=1}^T\log\frac{p_\theta(X_{t-1}|X_t)}{q(X_{t-1}|X_{t})}\frac{q(X_{t-1})}{q(X_t)} \right]\\
&amp;=\mathbb{E}_q\left[-\log \frac{p(X_T)}{q(X_T)}-\sum_{t=1}^T\log\frac{p_\theta(X_{t-1}|X_t)}{q(X_{t-1}|X_{t})}-\log q(X_0) \right]\\
&amp;=D_\text{KL}(q(x_T)||p(x_T)) + \mathbb{E}_q\left[\sum_{t=1}^TD_\text{KL}(q(x_{t-1}|X_t)||p_\theta(x_{t-1}|X_t))\right] + H(X_0)
\end{aligned}\]

<h3 id="close-form-of-forward-process">Close Form of Forward Process</h3>

<p>A notable property of the forward process is that it admits sampling \(X_t\) at an arbitrary timestep \(t\) in closed form: using the notation \(\alpha_t:=1-\beta_t\) and \(\overline{\alpha}_t:=\prod_{s=1}^t\alpha_s\), we have</p>

\[q(x_t|x_0)=\mathcal{N}(x_t;\sqrt{\overline{\alpha}_t}x_0,(1-\overline{\alpha}_t)I)\]

<p>Proof:</p>

<p>Let \(\epsilon_1,\dots,\epsilon_n\) be noise sampled iid from \(\mathcal{N}(0, I)\).</p>

\[\begin{aligned}
X_t&amp;=\sqrt{\alpha_t}X_{t-1}+\sqrt{1-\alpha_t}\epsilon_t \\
&amp;= \sqrt{\alpha_t}(\sqrt{\alpha_{t-1}}X_{t-2}+\sqrt{1-\alpha_{t-1}}\epsilon_{t-1})+\sqrt{1-\alpha_t}\epsilon_t \\
&amp;=\dots \\
&amp;=\sqrt{\alpha_t\cdots\alpha_1}X_0+\sqrt{\alpha_t\cdots\alpha_2}\sqrt{1-\alpha_1}\epsilon_1+\cdots+\sqrt{\alpha_t}\sqrt{(1-\alpha_{t-1})}\epsilon_{t-1}+\sqrt{1-\alpha_t}\epsilon_t \\
&amp;=\sqrt{\overline{\alpha}_t}X_0+\sqrt{1-\overline\alpha_t}\overline\epsilon_t,
\end{aligned}\]

<p>where \(\overline\epsilon_t\sim\mathcal{N}(0, I)\). To obtain the last equality, we use the fact that</p>

\[\begin{aligned}
(\sqrt{\alpha_t\cdots\alpha_1})^2+(\sqrt{\alpha_t\cdots\alpha_2}\sqrt{1-\alpha_1})^2+\cdots+(\sqrt{\alpha_t}\sqrt{(1-\alpha_{t-1})})^2+(\sqrt{1-\alpha_t})^2&amp;=1\\
(\sqrt{\alpha_t\cdots\alpha_2}\sqrt{1-\alpha_1})^2+\cdots+(\sqrt{\alpha_t}\sqrt{(1-\alpha_{t-1})})^2+(\sqrt{1-\alpha_t})^2&amp;=1-\overline\alpha_t
\end{aligned}\]

<h3 id="kl-divergence-between-two-gaussian-distributions">KL Divergence between Two Gaussian Distributions</h3>

<p>Let \(p:=\mathcal{N}(\mu_p,\Sigma_p)\) and \(q:=\mathcal{N}(\mu_q,\Sigma_q)\) be two \(d\)-dimensional Gaussian distributions.</p>

\[\begin{aligned}
D_\text{KL}(p||q)&amp;=\mathbb{E}_p\left[\log(p)-\log(q)\right] \\
&amp;=\mathbb{E}_p\left[\frac{1}{2}\log\frac{|\Sigma_q|}{|\Sigma_p|}-\frac{1}{2}(X-\mu_p)^T\Sigma_p^{-1}(X-\mu_p)+\frac{1}{2}(X-\mu_q)^T\Sigma_q^{-1}(X-\mu_q)\right] \\
&amp;=\frac{1}{2}\log\frac{|\Sigma_q|}{|\Sigma_p|}-\frac{1}{2}\mathbb{E}_p\left[(X-\mu_p)^T\Sigma_p^{-1}(X-\mu_p)\right]+\frac{1}{2}\mathbb{E}_p\left[(X-\mu_q)^T\Sigma_q^{-1}(X-\mu_q)\right]
\end{aligned}\]

<p>The second term is</p>

\[\begin{aligned}
\mathbb{E}_p\left[(X-\mu_p)^T\Sigma_p^{-1}(X-\mu_p)\right]&amp;=\mathbb{E}_p\left[tr\{(X-\mu_p)^T\Sigma_p^{-1}(X-\mu_p)\}\right] \\
&amp;=\mathbb{E}_p\left[tr\{(X-\mu_p)(X-\mu_p)^T\Sigma_p^{-1}\}\right] \\
&amp;=tr\{\mathbb{E}_p\left[(X-\mu_p)(X-\mu_p)^T\Sigma_p^{-1}\right]\} \\
&amp;=tr\{\mathbb{E}_p\left[(X-\mu_p)(X-\mu_p)^T\right]\Sigma_p^{-1}\} \\
&amp; =tr\{\Sigma_p \Sigma_p^{-1}\}\\
&amp;= tr\{I\}\\
&amp;= d
\end{aligned}\]

<p>The third term is</p>

\[\begin{aligned}
\mathbb{E}_p\left[(X-\mu_q)^T\Sigma_q^{-1}(X-\mu_q)\right]&amp; =\mathbb{E}_p\left[(X-\mu_p+\mu_p-\mu_q)^T\Sigma_q^{-1}(X-\mu_p+\mu_p-\mu_q)\right]\\
&amp;=\mathbb{E}_p\left[(X-\mu_p)^T\Sigma_q^{-1}(X-\mu_p)\right] + (\mu_p-\mu_q)^T\Sigma_q^{-1}(\mu_p-\mu_q)\\
&amp;=tr\{\Sigma_p \Sigma_q^{-1}\}+ (\mu_p-\mu_q)^T\Sigma_q^{-1}(\mu_p-\mu_q)
\end{aligned}\]

<p>Combining all this we get,</p>

\[D_\text{KL}(p||q)=\frac{1}{2}\left[\log\frac{|\Sigma_q|}{|\Sigma_p|}-d+(\mu_p-\mu_q)^T\Sigma_q^{-1}(\mu_p-\mu_q) +tr\{\Sigma_p \Sigma_q^{-1}\} \right]\]

<p>Moreover, if \(\Sigma_p=\sigma^2_p I, \Sigma_q=\sigma^2_q I\) we have,</p>

\[D_\text{KL}(p||q)=\frac{1}{2}\left[d\log\frac{\sigma_q^2}{\sigma_p^2}-d+\frac{||\mu_p-\mu_q||_2^2}{\sigma^2_q} + \frac{\sigma_p^2}{\sigma_q^2}d \right]\]

<h3 id="special-cases-for-optimal-posterior-variance">Special Cases for Optimal Posterior Variance</h3>

<p>With \(\Sigma_\theta(x_t,t)=\sigma_t^2\) and the KL Divergence formular in <a href="#kl-divergence-between-two-gaussian-distributions">the previous section</a>, we have</p>

\[\begin{aligned}
L_{t-1} &amp;:=\mathbb{E}_q [ D_\text{KL}(q(x_{t-1}|X_t,X_0)||p_\theta(x_{t-1}|X_t)) \\
&amp;=\frac{1}{2}\left[d\log\frac{\sigma_t^2}{\tilde\beta_t}-d+\frac{\mathbb{E}_q||\tilde\mu_t-\mu_\theta||_2^2}{\sigma^2_t} + \frac{\tilde\beta_t}{\sigma_t^2}d \right]
\end{aligned}\]

<p>Case 1: \(X_0\) is deterministic</p>

<p>When \(X_0\) is deterministically taking some value \(x_0\), \(L_{t-1}\) is minimized when \(\mu_\theta=\tilde\mu_t\) and \(\sigma_t^2=\tilde\beta_t\)</p>

<p>Case 2: \(X_0\sim\mathcal{N}(0,1)\)</p>

<p>If \(X_0\sim\mathcal{N}(0,1)\), then \(X_t=\sqrt{\overline{\alpha}_t}X_0+\sqrt{1-\overline\alpha_t}\overline\epsilon_t\sim\mathcal{N}(0,1)\) and \(\text{Cov}(X_t,X_0)=\sqrt{\overline{\alpha}_t}I\). Therefore, \(q(x_0\vert x_t)=\mathcal{N}(x_0;\sqrt{\overline{\alpha}_t}x_t,(1-\overline{\alpha}_t)I)\). To minimize the expectation of the KL divergence we set</p>

\[\begin{aligned}
\mu_\theta&amp;=\mathbb{E}[\tilde\mu_t|X_t]\\
&amp;=\frac{\sqrt{\overline\alpha_{t-1}}\beta_t}{1-\overline\alpha_t}\mathbb{E}[X_0|X_t]+\frac{\sqrt{\alpha_t}(1-\overline{\alpha}_{t-1})}{1-\overline{\alpha}_t}X_t\\
&amp;=\frac{\sqrt{\overline\alpha_{t-1}}\beta_t}{1-\overline\alpha_t}\sqrt{\overline\alpha_t}X_t+\frac{\sqrt{\alpha_t}(1-\overline{\alpha}_{t-1})}{1-\overline{\alpha}_t}X_t
\end{aligned}\]

<p>and therefore,</p>

\[\mathbb{E}_q||\tilde\mu_t-\mu_\theta||_2^2=\left(\frac{\sqrt{\overline\alpha_{t-1}}\beta_t}{1-\overline\alpha_t}\right)^2(1-\overline\alpha_t)d\]

<p>Differentiating \(L_{t-1}\) in this case with respect to \(\sigma_t^2\), and set to zero,</p>

\[\frac{\partial L_{t-1}}{\partial\sigma_t^2} = \frac{d}{2}\left[\frac{1}{\sigma_t^2}- \frac{\left(\frac{\sqrt{\overline\alpha_{t-1}}\beta_t}{1-\overline\alpha_t}\right)^2(1-\overline\alpha_t)+\tilde\beta_t}{\sigma_t^4}  \right]=0\]

<p>We get</p>

\[\begin{aligned}
\sigma_t^2&amp;=\left(\frac{\sqrt{\overline\alpha_{t-1}}\beta_t}{1-\overline\alpha_t}\right)^2(1-\overline\alpha_t)+\tilde\beta_t\\
&amp;=\left[\frac{\overline\alpha_{t-1}(1-\overline\alpha_t)\beta_t}{(1-\overline\alpha_t)^2}+\frac{1-\overline\alpha_{t-1}}{1-\overline\alpha_{t}}\right]\beta_t \\
&amp;=\left[\frac{\overline\alpha_{t-1}(1-\alpha_t)}{1-\overline\alpha_t}+\frac{1-\overline\alpha_{t-1}}{1-\overline\alpha_{t}}\right]\beta_t \\
&amp;=\beta_t
\end{aligned}\]]]></content><author><name></name></author><summary type="html"><![CDATA[Denoising Diffusion Probabilistic Models]]></summary></entry><entry><title type="html">T5: Text-to-Text Transfer Transformer</title><link href="https://daviddmc.github.io/blog/2019/T5/" rel="alternate" type="text/html" title="T5: Text-to-Text Transfer Transformer" /><published>2019-10-23T00:00:00+00:00</published><updated>2019-10-23T00:00:00+00:00</updated><id>https://daviddmc.github.io/blog/2019/T5</id><content type="html" xml:base="https://daviddmc.github.io/blog/2019/T5/"><![CDATA[<h2 id="takeaways">Takeaways</h2>

<ul>
  <li>
    <p><strong>Text-to-text</strong> provides a simple way to train a single model on a wide variety of text tasks. T2T is simple, yet obtained comparable performance to task-specific architectures and ultimately produced SOTA results when combined with scale.</p>
  </li>
  <li>
    <p><strong>Architectures:</strong> The original encoder-decoder form worked best in T2T.</p>
  </li>
  <li>
    <p><strong>Unsupervised objectives:</strong> The denoising objectives performed best in T2T.</p>
  </li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Motivation: There is a need for a more rigorous understanding of the contributions of different components in transfer learning for NLP (large-scale pre-training models), e.g., different models, pre-training objectives, datasets, and fine-tuning methods.</p>

<p>The basic idea: Introduce a unified framework (T5) that converts all text-based language problems into a text-to-text format. The text-to-text framework allows us to directly apply the same model, objective, training procedure, and decoding process to every task considered.</p>

<p>This work primarily comprises a survey, exploration, and empirical comparison of existing techniques, and explores the limits of current approaches by scaling up the insights (training models up to 11 B parameters on dataset up to 750GB)</p>

<h2 id="methods">Methods</h2>

<h3 id="model">Model</h3>

<p>T5 closely follows the original Transformer<d-cite key="Transformer"></d-cite>.</p>

<p>Main differences:</p>
<ul>
  <li>LayerNorm
    <ul>
      <li>LayerNorms are used at the start of each block and the end of the last block.</li>
      <li>Scale-only LayerNorm, i.e., no additive bias.</li>
    </ul>
  </li>
  <li>Positional embedding
    <ul>
      <li>Relative positional embedding.</li>
      <li>Simplified position embeddings where each “embedding” is simply a scalar that is added to the corresponding logit used for computing the attention weights.</li>
      <li>Share the position embedding parameters across all layers in the model, though within a given layer each attention head uses a different learned position embedding.</li>
      <li>Use 32 embeddings with ranges that increase in size logarithmically up to an offset of 128 beyond which we assign all relative positions to the same embedding. (<a href="https://github.com/huggingface/transformers/blob/v4.25.1/src/transformers/models/t5/modeling_t5.py#L374">Implementation</a>)</li>
    </ul>
  </li>
  <li>Input embedding matrix
    <ul>
      <li>The weights of the output dense layer (before the final softmax) are shared with the input embedding matrix.</li>
    </ul>
  </li>
</ul>

<p>T5 uses an encoder-decoder architecture as in the original Transformer<d-cite key="Transformer"></d-cite>. In comparison, GPT<d-cite key="GPT"></d-cite>, GPT-2<d-cite key="GPT-2"></d-cite>, BERT<d-cite key="BERT"></d-cite> use a single stack of Transformer layers.</p>

<h3 id="dataset">Dataset</h3>

<p>The Colossal Clean Crawled Corpus (C4), ~ 750 GB.</p>

<ol>
  <li>Start with Common Crawl</li>
  <li>Retain lines that ended in a terminal punctuation mark.</li>
  <li>Discarded any page with fewer than 5 sentences and only retained lines that contained at least 3 words.</li>
  <li>Remove any page that contained any word on the “List of Dirty, Naughty, Obscene or Otherwise Bad Words”.</li>
  <li>Remove any line with the word Javascript.</li>
  <li>Remove any page where the phrase “lorem ipsum” (placeholder) appeared.</li>
  <li>Removed any pages that contained a curly bracket to avoid pages with code.</li>
  <li>Discarded all but one of any three-sentence span occurring more than once in the data set.</li>
  <li>Filter out non-English pages</li>
</ol>

<h3 id="input-and-output-format">Input and Output Format</h3>

<p>Cast all of the tasks considered into a “text-to-text” format, i.e., a task where the model is fed some text for context or conditioning and is then asked to produce some output text.</p>

<p>The text-to-text framework provides a consistent training objective both for pre-training and fine-tuning.</p>

<p>T5 is trained with a maximum likelihood objective (using “teacher forcing”, i.e., using ground truth as input, instead of model output from a prior time step as an input) and a cross-entropy loss regardless of the task. To specify which task the model should perform, a task-specific (text) prefix is added to the original input sequence before feeding it to the model.</p>

<p>Compare to GPT-2<d-cite key="GPT-2"></d-cite>, which also uses prompts:</p>

<ul>
  <li>GPT-2 is autoregressive (processing the prefix left-to-right), while T5 explicitly processes an input with an encoder (bidirectional attention).</li>
  <li>GPT-2 focuses on zero-shot learning, while T5 focuses on transfer learning with fine-tuning.</li>
</ul>

<h2 id="experiments">Experiments</h2>

<h3 id="baseline">Baseline</h3>

<h4 id="baseline-model">Baseline Model</h4>

<p>A standard encoder-decoder Transformer<d-cite key="Transformer"></d-cite> is designed so that the encoder and decoder are each similar in size and configuration to a BERT-base model.</p>

<h4 id="vocabulary">Vocabulary</h4>

<p>Use SentencePiece to encode text as WordPiece tokens (use a vocabulary of 32,000 wordpieces)</p>

<p>Trained the SentencePiece model on a mixture of 10 parts of English C4 data with 1 part each of data classified as German, French or Romanian. This vocabulary was shared across both the input and output of the model. Note that the vocabulary makes it so that the model can only process a predetermined, fixed set of languages.</p>

<h4 id="unsupervised-objective">Unsupervised Objective</h4>

<p>Use the “denoising” objectives, i.e., masked language modeling. The model is trained to predict missing or otherwise corrupted tokens in the input.</p>

<p>Design an objective that randomly samples and then drops out 15% of tokens in the input sequence. All consecutive spans of dropped-out tokens are replaced by a single sentinel token. Each sentinel token is assigned a token ID that is unique to the sequence.</p>

<p>The target then corresponds to all of the dropped-out spans of tokens, delimited by the same sentinel tokens used in the input sequence plus a final sentinel token to mark the end of the target sequence. An example is as follows.</p>

<p><em>Original text</em></p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Thank you for inviting me to your party last week
</code></pre></div></div>

<p><em>Inputs</em></p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Thank you for &lt;X&gt; to your party &lt;Y&gt; week
</code></pre></div></div>

<p><em>Target</em></p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;X&gt; for inviting &lt;Y&gt; last &lt;Z&gt;
</code></pre></div></div>

<h3 id="architectures">Architectures</h3>

<p>Review and compare the following architectural variants.</p>

<div class="l-body" style="text-align:center;">
  <img src="https://media.arxiv-vanity.com/render-output/5540256/x4.png" width="80%" style="margin-bottom: 12px; background-color: white;" />
  <p>Different schematics of the Transformer architecture variants.</p>
</div>

<div class="l-body" style="text-align:center;">
  <img src="https://media.arxiv-vanity.com/render-output/5540256/x3.png" width="80%" style="margin-bottom: 12px; background-color: white;" />
  <p>Different attention mask patterns.</p>
</div>

<h4 id="model-structures">Model Structures</h4>

<p>A major distinguishing factor for different architectures is the “mask” used by different attention mechanisms in the model.</p>

<table>
  <thead>
    <tr>
      <th>Architectures</th>
      <th>mask</th>
      <th># of layer stacks</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Encoder-Decoder (e.g. T5)</td>
      <td>Encoder: Fully-visible, Decoder: Causal</td>
      <td>2</td>
    </tr>
    <tr>
      <td>Language model (e.g. GPT)</td>
      <td>Causal</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Prefix LM</td>
      <td>Causal with prefix</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<p>A fundamental and frequently cited drawback of using an LM in the text-to-text setting is that causal masking forces the model’s representation of the \(i\)-th entry of the input sequence to only depend on the entries up until \(i\). This issue can be avoided in a Transformer-based language model simply by changing the masking pattern (Prefix LM).</p>

<p>The main difference between a prefix LM and the BERT architecture is that the classifier is simply integrated into the output layer of the Transformer decoder in the prefix LM.</p>

<h4 id="objectives">Objectives</h4>

<p>Considered both the standard language modeling objective and the denoising objective discussed in <a href="#unsupervised-objective">the previous section</a>.</p>

<p>Language modeling objective:</p>

<p>For models that ingest a prefix before making predictions (the encoder-decoder model and prefix LM), we sample a span of text from our unlabeled data set and choose a random point to split it into prefix and target portions.</p>

<p>For the standard language model, we train the model to predict the entire span from beginning to end.</p>

<p>Denoising objective:</p>

<p>The unsupervised denoising objective is designed for text-to-text models; to adapt it for use with a language model the inputs and targets are concatenated.</p>

<h4 id="results">Results</h4>

<ul>
  <li>For all tasks, the encoder-decoder architecture with the denoising objective performed best.</li>
  <li>Though an encoder-decoder model uses twice as many parameters as “encoder-only” (e.g. BERT) or “decoder-only” (language model) architectures, it has a similar computational cost.</li>
  <li>Sharing the parameters in the encoder and decoder did not result in a substantial performance drop while halving the total parameter count.</li>
</ul>

<h3 id="unsupervised-objectives">Unsupervised Objectives</h3>

<p>Explore different unsupervised objectives. Overall, all of the objectives ingest a sequence of token IDs corresponding to a tokenized span of text from our unlabeled text data set. The token sequence is processed to produce a (corrupted) input sequence and a corresponding target. Then, the model is trained as usual with maximum likelihood to predict the target sequence.</p>

<h4 id="choices-of-objectives">Choices of Objectives</h4>

<table>
  <thead>
    <tr>
      <th>Objective</th>
      <th>Example input</th>
      <th>Example target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Prefix LM</td>
      <td>Thank you for inviting</td>
      <td>me to your party last week .</td>
    </tr>
    <tr>
      <td>BERT-style</td>
      <td>Thank you <code class="language-plaintext highlighter-rouge">&lt;M&gt;</code> <code class="language-plaintext highlighter-rouge">&lt;M&gt;</code> me to your party <strong>apple</strong> week .</td>
      <td><em>(original text)</em></td>
    </tr>
    <tr>
      <td>Deshuffling</td>
      <td>party me for your to . last fun you inviting week Thank</td>
      <td><em>(original text)</em></td>
    </tr>
    <tr>
      <td>MASS-style</td>
      <td>Thank you <code class="language-plaintext highlighter-rouge">&lt;M&gt;</code> <code class="language-plaintext highlighter-rouge">&lt;M&gt;</code> me to your party <code class="language-plaintext highlighter-rouge">&lt;M&gt;</code> week .</td>
      <td><em>(original text)</em></td>
    </tr>
    <tr>
      <td>I.i.d. noise, replace spans</td>
      <td>Thank you <code class="language-plaintext highlighter-rouge">&lt;X&gt;</code> me to your party <code class="language-plaintext highlighter-rouge">&lt;Y&gt;</code> week .</td>
      <td><code class="language-plaintext highlighter-rouge">&lt;X&gt;</code> for inviting <code class="language-plaintext highlighter-rouge">&lt;Y&gt;</code> last <code class="language-plaintext highlighter-rouge">&lt;Z&gt;</code></td>
    </tr>
    <tr>
      <td>I.i.d. noise, drop tokens</td>
      <td>Thank you me to your party week .</td>
      <td>for inviting last</td>
    </tr>
    <tr>
      <td>Random spans</td>
      <td>Thank you <code class="language-plaintext highlighter-rouge">&lt;X&gt;</code> to <code class="language-plaintext highlighter-rouge">&lt;Y&gt;</code> week .</td>
      <td><code class="language-plaintext highlighter-rouge">&lt;X&gt;</code> for inviting me <code class="language-plaintext highlighter-rouge">&lt;Y&gt;</code> your party last <code class="language-plaintext highlighter-rouge">&lt;Z&gt;</code></td>
    </tr>
  </tbody>
</table>

<h4 id="results-1">Results</h4>

<ul>
  <li><strong>Denoising objectives</strong> outperformed language modeling and deshuffling for pre-training.</li>
  <li>No remarkable difference across the many variants of the denoising objectives.</li>
  <li>Different objectives can lead to different sequence lengths and thus different training speeds.</li>
</ul>

<h3 id="pre-training-dataset">Pre-Training Dataset</h3>

<ul>
  <li>Performance degrades as the data set size shrinks.</li>
  <li>When comparing C4 to data sets that use additional filtering, the authors found that training on in-domain unlabeled data could boost performance in a few downstream tasks. However, constraining to a single domain typically results in a smaller data set.</li>
  <li>Performance can degrade when an unlabeled data set is small enough that it is repeated many times over the course of pre-training. This motivates the use of a large and diverse data set like C4 for generic language understanding tasks.</li>
</ul>

<h3 id="training-strategy">Training Strategy</h3>

<h4 id="fine-tuning-methods">Fine-Tuning Methods</h4>

<p>The standard method is to fine-tune <em>all</em> parameters in the model.</p>

<p>Two alternative methods:</p>

<ul>
  <li><em>Adapter layers:</em> additional dense-ReLU-dense blocks are added after each of the preexisting feed-forward networks in each block of the Transformer. When fine-tuning, only the adapter layer and layer normalization parameters are updated.</li>
  <li><em>gradual unfreezing:</em> more and more of the model’s parameters are fine-tuned over time.</li>
</ul>

<p>The standard method performs best.</p>

<h4 id="multi-task-learning">Multi-Task Learning</h4>

<p>Train the model on multiple tasks simultaneously (the unsupervised task and downstream supervised tasks). For the unified text-to-text framework, “multi-task learning” simply corresponds to mixing data sets
together.</p>

<p>In general, multi-task training underperforms pre-training followed by fine-tuning on most tasks.</p>

<h4 id="combining-multi-task-learning-with-fine-tuning">Combining Multi-Task Learning with Fine-Tuning</h4>

<p>The model is pre-trained on all tasks at once but is then fine-tuned on the individual
supervised tasks.</p>

<p>Fine-tuning after multi-task pre-training results in comparable performance to the baseline (unsupervised pre-training + supervised fine-tuning). This suggests that using fine-tuning after multi-task learning can help mitigate some of the trade-offs between different mixing rates.</p>

<h3 id="scaling">Scaling</h3>

<p>Compared various strategies for taking advantage of additional computing, including training the model on more data, training a larger model, and using an ensemble of models. Each approach conferred a significant boost in performance. Specifically,</p>

<ul>
  <li>Increasing the training time and/or model size consistently improves the baseline.</li>
  <li>In general, increasing the model size resulted in an additional bump in performance compared to solely increasing the training time or batch size.</li>
  <li>Increasing the training time and increasing the model size can be complementary means of improving performance.</li>
  <li>Training a smaller model on more data was often outperformed by training a larger model for fewer steps.</li>
  <li>An ensemble of models can provide substantially better results than a single model, which provides an orthogonal means of leveraging additional computation.</li>
  <li>Ensembling models that were fine-tuned from the same base pre-trained model performed worse than pre-training and fine-tuning all models completely separately, though fine-tune-only ensembling still substantially outperformed a single model.</li>
  <li>Different scaling methods have different trade-offs that are separate from their performance.</li>
</ul>

<h3 id="putting-it-all-together">Putting It All Together</h3>

<p>The final T5 model is as follows.</p>

<ul>
  <li><strong>Objective:</strong> the span-corruption objective, a variant of the denoising objective.</li>
  <li><strong>Longer training:</strong> pre-train for 1M steps on a batch size of 2048 sequences of length 512 corresponding to a total of about 1T pre-training tokens.</li>
  <li><strong>Model sizes:</strong> up to 11B.</li>
  <li><strong>Multi-task pre-training + fine-tuning</strong></li>
  <li><strong>Beam search:</strong> replace greedy decoding by a beam search with a beam width of 4 and a length penalty of \(\alpha=0.6\).</li>
</ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer]]></summary></entry><entry><title type="html">GPT-2</title><link href="https://daviddmc.github.io/blog/2019/GPT-2/" rel="alternate" type="text/html" title="GPT-2" /><published>2019-02-14T00:00:00+00:00</published><updated>2019-02-14T00:00:00+00:00</updated><id>https://daviddmc.github.io/blog/2019/GPT-2</id><content type="html" xml:base="https://daviddmc.github.io/blog/2019/GPT-2/"><![CDATA[<h2 id="takeaways">Takeaways</h2>

<ul>
  <li>GPT-2 begins to learn different tasks without any explicit supervision (<strong>zero-shot transfer</strong>) when trained on a new dataset of millions of webpages (WebText).</li>
  <li>GPT-2 is able to perform new tasks by conditioning on text that specifies the task and the input (<strong>prompt learning</strong>).</li>
  <li>The capacity of the LM is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks.</li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Supervised learning systems are brittle and sensitive to changes in distribution and task (“narrow experts”). The prevalence of single-task training on single-domain datasets might be a major contributor to the lack of generalization observed in current systems.</p>

<p>Multitask learning is a promising framework for improving general performance. However, multi-task training in NLP is still nascent.</p>

<ul>
  <li>Each (dataset, objective) pair is considered as a sample.</li>
  <li>Current ML systems need hundreds to thousands of examples to induce functions that generalize well.</li>
  <li>Difficult to scale with current approaches.</li>
</ul>

<p>The trend of pre-trained language representation NLP:</p>

<ol>
  <li>single-layer pre-trained word embedding + task-specific architectures</li>
  <li>multiple layers of representations (e.g. RNN) + task-specific architectures</li>
  <li>pre-train RNNs or Transformers, and then directly fine-tune, without task-specific architectures (e.g. GPT<d-cite key="GPT"></d-cite>, BERT<d-cite key="BERT"></d-cite>).</li>
</ol>

<p>This work: LM can perform a wide range of downstream tasks in a <strong>zero-shot</strong> setting, without any parameter or architecture modification.</p>

<h2 id="methods">Methods</h2>

<h3 id="language-model">Language Model</h3>

<p>Training LMs in a probabilistic framework as estimating a conditional distribution of the output given the input and the task information (multi-task/meta-learning),</p>

\[p(\texttt{output}|\texttt{input}, \texttt{task})\]

<p>The language model is <em>auto-regressive</em>, i.e., it predicts the next word given the previous words.</p>

<h4 id="task-conditioning">Task Conditioning</h4>

<ul>
  <li>architectural level, e.g., task-specific encoders and decoders</li>
  <li>algorithmic level, e.g., the inner and outer loop optimization framework of MAML</li>
  <li><strong>natural language</strong> provides a flexible way to specify tasks (prompt)</li>
</ul>

<h4 id="speculation">Speculation</h4>

<p>LMs with sufficient capacity will begin to learn to infer and perform the tasks demonstrated in natural language sequences in order to better predict them, regardless of their method of procurement. If LMs are able to do this it will be, in effect, performing unsupervised multitask learning.</p>

<p>Test this by analyzing the performance of LMs in a zero-shot setting on a wide variety of tasks.</p>

<h3 id="training-dataset">Training Dataset</h3>

<p>Although web scrapes such as Common Crawl are many orders of magnitude larger than current language modeling datasets, they have significant data quality issues.</p>

<p>The authors created a new web scrape that emphasizes document quality.</p>

<ol>
  <li>Only scraped web pages that have been curated/filtered by humans: scraped all outbound links from Reddit which received at least 3 karma. This can be thought of as a heuristic indicator for whether other users found the link interesting, educational, or just funny.</li>
  <li>Extract the text from HTML responses</li>
  <li>Ee-duplication and some heuristic-based cleaning</li>
  <li>Removed all Wikipedia documents from WebText since it is a common data source for other datasets and could complicate analysis due to overlapping training data with test evaluation tasks.</li>
</ol>

<p>Results in over 8 million documents for a total of 40 GB of text (about 40 Billion Bytes).</p>

<h3 id="input-representation">Input Representation</h3>

<p>A general LM should be able to compute the probability of (and also generate) any string.</p>

<p>While processing Unicode strings as a sequence of UTF-8 bytes elegantly fulfills this requirement, current byte-level LMs are not competitive with word-level LMs on large-scale datasets.</p>

<h4 id="byte-pair-encoding-bpe">Byte Pair Encoding (BPE)</h4>

<p>An interpolation between word-level inputs for frequent symbol sequences and character-level inputs for infrequent symbol sequences.</p>

<p>BPE on Unicode code points: The size of the base vocabulary is too large (&gt; 130,000) compared to the 32,000 to 64,000 token vocabularies often used with BPE.</p>

<h4 id="bpe-on-byte-level">BPE on Byte Level</h4>

<ol>
  <li>A base vocabulary of size 256</li>
  <li>Naive BPE results in suboptimal merges due to the greedy strategy. To avoid this, the authors prevent BPE from merging across character categories, with an exception for spaces.</li>
  <li>Enable the model to assign a probability to any Unicode string.</li>
</ol>

<h3 id="model">Model</h3>

<p>The model largely follows the details of the GPT<d-cite key="GPT"></d-cite> model with a few modifications.</p>

<ol>
  <li>LayerNorm was moved to the input of each sub-block and an additional LayerNorm was added after the final self-attention block.</li>
  <li>A modified initialization that accounts for the accumulation on the residual path with model depth is used. Scale the weights of residual layers at initialization by a factor of \(1/\sqrt{N}\), where \(N\) is the number of residual layers.</li>
  <li>The vocabulary is expanded to 50,257.</li>
  <li>We also increase the context size from 512 to 1024 tokens,</li>
  <li>A larger batch size of 512 is used.</li>
</ol>

<p>The largest model (GPT-2) has 1.5B parameters.</p>

<h2 id="experiments">Experiments</h2>

<h3 id="language-modeling">Language Modeling</h3>

<p>This is the primary task the models are trained for.</p>

<p>Task: Evaluating the log probability of different datasets according to the LM.</p>

<p>Results: GPT-2 transfers well across domains and datasets, improving the state of the art on 7 out of the 8 datasets in a zero-shot setting.</p>

<h3 id="lambada">LAMBADA</h3>

<p>The LAMBADA dataset tests the ability of systems to model long-range dependencies in text. The</p>

<p>Task: predict the final word of sentences that require at least 50 tokens of context for a human to successfully predict.</p>

<p>Results: GPT-2 improves the SOTA.</p>

<p>Common error: valid continuations of the sentence, but not valid final words.</p>

<p>This suggests that the LM is not using the additional useful constraint that the word must be the final of the sentence. Adding a stop-word filter as an approximation to this further increases accuracy.</p>

<h3 id="reading-comprehension">Reading Comprehension</h3>

<p>The Conversation Question Answering dataset (CoQA) consists of documents from 7 different domains paired with natural language dialogues between a question asker and a question answerer about the document. CoQA tests reading comprehension capabilities and also the ability of models to answer questions that depend on conversation history (such as “Why?”).</p>

<p>Use greedy decoding from GPT-2 conditioned on a document, the history of the associated conversation, and a final token <code class="language-plaintext highlighter-rouge">A:</code>.</p>

<p>Results: GPT-2 matches or exceeds the performance of 3 out of 4 baseline systems, and underperforms the supervised SOTA (BERT-based)</p>

<h3 id="summarization">Summarization</h3>

<p>Induce summarization behavior by adding the text <code class="language-plaintext highlighter-rouge">TL;DR:</code> after the article and generating 100 tokens with Top-\(k\) random sampling with \(k = 2\) which reduces repetition and encourages more abstractive summaries than greedy decoding. The first 3 generated sentences in these 100 tokens are used as the summary.</p>

<p>Results:</p>

<ul>
  <li>GPT-2 often focuses on recent content from the article or confuses specific details.</li>
  <li>GPT-2 only begins to approach the performance of classic neural baselines and just barely outperforms selecting 3 random sentences from the article.</li>
</ul>

<h3 id="translation">Translation</h3>

<p>Help GPT-2 infer the translation task, by conditioning the LM on a context of example pairs of the format <code class="language-plaintext highlighter-rouge">english sentence = french sentence</code> followed by a final prompt of <code class="language-plaintext highlighter-rouge">english sentence =</code>. After the prompt, outputs are sampled with greedy decoding and the first generated sentence is used as the translation.</p>

<p>Results:</p>

<ul>
  <li>English-French: GPT-2 is slightly worse than a word-by-word substitution with a bilingual lexicon.</li>
  <li>French-English: GPT-2 is able to leverage its very strong English LM and outperforms several unsupervised baselines but is still much worse than the SOTA unsupervised approach.</li>
</ul>

<p>Note: Since non-English webpages were filtered from WebText, it only contains a very small (10MB) corpus in the Frech language.</p>

<h3 id="question-answering">Question Answering</h3>

<p>Similar to translation, the context of the language model is seeded with example question answer pairs which helps the model infer the short answer style of the dataset.</p>

<p>Results: The performance of GPT-2 is still much, much, worse than the existing open domain question answering systems.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Language Models are Unsupervised Multitask Learners]]></summary></entry><entry><title type="html">BERT: Bidirectional Encoder Representations from Transformers</title><link href="https://daviddmc.github.io/blog/2018/BERT/" rel="alternate" type="text/html" title="BERT: Bidirectional Encoder Representations from Transformers" /><published>2018-10-11T00:00:00+00:00</published><updated>2018-10-11T00:00:00+00:00</updated><id>https://daviddmc.github.io/blog/2018/BERT</id><content type="html" xml:base="https://daviddmc.github.io/blog/2018/BERT/"><![CDATA[<h2 id="takeaways">Takeaways</h2>

<ul>
  <li>
    <p>BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on <strong>both left and right context</strong> in all layers.</p>
  </li>
  <li>
    <p>The pre-trained BERT model can be fine-tuned with just one additional output layer to create SOTA models for a wide range of tasks.</p>
  </li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Existing strategies for applying pre-trained representations to downstream tasks:</p>

<ul>
  <li><strong>feature-based:</strong> use task-specific architectures that include the pre-trained representations as additional features (e.g., ELMo<d-cite key="ELMo"></d-cite>).</li>
  <li><strong>fine-tuning:</strong> introduce <em>minimal task-specific parameters</em>, and fine-tune <em>all</em> pre-trained parameters on downstream tasks (e.g., GPT<d-cite key="GPT"></d-cite>).</li>
</ul>

<p>GPT uses unidirectional language modeling. This limits the choice of architectures that can be used during pre-training. Such restrictions are sub-optimal for sentence-level tasks and could be very harmful when applying fine-tuning based approaches to token-level tasks.</p>

<p>This work:</p>

<ul>
  <li>improves the fine-tuning based approaches,</li>
  <li>replaces the left-to-right language model by the masked language model (bidirectional),</li>
  <li>use a “next sentence prediction” task.</li>
</ul>

<h2 id="methods">Methods</h2>

<p>Two steps:</p>

<ol>
  <li><strong>Pre-training:</strong> BERT is trained on unlabeled data over different pre-training tasks.</li>
  <li><strong>Fine-Tuning:</strong> BERT is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks.</li>
</ol>

<div class="l-page" style="text-align:center;">
  <img src="https://production-media.paperswithcode.com/methods/new_BERT_Overall.jpg" width="100%" style="margin-bottom: 12px; background-color: white;" />
  <p>Overall pre-training and fine-tuning procedures for BERT.</p>
</div>

<h3 id="model">Model</h3>

<p>BERT uses a multi-layer bidirectional Transformer encoder, i.e., a single stack of transformer layers.</p>

<p>The BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to the context to its left.</p>

<h3 id="inputoutput-representation">Input/Output Representation</h3>

<p>To handle a variety of downstream tasks, BERT takes a single sentence or a pair of sentences (e.g., question and answer) as the input token sequence.</p>

<p>Token:</p>

<ul>
  <li>BERT uses WordPiece embeddings with a 30,000 token vocabulary.</li>
  <li>The first token of every sequence is a special classification token (<code class="language-plaintext highlighter-rouge">[CLS]</code>). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks.</li>
  <li>Sentence pairs are separated with a special token (<code class="language-plaintext highlighter-rouge">[SEP]</code>). A learned embedding is added to every token indicating whether it belongs to sentence <code class="language-plaintext highlighter-rouge">A</code> or sentence <code class="language-plaintext highlighter-rouge">B</code>.</li>
</ul>

<p>For a given token, its input representation is constructed by summing the corresponding <em>token</em>, <em>segment</em> (sentence <code class="language-plaintext highlighter-rouge">A</code> or <code class="language-plaintext highlighter-rouge">B</code>), and <em>position</em> embeddings.</p>

<h3 id="pre-training-bert">Pre-Training BERT</h3>

<h4 id="task-1-masked-lm">Task 1: Masked LM</h4>

<p>Intuitively, a deep bidirectional model is more powerful than either a left-to-right
model or the shallow concatenation of a left-to-right and a right-to-left model.</p>

<p>Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly “see itself”.</p>

<p>Solution: Mask Language Model (MLM), which masks some percentage of the input tokens at random, and then predicts those masked tokens.</p>

<ol>
  <li>BERT “masks” 15% of all tokens in each sequence at random.
    <ul>
      <li>80%: replaced by the <code class="language-plaintext highlighter-rouge">[MASK]</code> token</li>
      <li>10%: replaced by a random token</li>
      <li>10%: unchanged</li>
    </ul>
  </li>
  <li>The final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary.</li>
  <li>Only predict the masked words rather than reconstruct the entire input.</li>
</ol>

<h4 id="task-2-next-sentence-prediction-nsp">Task 2: Next Sentence Prediction (NSP)</h4>

<p>Motivation: Many downstream tasks are based on understanding the relationship between two sentences, which is not directly captured by LM.</p>

<ol>
  <li>Choosing the sentences <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code> for each pre-training example, s.t.
    <ul>
      <li>50% of the time <code class="language-plaintext highlighter-rouge">B</code> is the actual next sentence that follows <code class="language-plaintext highlighter-rouge">A</code> (labeled as <code class="language-plaintext highlighter-rouge">IsNext</code>),</li>
      <li>50% of the time it is a random sentence from the corpus (labeled as <code class="language-plaintext highlighter-rouge">NotNext</code>).</li>
    </ul>
  </li>
  <li>The final hidden vector of the <code class="language-plaintext highlighter-rouge">[CLS]</code> token is used for next sentence prediction.</li>
</ol>

<p>The NSP task is beneficial to both QA and NLI.</p>

<h4 id="pre-training-data">Pre-Training Data</h4>

<ul>
  <li>BooksCorpus (800M words)</li>
  <li>English Wikipedia (2,500M words)</li>
</ul>

<p>It is critical to use a document-level corpus rather than a shuffled sentence-level corpus in order to extract long contiguous sequences.</p>

<h3 id="fine-tuning-bert">Fine-Tuning BERT</h3>

<p>For each task, simply plug in the tasks-specific inputs and outputs into BERT and fine-tune all the parameters end-to-end.</p>

<p>Input sentence pairs</p>

<ul>
  <li>Paraphrasing: sentence pairs</li>
  <li>Entailment: hypothesis-premise pairs</li>
  <li>Question answering: question-passage pairs</li>
  <li>Text classification or sequence tagging: degenerate text-\(\varnothing\) pairs</li>
</ul>

<p>Output</p>

<ul>
  <li>Token level tasks (e.g. sequence tagging or question answering): the token representations are fed into an output layer.</li>
  <li>Text classification (e.g., entailment or sentiment analysis): the <code class="language-plaintext highlighter-rouge">[CLS]</code> representation is fed into an output layer.</li>
</ul>

<h2 id="experiments">Experiments</h2>

<h3 id="glue">GLUE</h3>

<p>The General Language Understanding Evaluation (GLUE) benchmark is a collection of diverse natural language understanding tasks.</p>

<p>Results: Both BERT-base and BERT-large outperform all systems on all tasks by a substantial margin.</p>

<h3 id="squad">SQuAD</h3>

<h4 id="v11">v1.1</h4>

<p>The Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowdsourced question/answer pairs.  Given a question and a passage from Wikipedia containing the answer, the task is to predict the answer text span in the passage.</p>

<p>Represent the input question and passage as a single packed sequence (<code class="language-plaintext highlighter-rouge">A</code>: question, <code class="language-plaintext highlighter-rouge">B</code>: passage).</p>

<p>Only introduce a start vector \(S\in\mathbb{R}^H\) and an end vector \(E\in\mathbb{R}^H\) (trainable weight vectors).</p>

<p>The probability of word \(i\) being the start (or end) of the answer span is computed as a dot product between the output representation \(T_i\) and \(S\) (or \(E\)) followed by a softmax over all of the words in the paragraph:</p>

\[P_i^S = \frac{e^{S\cdot T_i}}{\sum_j e^{S\cdot T_j}} \qquad\text{ or }\qquad P_i^E\frac{e^{E\cdot T_i}}{\sum_j e^{E\cdot T_j}}.\]

<p>Loss: the sum of the log-likelihoods of the correct start and end positions.</p>

<p>Prediction: Given a span \((i,j)\), compute the score as \(S\cdot T_i + E\cdot T_j\), and use the maximum scoring span where \(j\ge i\) as a prediction.</p>

<h4 id="v20">v2.0</h4>

<p>The SQuAD 2.0 task extends the SQuAD 1.1 problem definition by allowing for the possibility that no short answer exists in the provided paragraph, making the problem more realistic.</p>

<p>Model modification: treat questions that do not have an answer as having an answer span with start and end at the <code class="language-plaintext highlighter-rouge">[CLS]</code> token. For prediction, the score of the no-answer span: \(s_\texttt{null}\) is compared to the best non-null span \(\hat{s}_{i,j}\). A non-null answer is chosen when \(\hat{s}_{i,j}&gt;s_\texttt{null}+\tau\), where \(\tau\) is a threshold. \(s_\texttt{null}=S\cdot C+ E\cdot C\), where \(C\) is the output representation of the <code class="language-plaintext highlighter-rouge">[CLS]</code> token.</p>

<p>Results: BERT outperforms the previous best systems on both SQuAD v1.1 and v2.0.</p>

<h3 id="swag">SWAG</h3>

<p>The Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair completion examples that evaluate grounded commonsense inference. Given a sentence, the task is to choose the most plausible continuation among four choices.</p>

<p>Input: construct four input sequences, each is the concatenation of the given sentence <code class="language-plaintext highlighter-rouge">A</code> and a possible continuation <code class="language-plaintext highlighter-rouge">B</code>.</p>

<p>Output: introduce a vector whose dot product with the <code class="language-plaintext highlighter-rouge">[CLS]</code> token representation \(C\) denotes a score for each choice.</p>

<p>Results: BERT-large outperforms the ELMo and GPT.</p>

<h3 id="ablation-studies">Ablation Studies</h3>

<h4 id="effect-of-pre-training-tasks">Effect of Pre-training Tasks</h4>

<p>Different pre-training tasks:</p>

<ul>
  <li><strong>BERT</strong></li>
  <li><strong>No NSP:</strong> BERT without the “next sentence prediction” task</li>
  <li><strong>LTR &amp; No NSP:</strong> A standard Left-to-Right (LTR) LM without NSP, similar to GPT</li>
</ul>

<p>Results:</p>

<ul>
  <li>Removing NSP hurts the performance of BERT.</li>
  <li>The LTR model performs worse than the MLM model on all tasks</li>
</ul>

<h4 id="effect-of-model-size">Effect of Model Size</h4>

<ul>
  <li>Larger models lead to a strict accuracy improvement across different datasets.</li>
  <li>This work demonstrates that scaling to extreme model sizes also leads to large improvements on very <em>small-scale</em> tasks, provided that the model has been sufficiently pre-trained.</li>
  <li>Previous work shows evidence that larger models might not help the <em>feature-based</em> approaches.</li>
</ul>

<h4 id="feature-based-approach-with-bert">Feature-based Approach with BERT</h4>

<p>Advantages of the feature-based approaches:</p>

<ul>
  <li>Not all tasks can be easily represented by a Transformer encoder architecture, and therefore require a task-specific model architecture to be added.</li>
  <li>There are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation.</li>
</ul>

<p>To ablate the fine-tuning approach, the authors apply the feature-based approach by extracting the activations from one or more layers without fine-tuning any parameters of BERT. These contextual embeddings are used as input to a randomly initialized two-layer BiLSTM before the classification layer.</p>

<p>Results:</p>

<ul>
  <li>Fine-tuning outperforms feature-based approaches.</li>
  <li>The best-performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which achieves performance similar to fine-tuning.</li>
  <li>BERT is effective for both finetuning and feature-based approaches.</li>
</ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Pre-training of Deep Bidirectional Transformers for Language Understanding]]></summary></entry></feed>