<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    
    <!-- Website verification -->
    <meta name="google-site-verification" content="MARJu3erA7Z6jybcmR5fgSNRNtCNYMAYkk0jidGGr8A">
<!-- Avoid warning on Google Chrome
        Error with Permissions-Policy header: Origin trial controlled feature not enabled: 'interest-cohort'.
        see https://stackoverflow.com/a/75119417
    -->
    <meta http-equiv="Permissions-Policy" content="interest-cohort=()">

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>publications | Junshen  Xu</title>
    <meta name="author" content="Junshen  Xu">
    <meta name="description" content="publications">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/favicon_v2.ico">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://daviddmc.github.io/publications/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Junshen </span>Xu</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item active">
                <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">teaching</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- publication.html -->

<div class="post">

    <header class="post-header">
        <h1 class="post-title">publications</h1>
        <p class="post-description"><span class="star">*</span> denotes equal contribution</p>
        <!--<p class="post-description"> </p>-->
    </header>

    <article>
        <!-- _pages/publications.md -->

<p>An up-to-date list is available on <a href="https://scholar.google.com/citations?user=qNk6tgcAAAAJ" target="_blank" rel="noopener noreferrer">Google Scholar</a>.</p>

<div class="publications">
  <h2 class="year">2023</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://dspace.mit.edu/" rel="external nofollow noopener" target="_blank">Thesis</a></abbr></div>

  <!-- Entry bib key -->
  <div id="phdthesisjx" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">A Robust and Efficient Framework for Slice-to-Volume Reconstruction: Application to Fetal MRI</div>
    <!-- Author -->
    <div class="author">
      

      <em>Junshen Xu</em>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Massachusetts Institute of Technology</em>, 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Volumetric reconstruction in presence of motion is a challenging problem in medical imaging. When imaging moving targets, many modalities are limited to fast 2D imaging techniques that provide cross-sectional snapshots (2D images) of the subject with an attempt to "freeze" in-plane motion. However, inter-slice movement results in slice misalignment in 3D space, i.e., each image being an independent slice that fails to form a coherent volume for diagnosis and analysis. To this end, slice-to-volume reconstruction (SVR) has been proposed to reconstruct a high-quality 3D volume from misaligned 2D observations by performing inter-slice motion correction and super-resolution reconstruction. Existing SVR algorithms, however, have a limited capture range of slice motion and are time-consuming, particularly when producing high-resolution volumes. This thesis proposes a motion-robust and efficient machine learning framework for SVR, motivated by the application of magnetic resonance imaging (MRI) in assessing fetal brain development. We first introduce a slice-to-volume registration transformer that models input slices as a sequence and performs inter-slice motion correction by simultaneously predicting rigid transformations of all images in 3D space. We then reformulate the reconstruction problem using implicit neural representation, where the underlying volume is represented by a continuous function of 3D coordinates. This resolution-agnostic approach allows efficient reconstruction of high-resolution volumes. Finally, we extend this method to data that suffer from non-rigid motion by introducing an implicit motion field that captures slice-dependent deformation. These advances together enable robust and efficient 3D reconstruction and visualization in fetal MRI, benefiting diagnosis and downstream analysis. Additionally, the proposed framework has the potential for broader clinical implications in various applications that involve similar volumetric reconstruction problems.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="recon_60_axgif" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/recon_60_ax.gif"><div id="recon_60_axgif-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('recon_60_axgif-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="recon_60_axgif-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("recon_60_axgif-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('recon_60_axgif');
      var modalImg = document.getElementById("recon_60_axgif-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="10015091" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">NeSVoR: Implicit Neural Representation for Slice-to-Volume Reconstruction in MRI</div>
    <!-- Author -->
    <div class="author">
      

      <em>Junshen Xu</em>, <a href="https://dcmoyer.github.io/" rel="external nofollow noopener" target="_blank">Daniel Moyer</a>, <a href="https://scholar.google.com/citations?user=mXUhe04AAAAJ" rel="external nofollow noopener" target="_blank">Borjan Gagoski</a>, <a href="https://scholar.harvard.edu/iglesias" rel="external nofollow noopener" target="_blank">Juan Eugenio Iglesias</a>, and
      <span class="more-authors" title="click to view 3 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '3 more authors' ? 'P. Ellen Grant, Polina Golland, Elfar Adalsteinsson' : '3 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">3 more authors</span>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>IEEE Transactions on Medical Imaging</em>, 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://ieeexplore.ieee.org/document/10015091" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://www.techrxiv.org/articles/preprint/NeSVoR_Implicit_Neural_Representation_for_Slice-to-Volume_Reconstruction_in_MRI/21398868" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
      <a href="https://github.com/daviddmc/NeSVoR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Reconstructing 3D MR volumes from multiple motion-corrupted stacks of 2D slices has shown promise in imaging of moving subjects, e.g., fetal MRI. However, existing slice-to-volume reconstruction methods are time-consuming, especially when a high-resolution volume is desired. Moreover, they are still vulnerable to severe subject motion and when image artifacts are present in acquired slices. In this work, we present NeSVoR, a resolution-agnostic slice-to-volume reconstruction method, which models the underlying volume as a continuous function of spatial coordinates with implicit neural representation. To improve robustness to subject motion and other image artifacts, we adopt a continuous and comprehensive slice acquisition model that takes into account rigid inter-slice motion, point spread function, and bias fields. NeSVoR also estimates pixel-wise and slice-wise variances of image noise and enables removal of outliers during reconstruction and visualization of uncertainty. Extensive experiments are performed on both simulated and in vivo data to evaluate the proposed method. Results show that NeSVoR achieves state-of-the-art reconstruction quality while providing two to ten-fold acceleration in reconstruction times over the state-of-the-art algorithms.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://onlinelibrary.wiley.com/journal/15222594" rel="external nofollow noopener" target="_blank">MRM</a></abbr></div>

  <!-- Entry bib key -->
  <div id="https://doi.org/10.48550/arxiv.2208.13003" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Latent Signal Models: Learning Compact Representations of Signal Evolution for Improved Time-Resolved, Multi-Contrast MRI</div>
    <!-- Author -->
    <div class="author">
      

      <a href="https://yaminarefeen.github.io/" rel="external nofollow noopener" target="_blank">Yamin Arefeen</a>, <em>Junshen Xu</em>, <a href="https://scholar.google.com/citations?user=M6LlDA4AAAAJ" rel="external nofollow noopener" target="_blank">Molin Zhang</a>, Zijing Dong, and
      <span class="more-authors" title="click to view 4 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '4 more authors' ? 'Fuyixue Wang, Jacob White, Berkin Bilgic, Elfar Adalsteinsson' : '4 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">4 more authors</span>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Magnetic Resonance in Medicine</em>, 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.29657" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="http://arxiv.org/abs/2208.13003" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Purpose: Training auto-encoders on simulated signal evolution and inserting the decoder into the forward model improves reconstructions through more compact, Bloch-equation-based representations of signal in comparison to linear subspaces. Methods: Building on model-based nonlinear and linear subspace techniques that enable reconstruction of signal dynamics, we train auto-encoders on dictionaries of simulated signal evolution to learn more compact, non-linear, latent representations. The proposed Latent Signal Model framework inserts the decoder portion of the auto-encoder into the forward model and directly reconstructs the latent representation. Latent Signal Models essentially serve as a proxy for fast and feasible differentiation through the Bloch-equations used to simulate signal. This work performs experiments in the context of T2-shuffling, gradient echo EPTI, and MPRAGE-shuffling. We compare how efficiently auto-encoders represent signal evolution in comparison to linear subspaces. Simulation and in-vivo experiments then evaluate if reducing degrees of freedom by inserting the decoder into the forward model improves reconstructions in comparison to subspace constraints. Results: An auto-encoder with one real latent variable represents FSE, EPTI, and MPRAGE signal evolution as well as linear subspaces characterized by four basis vectors. In simulated/in-vivo T2-shuffling and in-vivo EPTI experiments, the proposed framework achieves consistent quantitative NRMSE and qualitative improvement over linear approaches. From qualitative evaluation, the proposed approach yields images with reduced blurring and noise amplification in MPRAGE shuffling experiments. Conclusion: Directly solving for non-linear latent representations of signal evolution improves time-resolved MRI reconstructions through reduced degrees of freedom.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://www.midl.io/" rel="external nofollow noopener" target="_blank">MIDL</a></abbr></div>

  <!-- Entry bib key -->
  <div id="zhang2023zero" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Zero-Shot Self-Supervised Joint Temporal Image and Sensitivity Map Reconstruction via Linear Latent Space</div>
    <!-- Author -->
    <div class="author">
      

      <a href="https://scholar.google.com/citations?user=M6LlDA4AAAAJ" rel="external nofollow noopener" target="_blank">Molin Zhang</a>, <em>Junshen Xu</em>, <a href="https://yaminarefeen.github.io/" rel="external nofollow noopener" target="_blank">Yamin Arefeen</a>, and <a href="https://www.rle.mit.edu/people/directory/elfar-adalsteinsson/" rel="external nofollow noopener" target="_blank">Elfar Adalsteinsson</a>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>In Medical Imaging with Deep Learning – MIDL 2023</em>
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="http://arxiv.org/abs/2303.02254" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Fast spin-echo (FSE) pulse sequences for Magnetic Resonance Imaging (MRI) offer important imaging contrast in clinically feasible scan times. T2-shuffling is widely used to resolve temporal signal dynamics in FSE acquisitions by exploiting temporal correlations via linear latent space and a predefined regularizer. However, predefined regularizers fail to exploit the incoherence especially for 2D acquisitions.Recent self-supervised learning methods achieve high-fidelity reconstructions by learning a regularizer from undersampled data without a standard supervised training data set. In this work, we propose a novel approach that utilizes a self supervised learning framework to learn a regularizer constrained on a linear latent space which improves time-resolved FSE images reconstruction quality. Additionally, in regimes without groundtruth sensitivity maps, we propose joint estimation of coil-sensitivity maps using an iterative reconstruction technique. Our technique functions is in a zero-shot fashion, as it only utilizes data from a single scan of highly undersampled time series images. We perform experiments on simulated and retrospective in-vivo data to evaluate the performance of the proposed zero-shot learning method for temporal FSE reconstruction. The results demonstrate the success of our proposed method where NMSE and SSIM are significantly increased and the artifacts are reduced.</p>
    </div>
  </div>
</div>
</li>
</ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="svortpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/svort.png"><div id="svortpng-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('svortpng-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="svortpng-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("svortpng-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('svortpng');
      var modalImg = document.getElementById("svortpng-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="10.1007/978-3-031-16446-0_1" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">SVoRT: Iterative Transformer for Slice-to-Volume Registration in Fetal Brain MRI</div>
    <!-- Author -->
    <div class="author">
      

      <em>Junshen Xu</em>, <a href="https://dcmoyer.github.io/" rel="external nofollow noopener" target="_blank">Daniel Moyer</a>, <a href="https://scholar.google.com/citations?user=W4dqZ7EAAAAJ" rel="external nofollow noopener" target="_blank">P. Ellen Grant</a>, <a href="https://people.csail.mit.edu/polina/" rel="external nofollow noopener" target="_blank">Polina Golland</a>, and
      <span class="more-authors" title="click to view 2 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '2 more authors' ? 'Juan Eugenio Iglesias, Elfar Adalsteinsson' : '2 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">2 more authors</span>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>In Medical Image Computing and Computer Assisted Intervention – MICCAI 2022</em>
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://doi.org/10.1007/978-3-031-16446-0_1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="http://arxiv.org/abs/2206.10802" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      <a href="https://github.com/daviddmc/SVoRT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      <a href="/assets/pdf/svort_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Volumetric reconstruction of fetal brains from multiple stacks of MR slices, acquired in the presence of almost unpredictable and often severe subject motion, is a challenging task that is highly sensitive to the initialization of slice-to-volume transformations. We propose a novel slice-to-volume registration method using Transformers trained on synthetically transformed data, which model multiple stacks of MR slices as a sequence. With the attention mechanism, our model automatically detects the relevance between slices and predicts the transformation of one slice using information from other slices. We also estimate the underlying 3D volume to assist slice-to-volume registration and update the volume and transformations alternately to improve accuracy. Results on synthetic data show that our method achieves lower registration error and better reconstruction quality compared with existing state-of-the-art methods. Experiments with real-world MRI data are also performed to demonstrate the ability of the proposed model to improve the quality of 3D reconstruction under severe fetal motion.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="iqa_looppng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/iqa_loop.png"><div id="iqa_looppng-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('iqa_looppng-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="iqa_looppng-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("iqa_looppng-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('iqa_looppng');
      var modalImg = document.getElementById("iqa_looppng-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="https://doi.org/10.1002/mrm.29106" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Automated Detection and Reacquisition of Motion-Degraded Images in Fetal HASTE Imaging at 3 T</div>
    <!-- Author -->
    <div class="author">
      

      <a href="https://scholar.google.com/citations?user=mXUhe04AAAAJ" rel="external nofollow noopener" target="_blank">Borjan Gagoski*</a>, <em>Junshen Xu*</em>, Paul Wighton, M. Dylan Tisdall, and
      <span class="more-authors" title="click to view 6 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '6 more authors' ? 'Robert Frost, Wei-Ching Lo, Polina Golland, Andre Kouwe, Elfar Adalsteinsson, P. Ellen Grant' : '6 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">6 more authors</span>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Magnetic Resonance in Medicine</em>, 2022
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.29106" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Purpose: Fetal brain Magnetic Resonance Imaging suffers from unpredictable and unconstrained fetal motion that causes severe image artifacts even with half-Fourier single-shot fast spin echo (HASTE) readouts. This work presents the implementation of a closed-loop pipeline that automatically detects and reacquires HASTE images that were degraded by fetal motion without any human interaction. Methods: A convolutional neural network that performs automatic image quality assessment (IQA) was run on an external GPU-equipped computer that was connected to the internal network of the MRI scanner. The modified HASTE pulse sequence sent each image to the external computer, where the IQA convolutional neural network evaluated it, and then the IQA score was sent back to the sequence. At the end of the HASTE stack, the IQA scores from all the slices were sorted, and only slices with the lowest scores (corresponding to the slices with worst image quality) were reacquired. Results: The closed-loop HASTE acquisition framework was tested on 10 pregnant mothers, for a total of 73 acquisitions of our modified HASTE sequence. The IQA convolutional neural network, which was successfully employed by our modified sequence in real time, achieved an accuracy of 85.2% and area under the receiver operator characteristic of 0.899. Conclusion: The proposed acquisition/reconstruction pipeline was shown to successfully identify and automatically reacquire only the motion degraded fetal brain HASTE slices in the prescribed stack. This minimizes the overall time spent on HASTE acquisitions by avoiding the need to repeat the entire stack if only few slices in the stack are motion-degraded.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://www.karger.com/journal/Home/224107" rel="external nofollow noopener" target="_blank">DNE</a></abbr></div>

  <!-- Entry bib key -->
  <div id="vasung2022cross" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Cross-sectional Observational Study of Typical in-utero Fetal Movements using Machine Learning</div>
    <!-- Author -->
    <div class="author">
      

      Lana Vasung*, <em>Junshen Xu*</em>, Esra Abaci-Turk*, Cindy Zhou, and
      <span class="more-authors" title="click to view 9 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '9 more authors' ? 'Elizabeth Holland, William H Barth, Carol Barnewolt, Susan Connolly, Judy Estroff, Polina Golland, Henry Feldman, Elfar Adalsteinsson, P. Ellen Grant' : '9 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">9 more authors</span>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Developmental Neuroscience</em>, 2022
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://doi.org/10.1159/000528757" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Early variations of fetal movements are the hallmark of a healthy developing central nervous system. However, there are no automatic methods to quantify the complex 3D motion of the developing fetus in-utero. The aim of this prospective study was to use machine learning (ML) on in-utero MRI to perform quantitative kinematic analysis of fetal limb movement, assessing the impact of maternal, placental, and fetal factors. In this cross-sectional, observational study, we used 76 sets of fetal (24-40 gestational weeks (GW)) blood oxygenation level-dependent (BOLD) MRI scans of 52 women (18-45 years old) during typical pregnancies. Pregnant women were scanned for 5 to 10 minutes while breathing room air (21% O2) and for 5 to 10 minutes while breathing 100% FiO2 in supine and/or lateral position. BOLD acquisition time was 20 minutes in total with an effective temporal resolution of approximately 3 seconds. To quantify upper and lower limb kinematics, we used a 3D convolutional neural network (CNN) previously trained to track fetal key points (wrists, elbows, shoulders, ankles, knees, hips) on similar BOLD time series. Tracking was visually assessed, errors manually corrected and the absolute movement time (AMT) for each joint was calculated. To identify variables that had a significant association with AMT, we constructed a mixed-model ANOVA with interaction terms. Fetuses showed significantly longer duration of limb movements during maternal hyperoxia. We also found a significant centrifugal increase of AMT across limbs and significantly longer AMT of upper extremities &lt; 31 GW and longer AMT of lower extremities &gt; 35 GW. In conclusion, using ML we successfully quantified complex 3D fetal limb motion in-utero and across gestation, showing maternal factors (hyperoxia) and fetal factors (gestational age, joint) impact movement. Quantification of fetal motion on MRI is a potential new biomarker of fetal health and neuromuscular development.</p>
    </div>
  </div>
</div>
</li>
</ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="stressgif" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/stress.gif"><div id="stressgif-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('stressgif-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="stressgif-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("stressgif-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('stressgif');
      var modalImg = document.getElementById("stressgif-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="10.1007/978-3-030-87234-2_19" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">STRESS: Super-Resolution for Dynamic Fetal MRI Using Self-Supervised Learning</div>
    <!-- Author -->
    <div class="author">
      

      <em>Junshen Xu</em>, <a href="https://scholar.google.com/citations?user=HYUyyTsAAAAJ" rel="external nofollow noopener" target="_blank">Esra Abaci Turk</a>, <a href="https://scholar.google.com/citations?user=W4dqZ7EAAAAJ" rel="external nofollow noopener" target="_blank">P. Ellen Grant</a>, <a href="https://people.csail.mit.edu/polina/" rel="external nofollow noopener" target="_blank">Polina Golland</a>, and
      <span class="more-authors" title="click to view 1 more author" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '1 more author' ? 'Elfar Adalsteinsson' : '1 more author';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">1 more author</span>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>In Medical Image Computing and Computer Assisted Intervention – MICCAI 2021</em>
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://doi.org/10.1007/978-3-030-87234-2_19" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="http://arxiv.org/abs/2106.12407" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      <a href="https://github.com/daviddmc/STRESS" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      <a href="/assets/pdf/stress_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Fetal motion is unpredictable and rapid on the scale of conventional MR scan times. Therefore, dynamic fetal MRI, which aims at capturing fetal motion and dynamics of fetal function, is limited to fast imaging techniques with compromises in image quality and resolution. Super-resolution for dynamic fetal MRI is still a challenge, especially when multi-oriented stacks of image slices for oversampling are not available and high temporal resolution for recording the dynamics of the fetus or placenta is desired. Further, fetal motion makes it difficult to acquire high-resolution images for supervised learning methods. To address this problem, in this work, we propose STRESS (Spatio-Temporal Resolution Enhancement with Simulated Scans), a self-supervised super-resolution framework for dynamic fetal MRI with interleaved slice acquisitions. Our proposed method simulates an interleaved slice acquisition along the high-resolution axis on the originally acquired data to generate pairs of low- and high-resolution images. Then, it trains a super-resolution network by exploiting both spatial and temporal correlations in the MR time series, which is used to enhance the resolution of the original data. Evaluations on both simulated and in utero data show that our proposed method outperforms other self-supervised super-resolution methods and improves image quality, which is beneficial to other downstream tasks and evaluations.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="deform2selfpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/deform2self.png"><div id="deform2selfpng-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('deform2selfpng-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="deform2selfpng-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("deform2selfpng-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('deform2selfpng');
      var modalImg = document.getElementById("deform2selfpng-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="10.1007/978-3-030-87196-3_3" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Deformed2Self: Self-Supervised Denoising for Dynamic Medical Imaging</div>
    <!-- Author -->
    <div class="author">
      

      <em>Junshen Xu</em>, and <a href="https://www.rle.mit.edu/people/directory/elfar-adalsteinsson/" rel="external nofollow noopener" target="_blank">Elfar Adalsteinsson</a>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>In Medical Image Computing and Computer Assisted Intervention – MICCAI 2021</em>
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://doi.org/10.1007/978-3-030-87196-3_3" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="http://arxiv.org/abs/2106.12175" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      <a href="https://github.com/daviddmc/Deform2Self" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      <a href="/assets/pdf/deform2self_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Image denoising is of great importance for medical imaging system, since it can improve image quality for disease diagnosis and downstream image analyses. In a variety of applications, dynamic imaging techniques are utilized to capture the time-varying features of the subject, where multiple images are acquired for the same subject at different time points. Although signal-to-noise ratio of each time frame is usually limited by the short acquisition time, the correlation among different time frames can be exploited to improve denoising results with shared information across time frames. With the success of neural networks in computer vision, supervised deep learning methods show prominent performance in single-image denoising, which rely on large datasets with clean-vs-noisy image pairs. Recently, several self-supervised deep denoising models have been proposed, achieving promising results without needing the pairwise ground truth of clean images. In the field of multi-image denoising, however, very few works have been done on extracting correlated information from multiple slices for denoising using self-supervised deep learning methods. In this work, we propose Deformed2Self, an end-to-end self-supervised deep learning framework for dynamic imaging denoising. It combines single-image and multi-image denoising to improve image quality and use a spatial transformer network to model motion between different slices. Further, it only requires a single noisy image with a few auxiliary observations at different time frames for training and inference. Evaluations on phantom and in vivo data with different noise statistics show that our method has comparable performance to other state-of-the-art unsupervised or self-supervised denoising methods and outperforms under high noise levels.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="odepng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/ode.png"><div id="odepng-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('odepng-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="odepng-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("odepng-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('odepng');
      var modalImg = document.getElementById("odepng-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="10.1007/978-3-030-87202-1_21" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Multi-Scale Neural ODEs for 3D Medical Image Registration</div>
    <!-- Author -->
    <div class="author">
      

      <em>Junshen Xu</em>, Eric Z. Chen, Xiao Chen, Terrence Chen, and
      <span class="more-authors" title="click to view 1 more author" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '1 more author' ? 'Shanhui Sun' : '1 more author';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">1 more author</span>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>In Medical Image Computing and Computer Assisted Intervention – MICCAI 2021</em>
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://doi.org/10.1007/978-3-030-87202-1_21" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="http://arxiv.org/abs/2106.08493" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      <a href="/assets/pdf/ode_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Image registration plays an important role in medical image analysis. Conventional optimization based methods provide an accurate estimation due to the iterative process at the cost of expensive computation. Deep learning methods such as learn-to-map are much faster but either iterative or coarse-to-fine approach is required to improve accuracy for handling large motions. In this work, we proposed to learn a registration optimizer via a multi-scale neural ODE model. The inference consists of iterative gradient updates similar to a conventional gradient descent optimizer but in a much faster way, because the neural ODE learns from the training data to adapt the gradient efficiently at each iteration. Furthermore, we proposed to learn a modal-independent similarity metric to address image appearance variations across different image contrasts. We performed evaluations through extensive experiments in the context of multi-contrast 3D MR images from both public and private data sources and demonstrate the superior performance of our proposed methods.</p>
    </div>
  </div>
</div>
</li>
</ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://dspace.mit.edu/" rel="external nofollow noopener" target="_blank">Thesis</a></abbr></div>

  <!-- Entry bib key -->
  <div id="masterthesisjx" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Online, Low-Latency Decision Making for Fetal Magnetic Resonance Imaging with Machine Learning</div>
    <!-- Author -->
    <div class="author">
      

      <em>Junshen Xu</em>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Massachusetts Institute of Technology</em>, 2020
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://dspace.mit.edu/handle/1721.1/127446" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Fetal Magnetic Resonance Imaging (MRI) with T2-weighted Half-Fourier-Acquisition Single-Shot Turbo-Spin-Echo (HASTE) sequence plays an important role in diagnosing brain abnormality. However, the quality of HASTE images routinely suffer from fetal motion which leads to image artifacts, incomplete brain coverage as well as longer scan times. To address this problem, interleaved 3D Echo-planar Imaging (EPI) navigators are acquired along with HASTE images, which can provide pose information for prospective motion correction. In this thesis, we first propose a fetal pose estimation model which detects important fetal landmarks from 3D EPI data using a deep convolution neural network. We further demonstrate its capability by applying this model to fetal motion analysis. In an attempt to improve the current fetal MRI protocol, we develop a machine learning based online decision making system for fetal MRI to improve the efficiency of acquiring high quality HASTE images for clinical diagnosis. The proposed system leverages an Image Quality Assessment (IQA) network to determine whether an acquired HASTE slice is contaminated by motion artifacts and improves image quality by re-acquisition. Evaluation on retrospective experiments and in vivo scans suggests that the proposed pipeline can improve image quality with a reasonable number of re-acquisition, potentially enabling a more efficient workflow for fetal brain MRI.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="iqa_networkpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/iqa_network.png"><div id="iqa_networkpng-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('iqa_networkpng-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="iqa_networkpng-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("iqa_networkpng-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('iqa_networkpng');
      var modalImg = document.getElementById("iqa_networkpng-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="10.1007/978-3-030-59725-2_37" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Semi-Supervised Learning for Fetal Brain MRI Quality Assessment with ROI Consistency</div>
    <!-- Author -->
    <div class="author">
      

      <em>Junshen Xu</em>, Sayeri Lala, <a href="https://scholar.google.com/citations?user=mXUhe04AAAAJ" rel="external nofollow noopener" target="_blank">Borjan Gagoski</a>, Esra Abaci Turk, and
      <span class="more-authors" title="click to view 3 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '3 more authors' ? 'P. Ellen Grant, Polina Golland, Elfar Adalsteinsson' : '3 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">3 more authors</span>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>In Medical Image Computing and Computer Assisted Intervention – MICCAI 2020</em>
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://doi.org/10.1007/978-3-030-59725-2_37" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="http://arxiv.org/abs/2006.12704" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      <a href="https://github.com/daviddmc/fetal-IQA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      <a href="/assets/pdf/iqa_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Fetal brain MRI is useful for diagnosing brain abnormalities but is challenged by fetal motion. The current protocol for T2-weighted fetal brain MRI is not robust to motion so image volumes are degraded by inter- and intra- slice motion artifacts. Besides, manual annotation for fetal MR image quality assessment are usually time-consuming. Therefore, in this work, a semi-supervised deep learning method that detects slices with artifacts during the brain volume scan is proposed. Our method is based on the mean teacher model, where we not only enforce consistency between student and teacher models on the whole image, but also adopt an ROI consistency loss to guide the network to focus on the brain region. The proposed method is evaluated on a fetal brain MR dataset with 11,223 labeled images and more than 200,000 unlabeled images. Results show that compared with supervised learning, the proposed method can improve model accuracy by about 6% and outperform other state-of-the-art semi-supervised learning methods. The proposed method is also implemented and evaluated on an MR scanner, which demonstrates the feasibility of online image quality assessment and image reacquisition during fetal MR scans.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 abbr"><abbr class="badge"><a href="http://www.miccai.org/" rel="external nofollow noopener" target="_blank">MICCAI</a></abbr></div>

  <!-- Entry bib key -->
  <div id="10.1007/978-3-030-59725-2_38" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Enhanced Detection of Fetal Pose in 3D MRI by Deep Reinforcement Learning with Physical Structure Priors on Anatomy</div>
    <!-- Author -->
    <div class="author">
      

      <a href="https://scholar.google.com/citations?user=M6LlDA4AAAAJ" rel="external nofollow noopener" target="_blank">Molin Zhang*</a>, <em>Junshen Xu*</em>, Esra Abaci Turk, <a href="https://scholar.google.com/citations?user=W4dqZ7EAAAAJ" rel="external nofollow noopener" target="_blank">P. Ellen Grant</a>, and
      <span class="more-authors" title="click to view 2 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '2 more authors' ? 'Polina Golland, Elfar Adalsteinsson' : '2 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">2 more authors</span>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>In Medical Image Computing and Computer Assisted Intervention – MICCAI 2020</em>
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://doi.org/10.1007/978-3-030-59725-2_38" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Fetal MRI is heavily constrained by unpredictable and substantial fetal motion that causes image artifacts and limits the set of viable diagnostic image contrasts. Current mitigation of motion artifacts is predominantly performed by fast, single-shot MRI and retrospective motion correction. Estimation of fetal pose in real time during MRI stands to benefit prospective methods to detect and mitigate fetal motion artifacts where inferred fetal motion is combined with online slice prescription with low-latency decision making. Current developments of deep reinforcement learning (DRL), offer a novel approach for fetal landmarks detection. In this task 15 agents are deployed to detect 15 landmarks simultaneously by DRL. The optimization is challenging, and here we propose an improved DRL that incorporates priors on physical structure of the fetal body. First, we use graph communication layers to improve the communication among agents based on a graph where each node represents a fetal-body landmark. Further, additional reward based on the distance between agents and physical structures such as the fetal limbs is used to fully exploit physical structure. Evaluation of this method on a repository of 3-mm resolution in vivo data demonstrates a mean accuracy of landmark estimation 10 mm of ground truth as 87.3%, and a mean error of 6.9 mm. The proposed DRL for fetal pose landmark search demonstrates a potential clinical utility for online detection of fetal motion that guides real-time mitigation of motion artifacts as well as health diagnosis during MRI of the pregnant mother.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://pippiworkshop.github.io/" rel="external nofollow noopener" target="_blank">PIPPI</a></abbr></div>

  <!-- Entry bib key -->
  <div id="10.1007/978-3-030-60334-2_20" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">3D Fetal Pose Estimation with Adaptive Variance and Conditional Generative Adversarial Network</div>
    <!-- Author -->
    <div class="author">
      

      <em>Junshen Xu</em>, <a href="https://scholar.google.com/citations?user=M6LlDA4AAAAJ" rel="external nofollow noopener" target="_blank">Molin Zhang</a>, <a href="https://scholar.google.com/citations?user=HYUyyTsAAAAJ" rel="external nofollow noopener" target="_blank">Esra Abaci Turk</a>, <a href="https://scholar.google.com/citations?user=W4dqZ7EAAAAJ" rel="external nofollow noopener" target="_blank">P. Ellen Grant</a>, and
      <span class="more-authors" title="click to view 2 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '2 more authors' ? 'Polina Golland, Elfar Adalsteinsson' : '2 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">2 more authors</span>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>In Medical Ultrasound, and Preterm, Perinatal and Paediatric Image Analysis</em>, 2020
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://doi.org/10.1007/978-3-030-60334-2_20" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://github.com/daviddmc/fetal-pose" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Fetal motion is the dominant challenge to reliable performance and diagnostic quality of fetal magnetic resonance imaging (MRI). The fetus can move unpredictably and rapidly, leading to severe image artifacts. Consequently, MR acquisitions are largely limited to so-called single-shot techniques in an attempt to “freeze” fetal motion through fast imaging, while the problem due to motion occur between slices still exists. In this work, we propose a deep learning method for fetal pose estimation from MR volumes using the paradigm of conditional generative adversarial network which consists of two networks, a generator and a discriminator. The generator is responsible for estimating keypoint heatmaps from input MRI and the discriminator tries to learn the features of plausible fetal pose and distinguish ground-truth heatmaps from generated ones. With this adversarial training scheme, the generator can robustly produce realistic heatmaps for fetal pose inference. Besides, we use adaptive variance to model the difference in intensity of motion of different keypoints. Evaluation shows that the proposed method can improve the performance of pose estimation in 3D MRI, achieving quantitatively an average error of 2.64 mm and 98.31% accuracy (with error less than 10 mm). The proposed method can process volumes with latency less than 300 ms, potentially enabling low-latency online tracking of fetal pose during MR scans.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="low_dose_networkpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/low_dose_network.png"><div id="low_dose_networkpng-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('low_dose_networkpng-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="low_dose_networkpng-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("low_dose_networkpng-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('low_dose_networkpng');
      var modalImg = document.getElementById("low_dose_networkpng-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="xu2020ultra" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Ultra-Low-Dose 18F-FDG Brain PET/MR Denoising using Deep Learning and Multi-Contrast Information</div>
    <!-- Author -->
    <div class="author">
      

      <em>Junshen Xu</em>, Enhao Gong, Jiahong Ouyang, <a href="https://web.stanford.edu/~pauly/" rel="external nofollow noopener" target="_blank">John Pauly</a>, and
      <span class="more-authors" title="click to view 1 more author" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '1 more author' ? 'Greg Zaharchuk' : '1 more author';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">1 more author</span>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>In Medical Imaging 2020: Image Processing</em>
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://doi.org/10.1117/12.2548350" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Positron emission tomography (PET) is a widely used molecular imaging modality for various clinical applications. With Magnetic Resonance Imaging (MRI) providing anatomical information, simultaneous PET/MR reduces the radiation risk. Both improved hardware and algorithms have been developed to further reduce the amount of radiotracer dosage, but these methods are not yet applied to very low dose. Here, we propose a Deep Learning based method to enable ultra-low-dose PET denoising with multi-contrast information from simultaneous MRI. Methods:The method is implemented to denoise 18F-fluorodeoxyglucose (FDG) brain PET images from low-dose images with 200-fold dose reduction through undersampling, and evaluated for glioblastoma (GBM) patients. Comprehensive quantitative and qualitative evaluations were conducted to verify the performance and clinical applicability of the proposed method, including quantitative accuracy evaluation, visual quality evaluation, reader study with manual tumor segmentation to evaluate the diagnostic quality. Results:The results demonstrate that the proposed method achieves superior results in performance and efficiency comparing with the state-of-art denoising methods. Conclusion:Though reconstructed from scans with only 0.5% of the standard dose, the denoised ultra-low-dose PET images deliver similar visual quality and diagnostic information as the standard-dose PET images. By combining PET and MR information, the proposed Deep Learning based method improves image quality of ultra-low-dose PET, preserves diagnostic quality, and potentially enables much safer, faster, and more cost-effective PET/MR studies.</p>
    </div>
  </div>
</div>
</li>
</ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="fetal_posepng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/fetal_pose.png"><div id="fetal_posepng-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('fetal_posepng-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="fetal_posepng-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("fetal_posepng-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('fetal_posepng');
      var modalImg = document.getElementById("fetal_posepng-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="10.1007/978-3-030-32251-9_44" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Fetal Pose Estimation in Volumetric MRI using a 3D Convolution Neural Network</div>
    <!-- Author -->
    <div class="author">
      

      <em>Junshen Xu*</em>, <a href="https://scholar.google.com/citations?user=M6LlDA4AAAAJ" rel="external nofollow noopener" target="_blank">Molin Zhang*</a>, <a href="https://scholar.google.com/citations?user=HYUyyTsAAAAJ" rel="external nofollow noopener" target="_blank">Esra Abaci Turk</a>, Larry Zhang, and
      <span class="more-authors" title="click to view 4 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '4 more authors' ? 'P. Ellen Grant, Kui Ying, Polina Golland, Elfar Adalsteinsson' : '4 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">4 more authors</span>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>In Medical Image Computing and Computer Assisted Intervention – MICCAI 2019</em>
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://doi.org/10.1007/978-3-030-32251-9_44" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="http://arxiv.org/abs/1907.04500" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      <a href="https://github.com/daviddmc/fetal-pose" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      <a href="/assets/pdf/fetal_pose_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>The performance and diagnostic utility of magnetic resonance imaging (MRI) in pregnancy is fundamentally constrained by fetal motion. Motion of the fetus, which is unpredictable and rapid on the scale of conventional imaging times, limits the set of viable acquisition techniques to single-shot imaging with severe compromises in signal-to-noise ratio and diagnostic contrast, and frequently results in unacceptable image quality. Surprisingly little is known about the characteristics of fetal motion during MRI and here we propose and demonstrate methods that exploit a growing repository of MRI observations of the gravid abdomen that are acquired at low spatial resolution but relatively high temporal resolution and over long durations (10–30 min). We estimate fetal pose per frame in MRI volumes of the pregnant abdomen via deep learning algorithms that detect key fetal landmarks. Evaluation of the proposed method shows that our framework achieves quantitatively an average error of 4.47 mm and 96.4% accuracy (with error less than 10 mm). Fetal pose estimation in MRI time series yields novel means of quantifying fetal movements in health and disease, and enables the learning of kinematic models that may enhance prospective mitigation of fetal motion artifacts during MRI acquisition.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://pubs.rsna.org/journal/radiology" rel="external nofollow noopener" target="_blank">Radiology</a></abbr></div>

  <!-- Entry bib key -->
  <div id="doi:10.1148/radiol.2018180940" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Ultra-Low-Dose 18F-Florbetaben Amyloid PET Imaging Using Deep Learning with Multi-Contrast MRI Inputs</div>
    <!-- Author -->
    <div class="author">
      

      Kevin T. Chen, Enhao Gong, Fabiola Bezerra Carvalho Macruz, <em>Junshen Xu</em>, and
      <span class="more-authors" title="click to view 9 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '9 more authors' ? 'Athanasia Boumis, Mehdi Khalighi, Kathleen L. Poston, Sharon J. Sha, Michael D. Greicius, Elizabeth Mormino, John M. Pauly, Shyam Srinivas, Greg Zaharchuk' : '9 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">9 more authors</span>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Radiology</em>, 2019
    </div>
    <div class="periodical">
      PMID: 30526350
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://doi.org/10.1148/radiol.2018180940" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p> PurposeTo reduce radiotracer requirements for amyloid PET/MRI without sacrificing diagnostic quality by using deep learning methods.Materials and MethodsForty data sets from 39 patients (mean age ± standard deviation [SD], 67 years ± 8), including 16 male patients and 23 female patients (mean age, 66 years ± 6 and 68 years ± 9, respectively), who underwent simultaneous amyloid (fluorine 18 [18F]–florbetaben) PET/MRI examinations were acquired from March 2016 through October 2017 and retrospectively analyzed. One hundredth of the raw list-mode PET data were randomly chosen to simulate a low-dose (1%) acquisition. Convolutional neural networks were implemented with low-dose PET and multiple MR images (PET-plus-MR model) or with low-dose PET alone (PET-only) as inputs to predict full-dose PET images. Quality of the synthesized images was evaluated while Bland-Altman plots assessed the agreement of regional standard uptake value ratios (SUVRs) between image types. Two readers scored image quality on a five-point scale (5 = excellent) and determined amyloid status (positive or negative). Statistical analyses were carried out to assess the difference of image quality metrics and reader agreement and to determine confidence intervals (CIs) for reading results.ResultsThe synthesized images (especially from the PET-plus-MR model) showed marked improvement on all quality metrics compared with the low-dose image. All PET-plus-MR images scored 3 or higher, with proportions of images rated greater than 3 similar to those for the full-dose images (−10% difference [eight of 80 readings], 95% CI: −15%, −5%). Accuracy for amyloid status was high (71 of 80 readings [89%]) and similar to intrareader reproducibility of full-dose images (73 of 80 [91%]). The PET-plus-MR model also had the smallest mean and variance for SUVR difference to full-dose images.ConclusionSimultaneously acquired MRI and ultra–low-dose PET data can be used to synthesize full-dose–like amyloid PET images.© RSNA, 2018Online supplemental material is available for this article.See also the editorial by Catana in this issue. </p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://journals.lww.com/nuclearmedicinecomm" rel="external nofollow noopener" target="_blank">NMC</a></abbr></div>

  <!-- Entry bib key -->
  <div id="jiang2019assessing" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Assessing EGFR Gene Mutation Status in Non-Small Cell Lung Cancer with Imaging Features from PET/CT</div>
    <!-- Author -->
    <div class="author">
      

      Mengmeng Jiang, Yiqian Zhang, <em>Junshen Xu</em>, Min Ji, and
      <span class="more-authors" title="click to view 6 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '6 more authors' ? 'Yinglong Guo, Yixian Guo, Jie Xiao, Xiuzhong Yao, Hongcheng Shi, Mengsu Zeng' : '6 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">6 more authors</span>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Nuclear medicine communications</em>, 2019
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a href="https://journals.lww.com/nuclearmedicinecomm/Abstract/2019/08000/Assessing_EGFR_gene_mutation_status_in_non_small.10.aspx" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
    </div>
    
    <div class="badges">
    </div>

    
  </div>
</div>
</li>
</ol>

  <h2 class="year">2017</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="low_dose_petgif" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/low_dose_pet.gif"><div id="low_dose_petgif-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('low_dose_petgif-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="low_dose_petgif-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("low_dose_petgif-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('low_dose_petgif');
      var modalImg = document.getElementById("low_dose_petgif-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="xu2017200x" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">200x Low-Dose PET Reconstruction using Deep Learning</div>
    <!-- Author -->
    <div class="author">
      

      <em>Junshen Xu*</em>, Enhao Gong*, <a href="https://web.stanford.edu/~pauly/" rel="external nofollow noopener" target="_blank">John Pauly</a>, and <a href="https://profiles.stanford.edu/greg-zaharchuk" rel="external nofollow noopener" target="_blank">Greg Zaharchuk</a>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>arXiv preprint arXiv:1712.04119</em>, 2017
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="http://arxiv.org/abs/1712.04119" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      <a href="/assets/pdf/low_dose_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Positron emission tomography (PET) is widely used in various clinical applications, including cancer diagnosis, heart disease and neuro disorders. The use of radioactive tracer in PET imaging raises concerns due to the risk of radiation exposure. To minimize this potential risk in PET imaging, efforts have been made to reduce the amount of radio-tracer usage. However, lowing dose results in low Signal-to-Noise-Ratio (SNR) and loss of information, both of which will heavily affect clinical diagnosis. Besides, the ill-conditioning of low-dose PET image reconstruction makes it a difficult problem for iterative reconstruction algorithms. Previous methods proposed are typically complicated and slow, yet still cannot yield satisfactory results at significantly low dose. Here, we propose a deep learning method to resolve this issue with an encoder-decoder residual deep network with concatenate skip connections. Experiments shows the proposed method can reconstruct low-dose PET image to a standard-dose quality with only two-hundredth dose. Different cost functions for training model are explored. Multi-slice input strategy is introduced to provide the network with more structural information and make it more robust to noise. Evaluation on ultra-low-dose clinical data shows that the proposed method can achieve better result than the state-of-the-art methods and reconstruct images with comparable quality using only 0.5% of the original regular dose.</p>
    </div>
  </div>
</div>
</li></ol>


</div>

    </article>

</div>
      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Junshen  Xu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.
Last updated: May 20, 2023.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-NYJ88YK0VS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-NYJ88YK0VS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
