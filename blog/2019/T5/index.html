<!DOCTYPE html>
<!-- _layouts/paper-note.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    
    <!-- Website verification -->
    <meta name="google-site-verification" content="MARJu3erA7Z6jybcmR5fgSNRNtCNYMAYkk0jidGGr8A">
<!-- Avoid warning on Google Chrome
        Error with Permissions-Policy header: Origin trial controlled feature not enabled: 'interest-cohort'.
        see https://stackoverflow.com/a/75119417
    -->
    <meta http-equiv="Permissions-Policy" content="interest-cohort=()">

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>T5: Text-to-Text Transfer Transformer | Junshen  Xu</title>
    <meta name="author" content="Junshen  Xu">
    <meta name="description" content="Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/favicon_v2.ico">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://daviddmc.github.io/blog/2019/T5/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
    <style type="text/css">
      .highlight pre:not(.language-text) { background-color: #272822; color: #f8f8f2;}
      .highlight .hll { background-color: #272822; }
      .highlight .comment { color: #75715e } /* Comment c */
      .highlight .err { color: #960050; background-color: #1e0010 } /* Error */
      .highlight .keyword { color: #66d9ef } /* Keyword k*/
      .highlight .l { color: #ae81ff } /* Literal */
      .highlight .n { color: #f8f8f2 } /* Name */
      .highlight .operator { color: #f92672 } /* Operator o*/
      .highlight .punctuation { color: #f8f8f2 } /* Punctuation p*/
      .highlight .cm { color: #75715e } /* Comment.Multiline */
      .highlight .cp { color: #75715e } /* Comment.Preproc */
      .highlight .c1 { color: #75715e } /* Comment.Single */
      .highlight .cs { color: #75715e } /* Comment.Special */
      .highlight .ge { font-style: italic } /* Generic.Emph */
      .highlight .gs { font-weight: bold } /* Generic.Strong */
      .highlight .kc { color: #66d9ef } /* Keyword.Constant */
      .highlight .kd { color: #66d9ef } /* Keyword.Declaration */
      .highlight .kn { color: #f92672 } /* Keyword.Namespace */
      .highlight .kp { color: #66d9ef } /* Keyword.Pseudo */
      .highlight .kr { color: #66d9ef } /* Keyword.Reserved */
      .highlight .kt { color: #66d9ef } /* Keyword.Type */
      .highlight .ld { color: #e6db74 } /* Literal.Date */
      .highlight .number { color: #ae81ff } /* Literal.Number m*/
      .highlight .string { color: #e6db74 } /* Literal.String s*/
      .highlight .na { color: #a6e22e } /* Name.Attribute */
      .highlight .builtin { color: #f8f8f2 } /* Name.Builtin nb*/
      .highlight .class-name { color: #a6e22e } /* Name.Class nc*/
      .highlight .no { color: #66d9ef } /* Name.Constant */
      .highlight .decorator { color: #a6e22e } /* Name.Decorator nd*/
      .highlight .ni { color: #f8f8f2 } /* Name.Entity */
      .highlight .ne { color: #a6e22e } /* Name.Exception */
      .highlight .function { color: #a6e22e } /* Name.Function nf*/
      .highlight .nl { color: #f8f8f2 } /* Name.Label */
      .highlight .nn { color: #f8f8f2 } /* Name.Namespace */
      .highlight .nx { color: #a6e22e } /* Name.Other */
      .highlight .py { color: #f8f8f2 } /* Name.Property */
      .highlight .nt { color: #f92672 } /* Name.Tag */
      .highlight .nv { color: #f8f8f2 } /* Name.Variable */
      .highlight .ow { color: #f92672 } /* Operator.Word */
      .highlight .w { color: #f8f8f2 } /* Text.Whitespace */
      .highlight .mf { color: #ae81ff } /* Literal.Number.Float */
      .highlight .mh { color: #ae81ff } /* Literal.Number.Hex */
      .highlight .mi { color: #ae81ff } /* Literal.Number.Integer */
      .highlight .mo { color: #ae81ff } /* Literal.Number.Oct */
      .highlight .sb { color: #e6db74 } /* Literal.String.Backtick */
      .highlight .sc { color: #e6db74 } /* Literal.String.Char */
      .highlight .sd { color: #e6db74 } /* Literal.String.Doc */
      .highlight .s2 { color: #e6db74 } /* Literal.String.Double */
      .highlight .se { color: #ae81ff } /* Literal.String.Escape */
      .highlight .sh { color: #e6db74 } /* Literal.String.Heredoc */
      .highlight .si { color: #e6db74 } /* Literal.String.Interpol */
      .highlight .sx { color: #e6db74 } /* Literal.String.Other */
      .highlight .sr { color: #e6db74 } /* Literal.String.Regex */
      .highlight .s1 { color: #e6db74 } /* Literal.String.Single */
      .highlight .ss { color: #e6db74 } /* Literal.String.Symbol */
      .highlight .bp { color: #f8f8f2 } /* Name.Builtin.Pseudo */
      .highlight .vc { color: #f8f8f2 } /* Name.Variable.Class */
      .highlight .vg { color: #f8f8f2 } /* Name.Variable.Global */
      .highlight .vi { color: #f8f8f2 } /* Name.Variable.Instance */
      .highlight .il { color: #ae81ff } /* Literal.Number.Integer.Long */
      .highlight .gh { } /* Generic Heading & Diff Header */
      .highlight .gu { color: #75715e; } /* Generic.Subheading & Diff Unified/Comment? */
      .highlight .gd { color: #f92672; } /* Generic.Deleted & Diff Deleted */
      .highlight .gi { color: #a6e22e; } /* Generic.Inserted & Diff Inserted */
    </style>
    <script> configObj = { "buttonD": "M8 18.568L10.8 21.333 16 16.198 21.2 21.333 24 18.568 16 10.667z", "buttonT": "translate(-1148 -172) translate(832 140) translate(32 32) translate(284)", "shadowSize": "none", "roundnessSize": "999px", "buttonDToBottom": "64px", "buttonDToRight": "32px", "selectedBackgroundColor": "#c2c0bf", "selectedIconColor": "#a31f34", "buttonWidth": "40px", "buttonHeight": "40px", "svgWidth": "32px", "svgHeight": "32px" }; function createButton(obj, pageSimulator) { const body = document.querySelector("body"); backToTopButton = document.createElement("span"); backToTopButton.classList.add("softr-back-to-top-button"); backToTopButton.id = "softr-back-to-top-button"; pageSimulator ? pageSimulator.appendChild(backToTopButton) : body.appendChild(backToTopButton); backToTopButton.style.width = obj.buttonWidth; backToTopButton.style.height = obj.buttonHeight; backToTopButton.style.marginRight = obj.buttonDToRight; backToTopButton.style.marginBottom = obj.buttonDToBottom; backToTopButton.style.borderRadius = obj.roundnessSize; backToTopButton.style.boxShadow = obj.shadowSize; backToTopButton.style.color = obj.selectedBackgroundColor; backToTopButton.style.backgroundColor = obj.selectedBackgroundColor; pageSimulator ? backToTopButton.style.position = "absolute" : backToTopButton.style.position = "fixed"; backToTopButton.style.outline = "none"; backToTopButton.style.bottom = "0px"; backToTopButton.style.right = "0px"; backToTopButton.style.cursor = "pointer"; backToTopButton.style.textAlign = "center"; backToTopButton.style.border = "solid 2px currentColor"; backToTopButton.innerHTML = '<svg class="back-to-top-button-svg" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32" > <g fill="none" fill-rule="evenodd"> <path d="M0 0H32V32H0z" transform="translate(-1028 -172) translate(832 140) translate(32 32) translate(164) matrix(1 0 0 -1 0 32)" /> <path class="back-to-top-button-img" fill-rule="nonzero" d="M11.384 13.333h9.232c.638 0 .958.68.505 1.079l-4.613 4.07c-.28.246-.736.246-1.016 0l-4.613-4.07c-.453-.399-.133-1.079.505-1.079z" transform="translate(-1028 -172) translate(832 140) translate(32 32) translate(164) matrix(1 0 0 -1 0 32)" /> </g> </svg>'; backToTopButtonSvg = document.querySelector(".back-to-top-button-svg"); backToTopButtonSvg.style.verticalAlign = "middle"; backToTopButtonSvg.style.margin = "auto"; backToTopButtonSvg.style.justifyContent = "center"; backToTopButtonSvg.style.width = obj.svgWidth; backToTopButtonSvg.style.height = obj.svgHeight; backToTopButton.appendChild(backToTopButtonSvg); backToTopButtonImg = document.querySelector(".back-to-top-button-img"); backToTopButtonImg.style.fill = obj.selectedIconColor; backToTopButtonSvg.appendChild(backToTopButtonImg); backToTopButtonImg.setAttribute("d", obj.buttonD); backToTopButtonImg.setAttribute("transform", obj.buttonT); if (!pageSimulator) { backToTopButton.style.display = "none"; window.onscroll = function () { if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) { backToTopButton.style.display = "block"; } else { backToTopButton.style.display = "none"; } }; backToTopButton.onclick = function () { document.body.scrollTop = 0; document.documentElement.scrollTop = 0; }; } }; document.addEventListener("DOMContentLoaded", function () { createButton(configObj, null); });</script>
  </head>
  
  <body class="fixed-top-nav">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Junshen </span>Xu</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">teaching</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>T5: Text-to-Text Transfer Transformer</h1>
        <p>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</p>
      </d-title>

      <d-byline>
          <div class="byline grid">
            <div>
              <h3>Published</h3>
                <p>October 23, 2019</p> 
            </div>
            
            <div>
              <h3>Paper</h3>
                <p><a href="https://arxiv.org/pdf/1910.10683.pdf" rel="external nofollow noopener" target="_blank">arXiv</a></p> 
            </div>
            
            
          </div>
      </d-byline>

      <d-article>
        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#takeaways">Takeaways</a></div>
            <div><a href="#introduction">Introduction</a></div>
            <div><a href="#methods">Methods</a></div>
            <ul>
              <li><a href="#model">Model</a></li>
              <li><a href="#dataset">Dataset</a></li>
              <li><a href="#input-and-output-format">Input and Output Format</a></li>
              
            </ul>
<div><a href="#experiments">Experiments</a></div>
            <ul>
              <li><a href="#baseline">Baseline</a></li>
              <li><a href="#architectures">Architectures</a></li>
              <li><a href="#unsupervised-objectives">Unsupervised Objectives</a></li>
              <li><a href="#pre-training-dataset">Pre-Training Dataset</a></li>
              <li><a href="#training-strategy">Training Strategy</a></li>
              <li><a href="#scaling">Scaling</a></li>
              <li><a href="#putting-it-all-together">Putting It All Together</a></li>
              
            </ul>
          </nav>
        </d-contents>

        <h2 id="takeaways">Takeaways</h2>

<ul>
  <li>
    <p><strong>Text-to-text</strong> provides a simple way to train a single model on a wide variety of text tasks. T2T is simple, yet obtained comparable performance to task-specific architectures and ultimately produced SOTA results when combined with scale.</p>
  </li>
  <li>
    <p><strong>Architectures:</strong> The original encoder-decoder form worked best in T2T.</p>
  </li>
  <li>
    <p><strong>Unsupervised objectives:</strong> The denoising objectives performed best in T2T.</p>
  </li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Motivation: There is a need for a more rigorous understanding of the contributions of different components in transfer learning for NLP (large-scale pre-training models), e.g., different models, pre-training objectives, datasets, and fine-tuning methods.</p>

<p>The basic idea: Introduce a unified framework (T5) that converts all text-based language problems into a text-to-text format. The text-to-text framework allows us to directly apply the same model, objective, training procedure, and decoding process to every task considered.</p>

<p>This work primarily comprises a survey, exploration, and empirical comparison of existing techniques, and explores the limits of current approaches by scaling up the insights (training models up to 11 B parameters on dataset up to 750GB)</p>

<h2 id="methods">Methods</h2>

<h3 id="model">Model</h3>

<p>T5 closely follows the original Transformer<d-cite key="Transformer"></d-cite>.</p>

<p>Main differences:</p>
<ul>
  <li>LayerNorm
    <ul>
      <li>LayerNorms are used at the start of each block and the end of the last block.</li>
      <li>Scale-only LayerNorm, i.e., no additive bias.</li>
    </ul>
  </li>
  <li>Positional embedding
    <ul>
      <li>Relative positional embedding.</li>
      <li>Simplified position embeddings where each “embedding” is simply a scalar that is added to the corresponding logit used for computing the attention weights.</li>
      <li>Share the position embedding parameters across all layers in the model, though within a given layer each attention head uses a different learned position embedding.</li>
      <li>Use 32 embeddings with ranges that increase in size logarithmically up to an offset of 128 beyond which we assign all relative positions to the same embedding. (<a href="https://github.com/huggingface/transformers/blob/v4.25.1/src/transformers/models/t5/modeling_t5.py#L374" rel="external nofollow noopener" target="_blank">Implementation</a>)</li>
    </ul>
  </li>
  <li>Input embedding matrix
    <ul>
      <li>The weights of the output dense layer (before the final softmax) are shared with the input embedding matrix.</li>
    </ul>
  </li>
</ul>

<p>T5 uses an encoder-decoder architecture as in the original Transformer<d-cite key="Transformer"></d-cite>. In comparison, GPT<d-cite key="GPT"></d-cite>, GPT-2<d-cite key="GPT-2"></d-cite>, BERT<d-cite key="BERT"></d-cite> use a single stack of Transformer layers.</p>

<h3 id="dataset">Dataset</h3>

<p>The Colossal Clean Crawled Corpus (C4), ~ 750 GB.</p>

<ol>
  <li>Start with Common Crawl</li>
  <li>Retain lines that ended in a terminal punctuation mark.</li>
  <li>Discarded any page with fewer than 5 sentences and only retained lines that contained at least 3 words.</li>
  <li>Remove any page that contained any word on the “List of Dirty, Naughty, Obscene or Otherwise Bad Words”.</li>
  <li>Remove any line with the word Javascript.</li>
  <li>Remove any page where the phrase “lorem ipsum” (placeholder) appeared.</li>
  <li>Removed any pages that contained a curly bracket to avoid pages with code.</li>
  <li>Discarded all but one of any three-sentence span occurring more than once in the data set.</li>
  <li>Filter out non-English pages</li>
</ol>

<h3 id="input-and-output-format">Input and Output Format</h3>

<p>Cast all of the tasks considered into a “text-to-text” format, i.e., a task where the model is fed some text for context or conditioning and is then asked to produce some output text.</p>

<p>The text-to-text framework provides a consistent training objective both for pre-training and fine-tuning.</p>

<p>T5 is trained with a maximum likelihood objective (using “teacher forcing”, i.e., using ground truth as input, instead of model output from a prior time step as an input) and a cross-entropy loss regardless of the task. To specify which task the model should perform, a task-specific (text) prefix is added to the original input sequence before feeding it to the model.</p>

<p>Compare to GPT-2<d-cite key="GPT-2"></d-cite>, which also uses prompts:</p>

<ul>
  <li>GPT-2 is autoregressive (processing the prefix left-to-right), while T5 explicitly processes an input with an encoder (bidirectional attention).</li>
  <li>GPT-2 focuses on zero-shot learning, while T5 focuses on transfer learning with fine-tuning.</li>
</ul>

<h2 id="experiments">Experiments</h2>

<h3 id="baseline">Baseline</h3>

<h4 id="baseline-model">Baseline Model</h4>

<p>A standard encoder-decoder Transformer<d-cite key="Transformer"></d-cite> is designed so that the encoder and decoder are each similar in size and configuration to a BERT-base model.</p>

<h4 id="vocabulary">Vocabulary</h4>

<p>Use SentencePiece to encode text as WordPiece tokens (use a vocabulary of 32,000 wordpieces)</p>

<p>Trained the SentencePiece model on a mixture of 10 parts of English C4 data with 1 part each of data classified as German, French or Romanian. This vocabulary was shared across both the input and output of the model. Note that the vocabulary makes it so that the model can only process a predetermined, fixed set of languages.</p>

<h4 id="unsupervised-objective">Unsupervised Objective</h4>

<p>Use the “denoising” objectives, i.e., masked language modeling. The model is trained to predict missing or otherwise corrupted tokens in the input.</p>

<p>Design an objective that randomly samples and then drops out 15% of tokens in the input sequence. All consecutive spans of dropped-out tokens are replaced by a single sentinel token. Each sentinel token is assigned a token ID that is unique to the sequence.</p>

<p>The target then corresponds to all of the dropped-out spans of tokens, delimited by the same sentinel tokens used in the input sequence plus a final sentinel token to mark the end of the target sequence. An example is as follows.</p>

<p><em>Original text</em></p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Thank you for inviting me to your party last week
</code></pre></div></div>

<p><em>Inputs</em></p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Thank you for &lt;X&gt; to your party &lt;Y&gt; week
</code></pre></div></div>

<p><em>Target</em></p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;X&gt; for inviting &lt;Y&gt; last &lt;Z&gt;
</code></pre></div></div>

<h3 id="architectures">Architectures</h3>

<p>Review and compare the following architectural variants.</p>

<div class="l-body" style="text-align:center;">
  <img src="https://media.arxiv-vanity.com/render-output/5540256/x4.png" width="80%" style="margin-bottom: 12px; background-color: white;">
  <p>Different schematics of the Transformer architecture variants.</p>
</div>

<div class="l-body" style="text-align:center;">
  <img src="https://media.arxiv-vanity.com/render-output/5540256/x3.png" width="80%" style="margin-bottom: 12px; background-color: white;">
  <p>Different attention mask patterns.</p>
</div>

<h4 id="model-structures">Model Structures</h4>

<p>A major distinguishing factor for different architectures is the “mask” used by different attention mechanisms in the model.</p>

<table>
  <thead>
    <tr>
      <th>Architectures</th>
      <th>mask</th>
      <th># of layer stacks</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Encoder-Decoder (e.g. T5)</td>
      <td>Encoder: Fully-visible, Decoder: Causal</td>
      <td>2</td>
    </tr>
    <tr>
      <td>Language model (e.g. GPT)</td>
      <td>Causal</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Prefix LM</td>
      <td>Causal with prefix</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<p>A fundamental and frequently cited drawback of using an LM in the text-to-text setting is that causal masking forces the model’s representation of the \(i\)-th entry of the input sequence to only depend on the entries up until \(i\). This issue can be avoided in a Transformer-based language model simply by changing the masking pattern (Prefix LM).</p>

<p>The main difference between a prefix LM and the BERT architecture is that the classifier is simply integrated into the output layer of the Transformer decoder in the prefix LM.</p>

<h4 id="objectives">Objectives</h4>

<p>Considered both the standard language modeling objective and the denoising objective discussed in <a href="#unsupervised-objective">the previous section</a>.</p>

<p>Language modeling objective:</p>

<p>For models that ingest a prefix before making predictions (the encoder-decoder model and prefix LM), we sample a span of text from our unlabeled data set and choose a random point to split it into prefix and target portions.</p>

<p>For the standard language model, we train the model to predict the entire span from beginning to end.</p>

<p>Denoising objective:</p>

<p>The unsupervised denoising objective is designed for text-to-text models; to adapt it for use with a language model the inputs and targets are concatenated.</p>

<h4 id="results">Results</h4>

<ul>
  <li>For all tasks, the encoder-decoder architecture with the denoising objective performed best.</li>
  <li>Though an encoder-decoder model uses twice as many parameters as “encoder-only” (e.g. BERT) or “decoder-only” (language model) architectures, it has a similar computational cost.</li>
  <li>Sharing the parameters in the encoder and decoder did not result in a substantial performance drop while halving the total parameter count.</li>
</ul>

<h3 id="unsupervised-objectives">Unsupervised Objectives</h3>

<p>Explore different unsupervised objectives. Overall, all of the objectives ingest a sequence of token IDs corresponding to a tokenized span of text from our unlabeled text data set. The token sequence is processed to produce a (corrupted) input sequence and a corresponding target. Then, the model is trained as usual with maximum likelihood to predict the target sequence.</p>

<h4 id="choices-of-objectives">Choices of Objectives</h4>

<table>
  <thead>
    <tr>
      <th>Objective</th>
      <th>Example input</th>
      <th>Example target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Prefix LM</td>
      <td>Thank you for inviting</td>
      <td>me to your party last week .</td>
    </tr>
    <tr>
      <td>BERT-style</td>
      <td>Thank you <code class="language-plaintext highlighter-rouge">&lt;M&gt;</code> <code class="language-plaintext highlighter-rouge">&lt;M&gt;</code> me to your party <strong>apple</strong> week .</td>
      <td><em>(original text)</em></td>
    </tr>
    <tr>
      <td>Deshuffling</td>
      <td>party me for your to . last fun you inviting week Thank</td>
      <td><em>(original text)</em></td>
    </tr>
    <tr>
      <td>MASS-style</td>
      <td>Thank you <code class="language-plaintext highlighter-rouge">&lt;M&gt;</code> <code class="language-plaintext highlighter-rouge">&lt;M&gt;</code> me to your party <code class="language-plaintext highlighter-rouge">&lt;M&gt;</code> week .</td>
      <td><em>(original text)</em></td>
    </tr>
    <tr>
      <td>I.i.d. noise, replace spans</td>
      <td>Thank you <code class="language-plaintext highlighter-rouge">&lt;X&gt;</code> me to your party <code class="language-plaintext highlighter-rouge">&lt;Y&gt;</code> week .</td>
      <td>
<code class="language-plaintext highlighter-rouge">&lt;X&gt;</code> for inviting <code class="language-plaintext highlighter-rouge">&lt;Y&gt;</code> last <code class="language-plaintext highlighter-rouge">&lt;Z&gt;</code>
</td>
    </tr>
    <tr>
      <td>I.i.d. noise, drop tokens</td>
      <td>Thank you me to your party week .</td>
      <td>for inviting last</td>
    </tr>
    <tr>
      <td>Random spans</td>
      <td>Thank you <code class="language-plaintext highlighter-rouge">&lt;X&gt;</code> to <code class="language-plaintext highlighter-rouge">&lt;Y&gt;</code> week .</td>
      <td>
<code class="language-plaintext highlighter-rouge">&lt;X&gt;</code> for inviting me <code class="language-plaintext highlighter-rouge">&lt;Y&gt;</code> your party last <code class="language-plaintext highlighter-rouge">&lt;Z&gt;</code>
</td>
    </tr>
  </tbody>
</table>

<h4 id="results-1">Results</h4>

<ul>
  <li>
<strong>Denoising objectives</strong> outperformed language modeling and deshuffling for pre-training.</li>
  <li>No remarkable difference across the many variants of the denoising objectives.</li>
  <li>Different objectives can lead to different sequence lengths and thus different training speeds.</li>
</ul>

<h3 id="pre-training-dataset">Pre-Training Dataset</h3>

<ul>
  <li>Performance degrades as the data set size shrinks.</li>
  <li>When comparing C4 to data sets that use additional filtering, the authors found that training on in-domain unlabeled data could boost performance in a few downstream tasks. However, constraining to a single domain typically results in a smaller data set.</li>
  <li>Performance can degrade when an unlabeled data set is small enough that it is repeated many times over the course of pre-training. This motivates the use of a large and diverse data set like C4 for generic language understanding tasks.</li>
</ul>

<h3 id="training-strategy">Training Strategy</h3>

<h4 id="fine-tuning-methods">Fine-Tuning Methods</h4>

<p>The standard method is to fine-tune <em>all</em> parameters in the model.</p>

<p>Two alternative methods:</p>

<ul>
  <li>
<em>Adapter layers:</em> additional dense-ReLU-dense blocks are added after each of the preexisting feed-forward networks in each block of the Transformer. When fine-tuning, only the adapter layer and layer normalization parameters are updated.</li>
  <li>
<em>gradual unfreezing:</em> more and more of the model’s parameters are fine-tuned over time.</li>
</ul>

<p>The standard method performs best.</p>

<h4 id="multi-task-learning">Multi-Task Learning</h4>

<p>Train the model on multiple tasks simultaneously (the unsupervised task and downstream supervised tasks). For the unified text-to-text framework, “multi-task learning” simply corresponds to mixing data sets
together.</p>

<p>In general, multi-task training underperforms pre-training followed by fine-tuning on most tasks.</p>

<h4 id="combining-multi-task-learning-with-fine-tuning">Combining Multi-Task Learning with Fine-Tuning</h4>

<p>The model is pre-trained on all tasks at once but is then fine-tuned on the individual
supervised tasks.</p>

<p>Fine-tuning after multi-task pre-training results in comparable performance to the baseline (unsupervised pre-training + supervised fine-tuning). This suggests that using fine-tuning after multi-task learning can help mitigate some of the trade-offs between different mixing rates.</p>

<h3 id="scaling">Scaling</h3>

<p>Compared various strategies for taking advantage of additional computing, including training the model on more data, training a larger model, and using an ensemble of models. Each approach conferred a significant boost in performance. Specifically,</p>

<ul>
  <li>Increasing the training time and/or model size consistently improves the baseline.</li>
  <li>In general, increasing the model size resulted in an additional bump in performance compared to solely increasing the training time or batch size.</li>
  <li>Increasing the training time and increasing the model size can be complementary means of improving performance.</li>
  <li>Training a smaller model on more data was often outperformed by training a larger model for fewer steps.</li>
  <li>An ensemble of models can provide substantially better results than a single model, which provides an orthogonal means of leveraging additional computation.</li>
  <li>Ensembling models that were fine-tuned from the same base pre-trained model performed worse than pre-training and fine-tuning all models completely separately, though fine-tune-only ensembling still substantially outperformed a single model.</li>
  <li>Different scaling methods have different trade-offs that are separate from their performance.</li>
</ul>

<h3 id="putting-it-all-together">Putting It All Together</h3>

<p>The final T5 model is as follows.</p>

<ul>
  <li>
<strong>Objective:</strong> the span-corruption objective, a variant of the denoising objective.</li>
  <li>
<strong>Longer training:</strong> pre-train for 1M steps on a batch size of 2048 sequences of length 512 corresponding to a total of about 1T pre-training tokens.</li>
  <li>
<strong>Model sizes:</strong> up to 11B.</li>
  <li><strong>Multi-task pre-training + fine-tuning</strong></li>
  <li>
<strong>Beam search:</strong> replace greedy decoding by a beam search with a beam width of 4 and a length penalty of \(\alpha=0.6\).</li>
</ul>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/paper-notes.bib"></d-bibliography>
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Junshen  Xu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.
Last updated: May 20, 2023.
      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-NYJ88YK0VS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-NYJ88YK0VS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
